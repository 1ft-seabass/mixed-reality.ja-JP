---
title: 視線
description: HoloLens 2 では、開発者がユーザーに関する情報を使用できるようにすることで、holographic experience 内で新しいレベルのコンテキストと人間の理解を実現できます。
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
keywords: 視線追跡、Mixed Reality、インプット、視線、視線
ms.openlocfilehash: c847f7de2cf4492c89225a88aeaf189f51cfbc40
ms.sourcegitcommit: b0b1b8e1182cce93929d409706cdaa99ff24fdee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/23/2019
ms.locfileid: "68387599"
---
# <a name="eye-gaze-on-hololens-2"></a><span data-ttu-id="27c23-104">HoloLens 2 での視線</span><span class="sxs-lookup"><span data-stu-id="27c23-104">Eye-gaze on HoloLens 2</span></span>
<span data-ttu-id="27c23-105">HoloLens 2 では、開発者がユーザーに関する情報を使用できるようにすることで、holographic experience 内で新しいレベルのコンテキストと人間の理解を実現できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-105">HoloLens 2 allows for a new level of context and human understanding within the holographic experience by providing developers with the ability of using information about what users are looking at.</span></span> <span data-ttu-id="27c23-106">このページでは、さまざまなユースケースの目の追跡や、視線を使用したユーザーインターフェイスの設計時に見られることについて、どのように役立つかを開発者に通知します。</span><span class="sxs-lookup"><span data-stu-id="27c23-106">This page tells developers how they can benefit from eye tracking for various use cases as well as what to look for when designing eye-gaze-based user interfaces.</span></span> 


## <a name="device-support"></a><span data-ttu-id="27c23-107">デバイスのサポート</span><span class="sxs-lookup"><span data-stu-id="27c23-107">Device support</span></span>

<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><span data-ttu-id="27c23-108"><strong>機能</strong></span><span class="sxs-lookup"><span data-stu-id="27c23-108"><strong>Feature</strong></span></span></td>
     <td><span data-ttu-id="27c23-109"><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></span><span class="sxs-lookup"><span data-stu-id="27c23-109"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
     <td><span data-ttu-id="27c23-110"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="27c23-110"><strong>HoloLens 2</strong></span></span></td>
     <td><span data-ttu-id="27c23-111"><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></span><span class="sxs-lookup"><span data-stu-id="27c23-111"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
</tr>
<tr>
     <td><span data-ttu-id="27c23-112">視線</span><span class="sxs-lookup"><span data-stu-id="27c23-112">Eye-gaze</span></span></td>
     <td><span data-ttu-id="27c23-113">❌</span><span class="sxs-lookup"><span data-stu-id="27c23-113">❌</span></span></td>
     <td><span data-ttu-id="27c23-114">✔️</span><span class="sxs-lookup"><span data-stu-id="27c23-114">✔️</span></span></td>
     <td><span data-ttu-id="27c23-115">❌</span><span class="sxs-lookup"><span data-stu-id="27c23-115">❌</span></span></td>
</tr>
</table>

## <a name="use-cases"></a><span data-ttu-id="27c23-116">使用事例</span><span class="sxs-lookup"><span data-stu-id="27c23-116">Use cases</span></span>
<span data-ttu-id="27c23-117">視線追跡を使用すれば、アプリケーションは、ユーザーが見ている場所をリアルタイムで追跡できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-117">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="27c23-118">次のユースケースでは、mixed reality での視線追跡で可能な相互作用について説明します。</span><span class="sxs-lookup"><span data-stu-id="27c23-118">The following use cases describe some interactions that are possible with eye tracking in mixed reality.</span></span>
<span data-ttu-id="27c23-119">[混合現実のツールキット](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)は、目を通して見やすいように、目を追って見て見やすくするための便利で強力な例をいくつか提供していることに注意してください。ユーザーが見ている内容。</span><span class="sxs-lookup"><span data-stu-id="27c23-119">Keep in mind that the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) is useful for providing several interesting and powerful examples for using eye tracking, such as quick and effortless eye-supported target selections as well as automatically scrolling through text based on what the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="27c23-120">ユーザー意図</span><span class="sxs-lookup"><span data-stu-id="27c23-120">User intent</span></span>    
<span data-ttu-id="27c23-121">ユーザーがどこで見ているかについての情報は、音声、ハンド、コントローラーなど**の他の入力に対し**て強力なコンテキストを提供します。</span><span class="sxs-lookup"><span data-stu-id="27c23-121">Information about where and what a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="27c23-122">これは、さまざまなタスクに利用できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-122">This can be used for various tasks.</span></span>
<span data-ttu-id="27c23-123">たとえば、ホログラムを見て「選択」と指示するだけで (「選択」を参照して[ください)、](gaze-and-commit.md)"select" を言い、"put the..." と指示し、ユーザーが配置する場所を調べることによって、シーン全体を**対象**とすることができます。ホログラムと言います。"</span><span class="sxs-lookup"><span data-stu-id="27c23-123">For example, this can range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying "select" (also see [Head-gaze and commit](gaze-and-commit.md)) or by saying "put this...", then looking over to where the user wants to place the hologram and say "...there".</span></span> <span data-ttu-id="27c23-124">この例は、「[Mixed Reality Toolkit - Eye-supported Target Selection (Mixed Reality Toolkit - 目で支援するターゲット選択)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)」と「[Mixed Reality Toolkit - Eye-supported Target Positioning (Mixed Reality Toolkit - 目で支援するターゲット配置)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)」に記載されています。</span><span class="sxs-lookup"><span data-stu-id="27c23-124">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="27c23-125">さらに、ユーザーの目的の例として、ユーザーが参照する情報を使用して、埋め込み仮想エージェントや対話型ホログラムによるエンゲージメントを強化することができます。</span><span class="sxs-lookup"><span data-stu-id="27c23-125">Additionally, an example for user intent might include using information about what users look at to enhance engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="27c23-126">たとえば、仮想エージェントは、現在表示されているコンテンツに基づいて、使用可能なオプションとその動作を適合させる場合があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-126">For instance, virtual agents might adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="27c23-127">暗黙的アクション</span><span class="sxs-lookup"><span data-stu-id="27c23-127">Implicit actions</span></span>
<span data-ttu-id="27c23-128">暗黙的アクションのカテゴリは、ユーザー意図に密接に関係しています。</span><span class="sxs-lookup"><span data-stu-id="27c23-128">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="27c23-129">これは、ホログラムまたはユーザーインターフェイスの要素が多少 instinctual な方法で対応することで、ユーザーがシステムと対話するのではなく、システムとユーザーが同期しているように感じる可能性があるということではないかもしれません。1つの例として、視線を使用した**自動スクロール**があります。これは、テキストがスクロールし続けているときや、ユーザーの宝石と同期しているときに、テキストを読み上げます。</span><span class="sxs-lookup"><span data-stu-id="27c23-129">The idea is that holograms or user interface elements react in a somewhat instinctual way that may not even feel like the user is interacting with the system at all, but rather that the system and the user are in sync. One example is **eye-gaze-based auto scroll** where the user reads text as the text continues to scroll or flow in sync with with the user's gaze.</span></span> <span data-ttu-id="27c23-130">この重要な点は、スクロール速度がユーザーの読み取り速度に応じて変化することです。</span><span class="sxs-lookup"><span data-stu-id="27c23-130">A key aspect of this is that scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="27c23-131">もう1つの例として、視線がサポートされ**ているズームとパン**があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-131">Another example is **eye-supported zoom and pan** where the user can feel like diving exactly toward what he or she is focused o.</span></span> <span data-ttu-id="27c23-132">ズームのトリガーとズーム速度の制御は、音声入力または手書き入力によって制御できます。これは、ユーザーがコントロールの感覚を持つことができるようにするために重要です。</span><span class="sxs-lookup"><span data-stu-id="27c23-132">Triggering zoom and controlling zoom speed can be controlled by voice or hand input, which is important for providing the user with the feeling of control while avoiding being overwhelmed.</span></span> <span data-ttu-id="27c23-133">これらの設計ガイドラインについては、以下で詳しく説明します。</span><span class="sxs-lookup"><span data-stu-id="27c23-133">We will talk about these design guidelines in more detail below.</span></span> <span data-ttu-id="27c23-134">拡大した後、ユーザーは、目を見つめて使用するだけで、自宅などの道をスムーズにたどることができます。</span><span class="sxs-lookup"><span data-stu-id="27c23-134">Once zoomed in, the user can  smoothly follow, for example, the course of a street to explore his or her neighborhood by simply using their eye-gaze.</span></span>
<span data-ttu-id="27c23-135">この種の相互作用に関するデモの例は、「[Mixed Reality Toolkit - Eye-supported Navigation (Mixed Reality Toolkit - 目で支援するナビゲーション)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)」のサンプルを参照してください。</span><span class="sxs-lookup"><span data-stu-id="27c23-135">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="27c23-136">_暗黙的なアクション_の追加のユースケースには、次のものが含まれます。</span><span class="sxs-lookup"><span data-stu-id="27c23-136">Additional use cases for _implicit actions_ can include:</span></span>
- <span data-ttu-id="27c23-137">**スマート通知:** 注目していた場所に通知がポップアップ表示されて不快に思ったことはありませんか?</span><span class="sxs-lookup"><span data-stu-id="27c23-137">**Smart notifications:** Ever get annoyed by notifications popping up right where you were focusing?</span></span> <span data-ttu-id="27c23-138">ユーザーがどのようなことに注目しているかを考慮して、ユーザーが現在使用している場所から通知をオフセットすることで、このエクスペリエンスを向上させることができます。</span><span class="sxs-lookup"><span data-stu-id="27c23-138">Taking into account what a user is paying attention to, you can make this experience better by offsetting notifications from where the user is currently gazing.</span></span> <span data-ttu-id="27c23-139">これにより、取られるが制限され、ユーザーの読み取りが完了すると自動的に破棄されます。</span><span class="sxs-lookup"><span data-stu-id="27c23-139">This limits distractions and automatically dismisses them once the user is finished reading.</span></span> 
- <span data-ttu-id="27c23-140">**気が利くホログラム:** Gazed 時に微妙に反応するホログラム。</span><span class="sxs-lookup"><span data-stu-id="27c23-140">**Attentive holograms:** Holograms that subtly react when being gazed upon.</span></span> <span data-ttu-id="27c23-141">これには、わずかに光る UI 要素から、ユーザーに見てみることができるように、または長時間の確認の後にユーザーの目を見つめないようにするために、わずかな咲きの花から仮想ペットまで、さまざまなものがあります。</span><span class="sxs-lookup"><span data-stu-id="27c23-141">This can range from slightly glowing UI elements to a slowly blooming flower to a virtual pet starting to look back at the user or trying to avoid the user's eye-gaze after a prolonged stare.</span></span> <span data-ttu-id="27c23-142">この相互作用によって、アプリケーションでの接続性と満足度がわかりやすくなる場合があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-142">This interaction might provide an interesting sense of connectivity and satisfaction in your application.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="27c23-143">注意追跡</span><span class="sxs-lookup"><span data-stu-id="27c23-143">Attention tracking</span></span>   
<span data-ttu-id="27c23-144">どのようなユーザーが見ているかについての情報は、設計の使いやすさを評価し、効率的なワークフローの問題を特定するための非常強力なツールです。</span><span class="sxs-lookup"><span data-stu-id="27c23-144">Information about where or what users look at is an immensely powerful tool to assess usability of designs, and to identify problems in efficient workflows.</span></span> <span data-ttu-id="27c23-145">さまざまなアプリケーション領域では、視線追跡の視覚化と分析が一般的な方法です。</span><span class="sxs-lookup"><span data-stu-id="27c23-145">Eye tracking visualization and analytics are a common practice in various application areas.</span></span> <span data-ttu-id="27c23-146">HoloLens 2 では、このことを理解するために新しいディメンションを提供しています。これは、3D ホログラムを実際のコンテキストで配置し、それに従って評価することができるためです。</span><span class="sxs-lookup"><span data-stu-id="27c23-146">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed accordingly.</span></span> <span data-ttu-id="27c23-147">[Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)には、視線追跡データをログに記録して読み込む方法と、それらを視覚化する方法の基本的な例が用意されています。</span><span class="sxs-lookup"><span data-stu-id="27c23-147">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and  how to visualize them.</span></span>

<span data-ttu-id="27c23-148">この領域の他のアプリケーションには、次のものが含まれます。</span><span class="sxs-lookup"><span data-stu-id="27c23-148">Other applications in this area can include:</span></span> 
-   <span data-ttu-id="27c23-149">**リモートの視線可視化:** リモートコラボレーターが見ていることを視覚化し、命令が正しく認識されているかどうかを確認します。</span><span class="sxs-lookup"><span data-stu-id="27c23-149">**Remote eye-gaze visualization:** Visualize what remote collaborators are looking at to ensure whether instructions are correctly understood and followed.</span></span>
-   <span data-ttu-id="27c23-150">**ユーザー調査研究:** アテンション追跡を使用すると、初心者と専門家のユーザーが視覚的にコンテンツを分析する方法や、医療データを分析したり、運用している場合などの複雑なタスクに対する手作業の調整方法を調べることができます。</span><span class="sxs-lookup"><span data-stu-id="27c23-150">**User research studies:** Attention tracking can be used to explore the way novice vs. expert users visually analyze content or how their hand-eye-coordination for complex tasks, such as for analysis of medical data or while operating machinery.</span></span>
-   <span data-ttu-id="27c23-151">**トレーニング シミュレーションとパフォーマンス監視:** 実行フローにおけるボトルネックをより効率的に特定することにより、タスクの実行を練習して最適化します。</span><span class="sxs-lookup"><span data-stu-id="27c23-151">**Training simulations and Performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span>
-   <span data-ttu-id="27c23-152">**デザイン評価、宣伝、市場調査:** 視線追跡は、web サイトと製品の設計を評価する際に市場調査を行うための一般的なツールです。</span><span class="sxs-lookup"><span data-stu-id="27c23-152">**Design evaluations, advertisement and marketing research:** Eye tracking is a common tool for market research when evaluateing website and product designs.</span></span>

### <a name="additional-use-cases"></a><span data-ttu-id="27c23-153">その他の使用事例</span><span class="sxs-lookup"><span data-stu-id="27c23-153">Additional use cases</span></span>
- <span data-ttu-id="27c23-154">**ゲーム:** スーパーパワーが欲しくなったことはありませんか?</span><span class="sxs-lookup"><span data-stu-id="27c23-154">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="27c23-155">こつをお教えしましょう。</span><span class="sxs-lookup"><span data-stu-id="27c23-155">Here's your chance!</span></span> <span data-ttu-id="27c23-156">ホログラムを levitate することで、ホログラムを作成できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-156">You can levitate holograms by staring at them.</span></span> <span data-ttu-id="27c23-157">目からレーザー ビームを発射します。</span><span class="sxs-lookup"><span data-stu-id="27c23-157">Shoot laser beams from your eyes.</span></span> <span data-ttu-id="27c23-158">敵を石にするか、固定します。</span><span class="sxs-lookup"><span data-stu-id="27c23-158">Turn enemies into stone or freeze them.</span></span> <span data-ttu-id="27c23-159">透視能力を使ってビルを探索します。</span><span class="sxs-lookup"><span data-stu-id="27c23-159">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="27c23-160">使い道は想像力次第です。</span><span class="sxs-lookup"><span data-stu-id="27c23-160">Your imagination is the limit!</span></span>  

- <span data-ttu-id="27c23-161">**表情豊かなアバター:** 視線追跡は、ライブ目の追跡日付を使用して、ユーザーがどのようなものかを示すアバターの目をアニメーション化することで、より表現力の高い3D アバターを支援します。</span><span class="sxs-lookup"><span data-stu-id="27c23-161">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live-eye tracking date to animate the avatar's eyes that indicate what the user is looking at.</span></span> <span data-ttu-id="27c23-162">また、ウインクとまばたきを追加すると、表現力が増します。</span><span class="sxs-lookup"><span data-stu-id="27c23-162">It also adds more expressiveness by adding winks and blinks.</span></span> 

- <span data-ttu-id="27c23-163">**テキスト入力:** 視力の追跡は、特に音声や手の使用が不便な場合に、低労力のテキスト入力の代替手段として使用できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-163">**Text entry:** Eye tracking can be used as an alternative for low-effort text entry, especially when speech or hands are inconvenient to use.</span></span> 


## <a name="eye-tracking-api"></a><span data-ttu-id="27c23-164">Eye Tracking API</span><span class="sxs-lookup"><span data-stu-id="27c23-164">Eye tracking API</span></span>
<span data-ttu-id="27c23-165">視線の相互作用に関する特定の設計ガイドラインについて詳しく説明する前に、HoloLens 2 アイ Tracker API が開発者に提供する機能について簡単に説明します。</span><span class="sxs-lookup"><span data-stu-id="27c23-165">Before going into detail about the specific design guidelines for eye-gaze interaction, we want to briefly point out the capabilities that the HoloLens 2 Eye Tracker API provides to developers.</span></span> <span data-ttu-id="27c23-166">ここでは、約_30 FPS_でデータを提供する、1つの目を見つめて見つめた出発点と方向を提供します。</span><span class="sxs-lookup"><span data-stu-id="27c23-166">It provides a single eye-gaze--gaze origin and direction--providing data at approximately _30 FPS_.</span></span> 

<span data-ttu-id="27c23-167">予測される視線は、ca 内にあります。</span><span class="sxs-lookup"><span data-stu-id="27c23-167">The predicted eye-gaze lies within ca.</span></span> <span data-ttu-id="27c23-168">1.0 ~ 1.5 °、実際のターゲットを中心にした角度。</span><span class="sxs-lookup"><span data-stu-id="27c23-168">1.0 - 1.5 degrees in visual angle around the actual target.</span></span> <span data-ttu-id="27c23-169">わずかなずれが想定されるため、この下限値のマージンを考慮する必要があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-169">As slight imprecisions are expected, you should plan for some margin around this lower bound value.</span></span> <span data-ttu-id="27c23-170">詳細については、後述します。</span><span class="sxs-lookup"><span data-stu-id="27c23-170">We will discuss this more below.</span></span> <span data-ttu-id="27c23-171">視線追跡が正しく機能するためには、各ユーザーが視線追跡ユーザー調整を行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-171">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="27c23-172">![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="27c23-172">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="27c23-173">*2メートル距離で最適なターゲットサイズ*</span><span class="sxs-lookup"><span data-stu-id="27c23-173">*Optimal target size at a 2-meter distance*</span></span>
<br>
<br>
<span data-ttu-id="27c23-174">[アイ TRACKING API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose)には、' EyesPose ' を使用してアクセスできます。</span><span class="sxs-lookup"><span data-stu-id="27c23-174">The [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) is accessible through: \`Windows.Perception.People.EyesPose'.</span></span> 

## <a name="eye-gaze-design-guidelines"></a><span data-ttu-id="27c23-175">視線設計ガイドライン</span><span class="sxs-lookup"><span data-stu-id="27c23-175">Eye-gaze design guidelines</span></span>
<span data-ttu-id="27c23-176">高速に動く目のターゲット設定を利用した対話を構築することは、やりがいのあるものとなり得ます。</span><span class="sxs-lookup"><span data-stu-id="27c23-176">Building an interaction that takes advantage of fast moving eye targeting can be challenging.</span></span> <span data-ttu-id="27c23-177">このセクションでは、アプリケーションの設計時に考慮する必要がある主な利点と課題についてまとめます。</span><span class="sxs-lookup"><span data-stu-id="27c23-177">In this section, we summarize the key advantages and challenges to take into account when designing your application.</span></span> 

### <a name="benefits-of-eye-gaze-input"></a><span data-ttu-id="27c23-178">視線入力の利点</span><span class="sxs-lookup"><span data-stu-id="27c23-178">Benefits of eye-gaze input</span></span>
- <span data-ttu-id="27c23-179">**高速ポインティング。**</span><span class="sxs-lookup"><span data-stu-id="27c23-179">**High speed pointing.**</span></span> <span data-ttu-id="27c23-180">眼筋は、人間の身体の中で最も速く反応する筋肉です。</span><span class="sxs-lookup"><span data-stu-id="27c23-180">The eye muscle is the fastest reacting muscle in our body.</span></span> 

- <span data-ttu-id="27c23-181">**低労力。**</span><span class="sxs-lookup"><span data-stu-id="27c23-181">**Low effort.**</span></span> <span data-ttu-id="27c23-182">身体の動きがほとんど必要ありません。</span><span class="sxs-lookup"><span data-stu-id="27c23-182">Barely any physical movements are necessary.</span></span> 

- <span data-ttu-id="27c23-183">**暗黙。**</span><span class="sxs-lookup"><span data-stu-id="27c23-183">**Implicitness.**</span></span> <span data-ttu-id="27c23-184">多くの場合、ユーザーによって "覚えている" ことが示されます。ユーザーの目の動きに関する情報によって、ユーザーがどのターゲットに参加するかをシステムが把握できるようになります。</span><span class="sxs-lookup"><span data-stu-id="27c23-184">Often described by users as "mind reading", information about a user's eye movements lets the system know which target the user plans to engage.</span></span> 

- <span data-ttu-id="27c23-185">**代替入力チャネル。**</span><span class="sxs-lookup"><span data-stu-id="27c23-185">**Alternative input channel.**</span></span> <span data-ttu-id="27c23-186">視線を使用すると、ユーザーからの手動による調整に基づく長年の経験に基づいて、手書き入力や音声入力に対する強力なサポート入力を提供できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-186">Eye-gaze can provide a powerful supporting input for hand and voice input building on years of experience from users based on their hand-eye coordination.</span></span>

- <span data-ttu-id="27c23-187">**視覚的注意。**</span><span class="sxs-lookup"><span data-stu-id="27c23-187">**Visual attention.**</span></span> <span data-ttu-id="27c23-188">もう1つの重要な利点は、ユーザーがどのようなことに注目しているかを推測できることです。</span><span class="sxs-lookup"><span data-stu-id="27c23-188">Another important benefit is the possibility to infer what a user is paying attention to.</span></span> <span data-ttu-id="27c23-189">これは、さまざまなアプリケーション領域において、より効率的なユーザーインターフェイスとリモート通信のためのソーシャルキューの強化を効果的に評価できるようにするために役立ちます。</span><span class="sxs-lookup"><span data-stu-id="27c23-189">This can help in various application areas ranging from more effectively evaluating different designs to aiding in smarter user interfaces and enhanced social cues for remote communication.</span></span>

<span data-ttu-id="27c23-190">簡単に言うと、視線を入力として使用することで、高速で簡単なコンテキスト信号を実現できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-190">In a nutshell, using eye-gaze as an input offers a fast and effortless contextual signal.</span></span> <span data-ttu-id="27c23-191">これは、*音声*入力や*手動*入力などの他の入力と組み合わせてユーザーの意図を確認する場合に特に強力です。</span><span class="sxs-lookup"><span data-stu-id="27c23-191">This is particularly powerful when combined with other inputs such as *voice* and *manual* input to confirm the user's intent.</span></span>


### <a name="challenges-of-eye-gaze-as-an-input"></a><span data-ttu-id="27c23-192">入力としての目を見つめた課題</span><span class="sxs-lookup"><span data-stu-id="27c23-192">Challenges of eye-gaze as an input</span></span>
<span data-ttu-id="27c23-193">多くの場合、には多くの責任があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-193">With lots of power, comes lots of responsibility.</span></span>
<span data-ttu-id="27c23-194">Thata を使用して、ユーザーエクスペリエンスの満足度を向上させることができます。スーパーヒーローのような感覚を持っているので、適切に対応していないことを把握しておくことも重要です。</span><span class="sxs-lookup"><span data-stu-id="27c23-194">While eye-gaze can be used to create satisfying user experiences thata makes you feel like a superhero, it is also important to know what it is not good at to appropriately account for this.</span></span> <span data-ttu-id="27c23-195">以下では、視線入力を扱うときの対処方法について考慮する必要があるいくつかの*課題*について説明します。</span><span class="sxs-lookup"><span data-stu-id="27c23-195">The following discusses some *challenges* to take into account as well as how to address them when working with eye-gaze input:</span></span> 

- <span data-ttu-id="27c23-196">**視線が "常時オン" になってい**ます。目の蓋を開くと、環境内での作業が開始されます。</span><span class="sxs-lookup"><span data-stu-id="27c23-196">**Your eye-gaze is "always on"** The moment you open your eye lids, your eyes start fixating on things in the environment.</span></span> <span data-ttu-id="27c23-197">何かが長すぎると結果が得られない場合は、どのような外観にしても、アクションを誤って発行することになります。</span><span class="sxs-lookup"><span data-stu-id="27c23-197">Reacting to every look you make and accidentally issuing actions because you looked at something for too long would result in an unsatisfying experience.</span></span>
<span data-ttu-id="27c23-198">このため、ターゲットの選択をトリガーするために、*音声コマンド*、*ハンドジェスチャ*、*ボタンクリック*、または拡張熟考を使用して視線を組み合わせることをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="27c23-198">This is why we recommend combining eye-gaze with a *voice command*, *hand gesture*, *button click* or extended dwell to trigger the selection of a target.</span></span>
<span data-ttu-id="27c23-199">また、このソリューションでは、involuntarily によって何かをトリガーすることなく、ユーザーが自由に検索できるモードを使用することもできます。</span><span class="sxs-lookup"><span data-stu-id="27c23-199">This solution also allows for a mode in which the user can freely look around without being overwhelmed by involuntarily triggering something.</span></span> <span data-ttu-id="27c23-200">ターゲットを見ているだけの場合の視覚的フィードバックと聴覚的フィードバックを設計するときにも、この問題を考慮する必要があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-200">This issue should also be taken into account when designing visual and auditory feedback when merely looking at a target.</span></span>
<span data-ttu-id="27c23-201">一瞬のポップアウト効果やホバー音でユーザーを混乱させないようにしてください。</span><span class="sxs-lookup"><span data-stu-id="27c23-201">Do not overwhelm the user with immediate pop-out effects or hover sounds.</span></span> <span data-ttu-id="27c23-202">はらみはキーです。</span><span class="sxs-lookup"><span data-stu-id="27c23-202">Subtlety is key.</span></span> <span data-ttu-id="27c23-203">後で、設計推奨事項について説明するときに、これに関するベスト プラクティスについても説明します。</span><span class="sxs-lookup"><span data-stu-id="27c23-203">We will discuss some best practices for this further below when talking about design recommendations.</span></span>

- <span data-ttu-id="27c23-204">**監視と制御**たとえば、壁の写真を正確にためるとします。</span><span class="sxs-lookup"><span data-stu-id="27c23-204">**Observation vs. control** Imagine that you want to precisely straighten a photograph on your wall.</span></span> <span data-ttu-id="27c23-205">写真の縁と周囲を見て、揃っているかどうかを確認します。</span><span class="sxs-lookup"><span data-stu-id="27c23-205">You look at its borders and its surroundings to see if it aligns well.</span></span> <span data-ttu-id="27c23-206">次に、画像を移動するための入力として目を見つめて使用する方法を考えてみましょう。</span><span class="sxs-lookup"><span data-stu-id="27c23-206">Now imagine how you would do that when you want to use your eye-gaze as an input to move the picture.</span></span> <span data-ttu-id="27c23-207">難しいのではないでしょうか。</span><span class="sxs-lookup"><span data-stu-id="27c23-207">Difficult, isn't it?</span></span> <span data-ttu-id="27c23-208">これは、入力と制御の両方に必要な場合に、視線の2つの役割について説明します。</span><span class="sxs-lookup"><span data-stu-id="27c23-208">This describes the double role of eye-gaze when it is required both for input and control.</span></span> 

- <span data-ttu-id="27c23-209">**クリックする前に離れる:** クイックターゲットを選択する場合は、ユーザーの目を見つめていて、手動でクリックする前 (たとえば、放映タップ) に移動できることが調査によって示されています。</span><span class="sxs-lookup"><span data-stu-id="27c23-209">**Leave before click:** For quick target selections, research has shown that a user's eye-gaze can move on before concluding a manual click (e.g., an airtap).</span></span> <span data-ttu-id="27c23-210">そのため、より低速なコントロール入力 (音声、ハンド、コントローラーなど) を使用して、高速の視線信号を同期するには、特別な注意が必要です。</span><span class="sxs-lookup"><span data-stu-id="27c23-210">Hence, special attention must be paid to synchronizing the fast eye-gaze signal with slower control input (e.g., voice, hands, controller).</span></span>

- <span data-ttu-id="27c23-211">**小さいターゲット:** 読みやすくするために少しすぎてテキストを読まようとすると、感じていることがわかりますか。</span><span class="sxs-lookup"><span data-stu-id="27c23-211">**Small targets:** Do you know the feeling when you try to read text that is just a bit too small to read comfortable?</span></span> <span data-ttu-id="27c23-212">これによって目が疲れてしまうことがあります。目を絞って再調整しようとしているため、疲れてしまうことがあります。</span><span class="sxs-lookup"><span data-stu-id="27c23-212">This straining feeling on your eyes can cause you to feel tired and worn out because you try to readjust your eyes to focus better.</span></span>
<span data-ttu-id="27c23-213">これは、ユーザーが視点を使用してアプリケーションで小さすぎるターゲットを選択する必要がある場合に、ユーザーに対して呼び出す可能性があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-213">This is a feeling you might invoke in your users when forcing them to select targets that are too small in your application using eye targeting.</span></span>
<span data-ttu-id="27c23-214">設計に際しては、ユーザーにとって楽しく快適なエクスペリエンスを作るため、ターゲットを視角で 2 度以上にする (さらに大きい方が好ましい) ことをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="27c23-214">For your design, to create a pleasant and comfortable experience for your users, we recommend that targets should be at least 2° in visual angle, preferably larger.</span></span>

- <span data-ttu-id="27c23-215">**不規則な視点の移動**この目では、固定から固定への迅速な移動を行います。</span><span class="sxs-lookup"><span data-stu-id="27c23-215">**Ragged eye-gaze movements** Our eyes perform rapid movements from fixation to fixation.</span></span> <span data-ttu-id="27c23-216">記録された目の動きのスキャン パスを見ると、視線が不規則に動いていることがわかります。</span><span class="sxs-lookup"><span data-stu-id="27c23-216">If you look at scan paths of recorded eye movements, you can see that they look ragged.</span></span> <span data-ttu-id="27c23-217">*頭の視線入力*や*手の動き*に比べて、目の動きはすばやく、無意識のうちに突然動くことがあります。</span><span class="sxs-lookup"><span data-stu-id="27c23-217">Your eyes move quickly and in spontaneous jumps in comparison to *head gaze* or *hand motions*.</span></span>  

- <span data-ttu-id="27c23-218">**追跡の信頼性:** 視線追跡の精度は、目が新しい条件に合わせて調整したときのほんの少しの光の変化で低下する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-218">**Tracking reliability:** Eye tracking accuracy may degrade a little in changing light as your eye adjust to the new conditions.</span></span>
<span data-ttu-id="27c23-219">これは必ずしもアプリケーションの設計に影響を与えるわけではありませんが、精度は2°の制限内にあるため、ユーザーが別の調整を実行することが必要になる場合があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-219">While this should not necessarily affect your application design, as the accuracy should be within the 2° limitation, i might be necessary for the user to run another calibration.</span></span> 


## <a name="design-recommendations"></a><span data-ttu-id="27c23-220">設計の推奨事項</span><span class="sxs-lookup"><span data-stu-id="27c23-220">Design recommendations</span></span>
<span data-ttu-id="27c23-221">次に示すのは、目を見つめた入力の利点と課題に基づいた、設計に関する特定の推奨事項の一覧です。</span><span class="sxs-lookup"><span data-stu-id="27c23-221">The following is a list of specific design recommendations based on the described advantages and challenges for eye-gaze input:</span></span>

1. <span data-ttu-id="27c23-222">**視線は、ヘッドを見つめます。**</span><span class="sxs-lookup"><span data-stu-id="27c23-222">**Eye-gaze != Head-gaze:**</span></span>
    - <span data-ttu-id="27c23-223">**高速だが不規則な目の動きが入力タスクに合っているかどうかを検討する:** 私たちの視野 (視界) を通じてターゲットをすばやく選択することによって、高速で不規則な視点が非常に優れていますが、smooth input 軌道 (描画や encircling の注釈など) を必要とするタスクには適用できません。</span><span class="sxs-lookup"><span data-stu-id="27c23-223">**Consider whether fast yet ragged eye movements fit your input task:** While our fast and ragged eye movements are great at quickly selecting targets across our field of view (FoV), it is less applicable for tasks that require smooth input trajectories (e.g., drawing or encircling annotations).</span></span> <span data-ttu-id="27c23-224">この場合は、手または頭によるポインティングをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="27c23-224">In this case, hand or head pointing should be preferred.</span></span>
  
    - <span data-ttu-id="27c23-225">**ユーザーの目を見つめた (スライダーやカーソルなど) に直接接続することは避けてください。**</span><span class="sxs-lookup"><span data-stu-id="27c23-225">**Avoid attaching something directly to the user’s eye-gaze (e.g., a slider or cursor).**</span></span>
<span data-ttu-id="27c23-226">カーソルの場合は、投影された視線の信号のオフセットがわずかであるため、"fleeing cursor" 効果が生じる可能性があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-226">In case of a cursor, this may result in the “fleeing cursor” effect due to slight offsets in the projected eye-gaze signal.</span></span> <span data-ttu-id="27c23-227">スライダーの場合は、スライダーを目で制御する2つの役割と競合し、オブジェクトが正しい位置にあるかどうかを確認することもできます。</span><span class="sxs-lookup"><span data-stu-id="27c23-227">In case of a slider, it can conflict with the double role of controlling the slider with your eyes while also wanting to check whether the object is at the correct location.</span></span> <span data-ttu-id="27c23-228">簡単に言うと、ユーザーにとっては特に、ユーザーの信号が不正確になる可能性があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-228">In a nutshell, users could become overwhelmed and distracted, especially if the signal is imprecise for that user.</span></span> 
  
2. <span data-ttu-id="27c23-229">**視線を他の入力と結合します。** ハンドジェスチャ、音声コマンド、ボタンの押下など、他の入力との視線追跡の統合には、いくつかの利点があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-229">**Combine eye-gaze with other inputs:** The integration of eye tracking with other inputs, such as hand gestures, voice commands or button presses, serves several advantages:</span></span>
    - <span data-ttu-id="27c23-230">**自由観察を可能にする:** 私たちの主要な役割は、環境を観察することであるということです。重要なのは、ユーザーが何も (視覚、聴覚などの) フィードバックまたはアクションをトリガーすることなく検索できることです。</span><span class="sxs-lookup"><span data-stu-id="27c23-230">**Allow for free observation:** Given that the main role of our eyes is to observe our environment, it is important users are allowed to look around without triggering any (visual, auditory, etc.) feedback or actions.</span></span> 
    <span data-ttu-id="27c23-231">視線追跡と別の入力コントロールを組み合わせることにより、視線監視と入力制御モードの間でスムーズに遷移できます。</span><span class="sxs-lookup"><span data-stu-id="27c23-231">Combining eye tracking with another input control allows smooth transitioning between eye tracking observation and input control modes.</span></span>
  
    - <span data-ttu-id="27c23-232">**強力なコンテキスト プロバイダー:** 音声コマンドを uttering したり、手の形でジェスチャを実行したりしているときに、ユーザーがどこで見ているかについての情報を使用することにより、ビューのフィールド間でシームレスに入力を channeling ことができます。</span><span class="sxs-lookup"><span data-stu-id="27c23-232">**Powerful context provider:** Using information about where and what the user is looking at while uttering a voice command or performing a hand gesture allows seamlessly channeling the input across the field-of-view.</span></span> <span data-ttu-id="27c23-233">以下に例を示します。「Put that there (それをあそこに置いて)」と言うときに、単にターゲットと宛先を見るだけで、シーンの中でホログラムを選択して配置する作業を迅速かつ滑らかに行えます。</span><span class="sxs-lookup"><span data-stu-id="27c23-233">For example: “Put that there” to quickly and fluently select and position a hologram across the scene by simply looking at a target and destination.</span></span> 

    - <span data-ttu-id="27c23-234">**マルチモーダル入力を同期する必要性 (「クリック前に離れる」問題):** 長い音声コマンドやハンドジェスチャなど、より複雑な追加入力を使用して迅速な移動を組み合わせることにより、追加の入力コマンドを完了する前に、目を見つめていくリスクが高くなります。</span><span class="sxs-lookup"><span data-stu-id="27c23-234">**Need for synchronizing multimodal inputs (“leave before click” issue):** Combining rapid eye movements with more complex additional inputs, such as long voice commands or hand gestures, bears the risk of continuing your eye-gaze before finishing the additional input command.</span></span> <span data-ttu-id="27c23-235">そのため、独自の入力コントロール (手のカスタム ジェスチャなど) を作成する場合は、この入力の開始またはおおよその持続時間をログに記録して、それをユーザーが前に凝視していたものと関連付けてください。</span><span class="sxs-lookup"><span data-stu-id="27c23-235">Hence, if you create your own input controls (e.g., custom hand gestures), make sure to log the onset of this input or approximate duration to correlate it with what a user had fixated on in the past.</span></span>
    
3. <span data-ttu-id="27c23-236">**視線追跡入力の繊細なフィードバック:** システムが意図したとおりに動作していることを示すためにターゲットを検索するときに、フィードバックを提供すると便利ですが、微妙に保つ必要があります。</span><span class="sxs-lookup"><span data-stu-id="27c23-236">**Subtle feedback for eye tracking input:** It's useful to provide feedback when a target is looked at to indicate that the system is working as intended, but should be kept subtle.</span></span> <span data-ttu-id="27c23-237">これには、徐々にブレンドを行う、視覚的に強調表示する、またはターゲットを少し増やして、ターゲットを見ていることをシステムが正しく検出したことをシステムが検出したことを示すその他の微妙なターゲット動作 (ユーザーの現在のワークフローを不必要に中断しています。</span><span class="sxs-lookup"><span data-stu-id="27c23-237">This can include slowly blending, in and out, visual highlights or perform other subtle target behaviors, such as slow motions, such as slightly increasing the target, to indicate that the system correctly detected that the user is looking at a target without unnecessarily interrupting the user’s current workflow.</span></span> 

4. <span data-ttu-id="27c23-238">**入力としての不自然な目の動きを強制しない:** アプリケーションのアクションをトリガーするために、ユーザーが特定の目の動き (宝石のジェスチャ) を実行しないようにします。</span><span class="sxs-lookup"><span data-stu-id="27c23-238">**Avoid enforcing unnatural eye movements as input:** Do not force users to perform specific eye movements (gaze gestures) to trigger actions in your application.</span></span>

5. <span data-ttu-id="27c23-239">**不正確さを考慮に入れる:** ユーザーにとってわかりやすく、オフセットとジッターという2種類の不正確性を区別します。</span><span class="sxs-lookup"><span data-stu-id="27c23-239">**Account for imprecisions:** We distinguish two types of imprecisions which are noticeable to users: offset and jitter.</span></span> <span data-ttu-id="27c23-240">オフセットに対処する最も簡単な方法は、操作に十分なサイズのターゲットを提供することです。</span><span class="sxs-lookup"><span data-stu-id="27c23-240">The easiest way to address an offset is to provide sufficiently large targets to interact with.</span></span> <span data-ttu-id="27c23-241">2°を超える視覚的な角度を参照として使用することをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="27c23-241">It is suggested that you use a visual angle greater than 2° as a reference.</span></span> <span data-ttu-id="27c23-242">たとえば、arm を拡張すると、サムネイルの表示角度が約2°になります。</span><span class="sxs-lookup"><span data-stu-id="27c23-242">For instance, your thumbnail is about 2° in visual angle when you stretch out your arm.</span></span> <span data-ttu-id="27c23-243">これが次のガイダンスにつながります。</span><span class="sxs-lookup"><span data-stu-id="27c23-243">This leads to the following guidance:</span></span>
    - <span data-ttu-id="27c23-244">ユーザーが小さいターゲットを選択することを強制しないでください。</span><span class="sxs-lookup"><span data-stu-id="27c23-244">Do not force users to select tiny targets.</span></span> <span data-ttu-id="27c23-245">ここでは、ターゲットが十分に大きく、システムが適切に設計されている場合に、ユーザーの対話を簡単で魔法のない方法で記述しています。</span><span class="sxs-lookup"><span data-stu-id="27c23-245">Research has shown that if targets are sufficiently large, and that the system is designed well, users describe their interactions as effortless and magical.</span></span> <span data-ttu-id="27c23-246">ターゲットを小さくしすぎると、ユーザーはエクスペリエンスを疲れる、いらだたしいものと表現します。</span><span class="sxs-lookup"><span data-stu-id="27c23-246">If targets become too small, users describe the experience as fatiguing and frustrating.</span></span>
   

## <a name="see-also"></a><span data-ttu-id="27c23-247">関連項目</span><span class="sxs-lookup"><span data-stu-id="27c23-247">See also</span></span>
* [<span data-ttu-id="27c23-248">頭の視線入力とコミット</span><span class="sxs-lookup"><span data-stu-id="27c23-248">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="27c23-249">DirectX での頭と視線</span><span class="sxs-lookup"><span data-stu-id="27c23-249">Head and eye-gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="27c23-250">Unity での視線 (Mixed Reality Toolkit)</span><span class="sxs-lookup"><span data-stu-id="27c23-250">Eye-gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="27c23-251">手のジェスチャ</span><span class="sxs-lookup"><span data-stu-id="27c23-251">Hand gestures</span></span>](gestures.md)
* [<span data-ttu-id="27c23-252">音声入力</span><span class="sxs-lookup"><span data-stu-id="27c23-252">Voice input</span></span>](voice-design.md)
* [<span data-ttu-id="27c23-253">モーション コントローラー</span><span class="sxs-lookup"><span data-stu-id="27c23-253">Motion controllers</span></span>](motion-controllers.md)
* [<span data-ttu-id="27c23-254">快適性</span><span class="sxs-lookup"><span data-stu-id="27c23-254">Comfort</span></span>](comfort.md)
