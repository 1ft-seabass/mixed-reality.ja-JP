---
title: 視線追跡
description: 視線追跡
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: 視線追跡, Mixed Reality, 入力, 目の視線入力
ms.openlocfilehash: 7298a34a946f86aaf789cfe44ad971169fc8ece3
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 06/05/2019
ms.locfileid: "66453702"
---
# <a name="eye-tracking-on-hololens-2"></a>HoloLens 2 上の視線追跡
HoloLens 2 を使用すると、開発者はユーザーが見ているものの情報を活用する素晴らしい能力を持つことができ、ホログラフィック エクスペリエンスにおけるコンテキストと人間の理解が大きく進みます。 このページでは、開発者がさまざまな使用事例で視線追跡から得られるメリットと、目の視線入力ベースのユーザー インターフェイスを設計する際に注意すべき点について概要を示します。 

## <a name="use-cases"></a>使用事例
視線追跡を使用すれば、アプリケーションは、ユーザーが見ている場所をリアルタイムで追跡できます。 このセクションでは、Mixed Reality で視線追跡を使用することで可能になる、潜在的な使用事例や新しい相互作用について説明します。
作業を始める前に、この先の部分では [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) に何度か言及することをお知らせしておきます。これは、視線追跡を使用する興味深く強力な例がいくつか付属しているためです。たとえば、視線を利用して迅速かつ楽にターゲットを選択することや、ユーザーが見ているものに基づいて自動的にテキストをスクロールすることなどです。 

### <a name="user-intent"></a>ユーザー意図    
ユーザーがどこを見ているかに関する情報は、音声、手、コントローラーなど、**他の入力のための強力なコンテキスト**となります。
これは、さまざまなタスクに利用できます。
その範囲としては、たとえば、ホログラムを見て「選択」と言うだけでシーンの中から迅速かつ楽に**ターゲット設定**することや (「[頭の視線入力とコミット](gaze-and-commit.md)」も参照)、「put this... (これを置いて)」と言ってからそのホログラムを配置したい場所を見渡して「...there (あそこへ)」という言うことなどがあります。 この例は、「[Mixed Reality Toolkit - Eye-supported Target Selection (Mixed Reality Toolkit - 目で支援するターゲット選択)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)」と「[Mixed Reality Toolkit - Eye-supported Target Positioning (Mixed Reality Toolkit - 目で支援するターゲット配置)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)」に記載されています。

ユーザー意図に関する別の例としては、具現化された仮想エージェントや対話型ホログラムとのやり取りを強化するために、ユーザーが何を見ているかについての情報を使う、というものがあります。 たとえば、現在視線が向けられているコンテンツに基づき、仮想エージェントは使用可能なオプションや動作を変化させることができます。 

### <a name="implicit-actions"></a>暗黙的アクション
暗黙的アクションのカテゴリは、ユーザー意図に密接に関係しています。
この考え方は、ホログラムやユーザー インターフェイス要素がいくらか本能的とも言える仕方で反応するので、システムと対話しているという感覚さえまったく与えることがなく、それでいてシステムとユーザーの間には協調関係がある、というものです。たとえば、大成功した 1 つの例が**目の視線入力ベースの自動スクロール**です。 考え方は次のように単純です。ユーザーはテキストを読み、ひたすら読み続けることができます。 テキストは、ユーザーの読む速度に合わせて少しずつ上に移動します。 重要な側面は、スクロール速度がユーザーの読む速度に適応していることです。
別の例は、まさに自分が焦点を合わせているものに向かって飛び込むような印象をユーザーに与える、**視線の支援を使ったズームとパン**です。 ズームのトリガーやズーム速度の管理は、音声や手の入力を使用して制御できます。これは、制御できているという感覚を与えたり、ユーザーの混乱を避けたりする面で重要です (詳細については、後述の設計ガイドラインを参照してください)。 ズームインすれば、ユーザーは、目の視線入力を使用するだけで、たとえば、通り沿いの道を滑らかにたどって近所を探索することができます。
この種の相互作用に関するデモの例は、「[Mixed Reality Toolkit - Eye-supported Navigation (Mixed Reality Toolkit - 目で支援するナビゲーション)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)」のサンプルを参照してください。

その他の_暗黙的アクション_の使用事例を以下に示します。
- **スマート通知:** 注目していた場所に通知がポップアップ表示されて不快に思ったことはありませんか? ユーザーが現在注目している場所を考慮すれば、改善できます。 気を散らさないように、ユーザーが現在見ている場所を避けて通知を表示し、読み終わったら自動的に消去します。 
- **気が利くホログラム:** 見られているときに繊細に反応するホログラム。 これには、UI 要素の輝度をわずかに上げる、ゆっくり花が咲くといったものから、ユーザーの方を振り返る仮想ペットや、長い凝視の後には目の視線入力を避けることまでが含まれます。 これにより、つながることの面白味や満足感をアプリに盛り込むことができます。

### <a name="attention-tracking"></a>注意追跡   
ユーザーが見ている場所に関する情報は、デザインの有用性を評価し、効率的な作業の流れを阻害している問題を特定するための非常に強力なツールです。 既に、視線追跡の可視化と分析は、さまざまな適用分野で広く採用されています。 HoloLens 2 を使用すれば、3D ホログラムを実際のコンテキスト内に配置して、同時に評価できるため、この認識に新たな次元が加えられます。 [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) には、視線追跡データの記録および読み込みとそれらの可視化方法に関する基本的な例が付属しています。

この分野の他の適用方法を以下に示します。 
-   **リモートの目の視線入力の可視化:** たとえば、リモート コラボレーターが見ているものを可視化して、手順が正しく理解され、実行されているかどうかを確認します。
-   **ユーザー調査研究:** 注意追跡は、初級ユーザーと上級ユーザーがコンテンツを視覚的に分析する方法や、複雑な作業における手と目の協調 (医療データの分析や機械の操作など) を探求するために使用できます。
-   **トレーニング シミュレーションとパフォーマンス監視:** 実行フローにおけるボトルネックをより効率的に特定することにより、タスクの実行を練習して最適化します。
-   **デザイン評価、宣伝、市場調査:** 視線追跡は、Web サイトや製品のデザインを評価するための市場調査用の一般的なツールです。

### <a name="additional-use-cases"></a>その他の使用事例
- **ゲーム:** スーパーパワーが欲しくなったことはありませんか? こつをお教えしましょう。 ホログラムを凝視して浮き上がらせます。 目からレーザー ビームを発射します。 敵を石に変えたり、凍らせたりします。 透視能力を使ってビルを探索します。 使い道は想像力次第です。  

- **表情豊かなアバター:** 視線追跡のライブ データを使用してユーザーが現在見ているものを示すようにアバターの目をアニメーション化すれば、視線追跡は 3D アバターの表情を豊かにするのに役立ちます。 また、ウインクとまばたきを追加すると、表現力が増します。 

- **テキスト入力:** 視線追跡は、特に声や手が使えない場合の低労力のテキスト入力の興味深い代替手段として使用できます。 


## <a name="eye-tracking-api"></a>Eye Tracking API
目の視線入力の相互作用に関する特定の設計ガイドラインの詳細に入る前に、HoloLens 2 Eye Tracker が提供している機能について簡単に説明します。 [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) は `Windows.Perception.People.EyesPose` 経由でアクセスできます。 この API は、目の視線入力の 1 本の光線 (視線入力の原点と方向) を開発者に提供します。
アイ トラッカーは、約 _30 FPS_ でデータを提供します。
予測される目の視線入力は、 実際に見ているターゲットを中心とした視角で約 1.0 - 1.5 度以内に収まっています。 わずかなずれが想定されるため、この下限値のマージンを考慮する必要があります。 詳細については、後述します。 視線追跡が正しく機能するためには、各ユーザーが視線追跡ユーザー調整を行う必要があります。 

![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)<br>
*2 m の距離での最適なターゲット サイズ*


## <a name="eye-gaze-design-guidelines"></a>目の視線入力設計ガイドライン
高速に動く目のターゲット設定を利用した対話を構築することは、やりがいのあるものとなり得ます。 このセクションでは、アプリの設計時に考慮すべき主なメリットと課題をまとめます。 

### <a name="benefits-of-eye-gaze-input"></a>目の視線入力で入力を行うメリット
- **高速ポインティング。** 眼筋は、人間の身体の中で最も速く反応する筋肉です。 

- **低労力。** 身体の動きがほとんど必要ありません。 

- **暗黙。** ユーザーから「読心術」と言われることもしばしばであり、ユーザーの目の動きの情報から、ユーザーが操作しようと思っているターゲットがシステムに伝わります。 

- **代替入力チャネル。** 目の視線入力は、手と音声による入力を強力に支援できます。これは、手と目の協調を基にしたユーザーの長年の経験に立脚したものです。

- **視覚的注意。** もう 1 つの重要なメリットは、ユーザーが注目しているものを推測できる可能性です。 これはさまざまな適用分野に役立つ可能性があり、その範囲は、さまざまなデザインを効果的に評価することから、より賢いユーザー インターフェイスや遠隔コミュニケーションのための拡大された社会的手掛かりを支援することにまで及びます。

簡単に言えば、目の視線入力を入力として使用することにより、高速で簡単なコンテキスト信号を提供できる可能性があります。これは、特に、*音声*入力や*手*入力などの他の入力と組み合わせてユーザーの意図を確認する場合に効果を発揮します。


### <a name="challenges-of-eye-gaze-as-an-input"></a>入力としての目の視線入力の課題
能力が高いほど、責任も重くなります。目の視線入力を使用すれば、スーパーヒーローになったような魅力的なユーザー エクスペリエンスを創造できますが、その欠点について知っておくことも、正しく説明する責任を果たす上で重要です。 ここでは、目の視線入力による入力を使用して作業する場合に考慮すべき*課題*とその解決方法について説明します。 

- **目の視線入力は「常時オン」** まぶたを開いた瞬間に、目は環境内の物の凝視を開始します。 見たものすべてに反応したり、長く見続けたせいで偶発的にアクションが発行されたりすることによって、予想外のことが起きる場合があります。
これが、目の視線入力と、*音声コマンド*、*手のジェスチャ*、*ボタン クリック*、または長期間のドウェルを組み合わせてターゲットの選択をトリガーすることが推奨されている理由です。
この解決策は、ユーザーが無意識に何かをトリガーして混乱を覚えることなく、自由に周りを見回すことができるモードも可能にします。 ターゲットを見ているだけの場合の視覚的フィードバックと聴覚的フィードバックを設計するときにも、この問題を考慮する必要があります。
一瞬のポップアウト効果やホバー音でユーザーを混乱させないようにしてください。 繊細さが重要です。 後で、設計推奨事項について説明するときに、これに関するベスト プラクティスについても説明します。

- **観察と制御** 壁に写真をきれいに並べたい場合を想像してください。 写真の縁と周囲を見て、揃っているかどうかを確認します。 今度は、それを行うのと同時に、写真を動かすための入力として自分の目の視線入力を用いようとする場面を想像してみてください。 難しいのではないでしょうか。 これは、目の視線入力が、入力と制御の両方に必要になると、その役割が二重になることを示します。 

- **クリックする前に離れる:** 調査によると、迅速なターゲット選択の場合、手動クリック (エアタップなど) が完了する前にユーザーの目の視線入力が動いてしまう可能性があることがわかりました。 そのため、低速のコントロール入力 (音声、手、コントローラーなど) と高速の目の視線入力信号の同期には特別な注意を払う必要があります。

- **小さいターゲット:** 小さすぎてよく見えないテキストを必死で読もうとするときの気持ちをご存知でしょうか。 この暗い気持ちのせいで、焦点を合わせようとして目を酷使することになり、疲れを感じたり、心が折れたりします。
これが、目によるターゲット設定を使用したアプリで小さすぎるターゲットを選択させたときにユーザーに与える可能性のある気持ちです。
設計に際しては、ユーザーにとって楽しく快適なエクスペリエンスを作るため、ターゲットを視角で 2 度以上にする (さらに大きい方が好ましい) ことをお勧めします。

- **不規則な目の視線入力の動き** 人間の目はすばやく移動し、あらゆるものを次々に凝視します。 記録された目の動きのスキャン パスを見ると、視線が不規則に動いていることがわかります。 *頭の視線入力*や*手の動き*に比べて、目の動きはすばやく、無意識のうちに突然動くことがあります。  

- **追跡の信頼性:** 視線追跡の精度は、目が新しい条件に合わせて調整したときのほんの少しの光の変化で低下する可能性があります。
これはアプリの設計に必ずしも影響しませんが、精度は前述の 2 度の制限内にする必要があります。 これは、ユーザーがもう一度調整を行わなければならない場合もあるということです。 


### <a name="design-recommendations"></a>設計の推奨事項
ここでは、説明した目の視線入力による入力のメリットと課題に基づき、設計に関する特定の推奨事項を一覧表示します。

1. **目の視線入力 != 頭の視線入力:**
    - **高速だが不規則な目の動きが入力タスクに合っているかどうかを検討する:** 高速で不規則な目の動きは視界全体から迅速にターゲット選択する場合に適していますが、滑らかな入力軌道が必要なタスク (描画や注釈を囲むなど) にはあまり適していません。 この場合は、手または頭によるポインティングをお勧めします。
  
    - **ユーザーの目の視線入力に何か (スライダーやカーソルなど) を直接に取り付けるのは避けてください。**
カーソルの場合は、投影された目の視線入力信号のわずかなオフセットのせいで「逃げるカーソル」効果が発生する可能性があります。 スライダーの場合は、目でスライダーを制御しながら、対象物が正しい位置にあるかどうかも確認したいという二重の役割に抵触します。 簡単に言えば、ユーザーは、特に自分に関する信号が不正確な場合、たちまち混乱したり、集中できなくなったりする可能性があります。 
  
2. **目の視線入力と他の入力を組み合わせる:** 視線追跡と、手のジェスチャ、音声コマンド、ボタン押下などの他の入力との統合には、いくつかのメリットがあります。
    - **自由観察を可能にする:** 目の主要な役割が環境を観察することだとすると、ユーザーが (視覚や聴覚などの) フィードバックやアクションをトリガーすることなく周囲を見回すことができるようにすることが重要です。 
    別の入力コントロールと ET を組み合わせることにより、ET 観察モードと入力コントロール モード間のスムーズな遷移が可能になります。
  
    - **強力なコンテキスト プロバイダー:** 音声コマンドを発音したり、手のジェスチャを実行したりするときに、ユーザーが見ている場所に関する情報を使用すれば、視界全体での入力のチャネル処理が楽に行えます。 たとえば、次のものがあります。「Put that there (それをあそこに置いて)」と言うときに、単にターゲットと宛先を見るだけで、シーンの中でホログラムを選択して配置する作業を迅速かつ滑らかに行えます。 

    - **マルチモーダル入力を同期する必要性 (「クリック前に離れる」問題):** 迅速な目の動きと、より複雑な追加入力 (長い音声コマンドや手のジェスチャなど) を組み合わせると、追加の入力コマンドが完了する前に目の視線入力が動いてしまう可能性があります。 そのため、独自の入力コントロール (手のカスタム ジェスチャなど) を作成する場合は、この入力の開始またはおおよその持続時間をログに記録して、それをユーザーが前に凝視していたものと関連付けてください。
    
3. **視線追跡入力の繊細なフィードバック:** ターゲットが見つめられたときにフィードバックを提供するのは有用ですが (システムが意図どおりに機能していることを示すため)、繊細さを保つ必要があります。 これにはビジュアル ハイライトのゆっくりしたブレンドインまたはブレンドアウトや、他の繊細なターゲットの動作が含まれます。たとえば、低速モーション (ターゲットが若干大きくなる) などを使って、ユーザーがターゲットを見つめたことがシステムによって正しく検出されたことを示します。しかし、ユーザーの現在のワークフローを不必要に中断させることは避けます。 

4. **入力としての不自然な目の動きを強制しない:** アプリ内のアクションをトリガーするために特定の目の動き (視線入力ジェスチャ) を実行するようにユーザーに強制しないでください。

5. **不正確さを考慮に入れる:** ユーザーに容易に気付かれる 2 種類の不正確さが特定されています。オフセットとジッターです。 オフセットを解消する簡単な方法は、操作する対象として十分大きいターゲットを用意することです (視角で 2 度以上 – 参考までにお知らせすると、腕を伸ばしたとき、親指の爪の視角は約 2 度です (1))。 これが次のガイダンスにつながります。
    - 小さいターゲットを選択するようにユーザーに強制しないでください。調査では、ターゲットが十分大きい (かつシステムが適切に設計されている) 場合は、ユーザーが操作が簡単で魔法のようだと答えていることがわかりました。 ターゲットを小さくしすぎると、ユーザーはエクスペリエンスを疲れる、いらだたしいものと表現します。
   

## <a name="see-also"></a>関連項目
* [頭の視線入力とコミット](gaze-and-commit.md)
* [DirectX でのヘッド視線入力とアイ視線入力](gaze-in-directx.md)
* [Unity (Mixed Reality Toolkit) の目の視線入力](https://aka.ms/mrtk-eyes)
* [手のジェスチャ](gestures.md)
* [音声入力](voice-design.md)
* [モーション コントローラー](motion-controllers.md)
* [快適性](comfort.md)
