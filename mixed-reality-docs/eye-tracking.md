---
title: 視線追跡
description: HoloLens 2 を使用すると、開発者はユーザーが見ているものについての情報を使用できるので、ホログラフィック エクスペリエンスにおけるコンテキストと人間の理解が大きく進みます。
author: sostel
ms.author: sostel
ms.date: 10/29/2019
ms.topic: article
keywords: 視線追跡、mixed reality、インプット、視線、調整
ms.openlocfilehash: 60de5ceb9f55ca7e2f74856af9bd75567763e382
ms.sourcegitcommit: a5dc182da237f63f0487d40a2e11894027208b6c
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/02/2019
ms.locfileid: "73441113"
---
# <a name="eye-tracking-on-hololens-2"></a><span data-ttu-id="223c6-104">HoloLens 2 上の視線追跡</span><span class="sxs-lookup"><span data-stu-id="223c6-104">Eye tracking on HoloLens 2</span></span>

![MRTK の視線追跡デモ](images/mrtk_et_scenemenu.jpg)

<span data-ttu-id="223c6-106">HoloLens 2 を使用すると、開発者はユーザーが見ているものについての情報を使用できるので、ホログラフィック エクスペリエンスにおけるコンテキストと人間の理解が大きく進みます。</span><span class="sxs-lookup"><span data-stu-id="223c6-106">HoloLens 2 allows for a new level of context and human understanding within the holographic experience by providing developers with the ability of using information about what users are looking at.</span></span> <span data-ttu-id="223c6-107">このページでは、開発者とデザイナーに対するこの新機能の概要を説明します。さまざまなユースケースや基本的な開発者ガイドの視線追跡によってどのようにメリットが得られるかについて説明します。</span><span class="sxs-lookup"><span data-stu-id="223c6-107">This page provides an overview of this new capability to developers and designers on how they can benefit from eye tracking for various use cases and basic developer guidance.</span></span> 


## <a name="calibration"></a><span data-ttu-id="223c6-108">目盛り</span><span class="sxs-lookup"><span data-stu-id="223c6-108">Calibration</span></span> 
<span data-ttu-id="223c6-109">視線追跡を正確に機能させるには、各ユーザーが、一連の holographic ターゲットを確認する必要がある、[目の追跡ユーザーの調整](calibration.md)を行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-109">For eye tracking to work accurately, each user is required to go through an [eye tracking user calibration](calibration.md) for which the user has to look at a set of holographic targets.</span></span> <span data-ttu-id="223c6-110">これにより、デバイスはシステムを調整して、より快適で品質の高い閲覧エクスペリエンスをユーザーに提供し、同時に正確な視点を追跡することができます。</span><span class="sxs-lookup"><span data-stu-id="223c6-110">This allows the device to adjust the system for a more comfortable and higher quality viewing experience for the user and to ensure accurate eye tracking at the same time.</span></span> <span data-ttu-id="223c6-111">視線追跡はほとんどのユーザーに対して機能しますが、ユーザーが正常に調整できない場合もまれにあります。</span><span class="sxs-lookup"><span data-stu-id="223c6-111">Eye tracking should work for most users, but there are rare cases in which a user might be unable to calibrate successfully.</span></span>
<span data-ttu-id="223c6-112">調整の詳細と、スムーズなエクスペリエンスを保証する方法の詳細については、「[ユーザーの調整](calibration.md)の監視」ページをご覧ください。</span><span class="sxs-lookup"><span data-stu-id="223c6-112">To learn more about the calibration and about how to ensure a smooth experience, please check our [eye tracking user calibration](calibration.md) page.</span></span>


## <a name="device-support"></a><span data-ttu-id="223c6-113">デバイスのサポート</span><span class="sxs-lookup"><span data-stu-id="223c6-113">Device support</span></span>
<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><span data-ttu-id="223c6-114"><strong>機能</strong></span><span class="sxs-lookup"><span data-stu-id="223c6-114"><strong>Feature</strong></span></span></td>
     <td><span data-ttu-id="223c6-115"><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></span><span class="sxs-lookup"><span data-stu-id="223c6-115"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
     <td><span data-ttu-id="223c6-116"><a href="https://docs.microsoft.com/hololens/hololens2-hardware"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="223c6-116"><a href="https://docs.microsoft.com/hololens/hololens2-hardware"><strong>HoloLens 2</strong></span></span></td>
     <td><span data-ttu-id="223c6-117"><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></span><span class="sxs-lookup"><span data-stu-id="223c6-117"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
</tr>
<tr>
     <td><span data-ttu-id="223c6-118">視線</span><span class="sxs-lookup"><span data-stu-id="223c6-118">Eye-gaze</span></span></td>
     <td>❌</td>
     <td><span data-ttu-id="223c6-119">✔️</span><span class="sxs-lookup"><span data-stu-id="223c6-119">✔️</span></span></td>
     <td>❌</td>
</tr>
</table>

## <a name="available-eye-tracking-data"></a><span data-ttu-id="223c6-120">利用可能なアイトラッキングデータ</span><span class="sxs-lookup"><span data-stu-id="223c6-120">Available eye tracking data</span></span>
<span data-ttu-id="223c6-121">目を見つめた入力の特定のユースケースについて詳しく説明する前に、HoloLens 2[目の追跡 API](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose)によって提供される機能について簡単に説明します。</span><span class="sxs-lookup"><span data-stu-id="223c6-121">Before going into detail about specific use cases for eye-gaze input, we want to briefly point out the capabilities that the HoloLens 2 [Eye Tracking API](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose) provides.</span></span> <span data-ttu-id="223c6-122">開発者は約_30 FPS (30 Hz)_ で、1つの目を見つめた射線 (宝石の出発点と向き) にアクセスできます。</span><span class="sxs-lookup"><span data-stu-id="223c6-122">Developers get access to a single eye-gaze ray (gaze origin and direction) at approximately _30 FPS (30 Hz)_.</span></span>
<span data-ttu-id="223c6-123">視線追跡データへのアクセス方法の詳細については、「開発者ガイド」を参照してください。 [DirectX での視線](gaze-in-directx.md)と、 [Unity で](https://aka.ms/mrtk-eyes)の視線の使用に関するガイドを参照してください。</span><span class="sxs-lookup"><span data-stu-id="223c6-123">For more detailed information about how to access eye tracking data, please refer to our developer guides on using [eye-gaze in DirectX](gaze-in-directx.md) and [eye-gaze in Unity](https://aka.ms/mrtk-eyes).</span></span>

<span data-ttu-id="223c6-124">予測された視線は、実際のターゲットを中心に約1.5 °の範囲で表示されます (次の図を参照してください)。</span><span class="sxs-lookup"><span data-stu-id="223c6-124">The predicted eye-gaze is approximately within 1.5 degrees in visual angle around the actual target (see the illustration below).</span></span> <span data-ttu-id="223c6-125">わずかな不正確性が想定されているため、開発者はこの下限値の周りにいくらかの余白を計画する必要があります (たとえば、2.0-3.0 度では、より快適なエクスペリエンスが得られる可能性があります)。</span><span class="sxs-lookup"><span data-stu-id="223c6-125">As slight imprecisions are expected, developers should plan for some margin around this lower bound value (e.g., 2.0-3.0 degrees may result in a much more comfortable experience).</span></span> <span data-ttu-id="223c6-126">以下では、小規模なターゲットの選択に対処する方法について説明します。</span><span class="sxs-lookup"><span data-stu-id="223c6-126">We will discuss how to address the selection of small targets in more detail below.</span></span> <span data-ttu-id="223c6-127">視線追跡が正しく機能するためには、各ユーザーが視線追跡ユーザー調整を行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-127">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="223c6-128">![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="223c6-128">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="223c6-129">*2メートル距離で最適なターゲットサイズ*</span><span class="sxs-lookup"><span data-stu-id="223c6-129">*Optimal target size at a 2-meter distance*</span></span>

<br>

## <a name="use-cases"></a><span data-ttu-id="223c6-130">使用事例</span><span class="sxs-lookup"><span data-stu-id="223c6-130">Use cases</span></span>
<span data-ttu-id="223c6-131">視線追跡を使用すれば、アプリケーションは、ユーザーが見ている場所をリアルタイムで追跡できます。</span><span class="sxs-lookup"><span data-stu-id="223c6-131">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="223c6-132">次のユースケースでは、2つの混合現実の HoloLens 2 での視線追跡によって可能な相互作用について説明します。</span><span class="sxs-lookup"><span data-stu-id="223c6-132">The following use cases describe some interactions that are possible with eye tracking on HoloLens 2 in mixed reality.</span></span>
<span data-ttu-id="223c6-133">これらのユースケースはまだ Holographic シェルエクスペリエンスの一部ではないことに注意してください (つまり、HoloLens 2 を起動したときに表示されるインターフェイス)。</span><span class="sxs-lookup"><span data-stu-id="223c6-133">Please note that these use cases are not yet part of the Holographic Shell experience (i.e., the interface that you see when you start up your HoloLens 2).</span></span>
<span data-ttu-id="223c6-134">これらの[ツールキット](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)を使用すると、よく見られる簡単なターゲット選択や、テキストベースの自動スクロールなど、視線追跡を使用するためのいくつかの便利で強力な例を提供します。ユーザーが確認できます。</span><span class="sxs-lookup"><span data-stu-id="223c6-134">You can try some of them out in the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) which provides several interesting and powerful examples for using eye tracking, such as quick and effortless eye-supported target selections as well as automatically scrolling through text based on what the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="223c6-135">ユーザー意図</span><span class="sxs-lookup"><span data-stu-id="223c6-135">User intent</span></span>    
<span data-ttu-id="223c6-136">ユーザーがどこで見ているかについての情報は、音声、ハンド、コントローラーなど**の他の入力に対し**て強力なコンテキストを提供します。</span><span class="sxs-lookup"><span data-stu-id="223c6-136">Information about where and what a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="223c6-137">これは、さまざまなタスクに利用できます。</span><span class="sxs-lookup"><span data-stu-id="223c6-137">This can be used for various tasks.</span></span>
<span data-ttu-id="223c6-138">たとえば、ホログラムを見て *"選択"* ( "選択" とも呼ばれ[ます) を](gaze-and-commit.md)指示するだけで、または "select" を言い、次に *"put this..."* を言い、ホログラムを配置したい*と言います。"*</span><span class="sxs-lookup"><span data-stu-id="223c6-138">For example, this can range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying *"select"* (also see [gaze and commit](gaze-and-commit.md)) or by saying *"put this..."*, then looking over to where the user wants to place the hologram and say *"...there"*.</span></span> <span data-ttu-id="223c6-139">この例は、「[Mixed Reality Toolkit - Eye-supported Target Selection (Mixed Reality Toolkit - 目で支援するターゲット選択)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)」と「[Mixed Reality Toolkit - Eye-supported Target Positioning (Mixed Reality Toolkit - 目で支援するターゲット配置)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)」に記載されています。</span><span class="sxs-lookup"><span data-stu-id="223c6-139">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="223c6-140">さらに、ユーザーの目的の例として、ユーザーが参照する情報を使用して、埋め込み仮想エージェントや対話型ホログラムによるエンゲージメントを強化することができます。</span><span class="sxs-lookup"><span data-stu-id="223c6-140">Additionally, an example for user intent might include using information about what users look at to enhance engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="223c6-141">たとえば、仮想エージェントは、現在表示されているコンテンツに基づいて、使用可能なオプションとその動作を適合させる場合があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-141">For instance, virtual agents might adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="223c6-142">暗黙的アクション</span><span class="sxs-lookup"><span data-stu-id="223c6-142">Implicit actions</span></span>
<span data-ttu-id="223c6-143">暗黙的アクションのカテゴリは、ユーザー意図に密接に関係しています。</span><span class="sxs-lookup"><span data-stu-id="223c6-143">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="223c6-144">考えられるのは、ホログラムまたはユーザーインターフェイスの要素は、ユーザーがシステムと対話しているのではなく、システムとユーザーが同期しているのではなく、instinctual な方法で対応できるということです。一例として、ユーザーがテキストを読み取ったときに自動的にスクロールが開始され、ユーザーがテキストボックスの一番下に移動したときに自動的にスクロールが開始されるような**自動スクロール**があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-144">The idea is that holograms or user interface elements react in an instinctual way that may not even feel like the user is interacting with the system at all, but rather that the system and the user are in sync. One example is **eye-gaze-based auto scroll** where the user can read a long text which automatically starts scrolling once the user gets to the bottom of the textbox to keep the user in the flow of reading without lifting a finger.</span></span>  
<span data-ttu-id="223c6-145">この重要な点は、スクロール速度がユーザーの読み取り速度に適応することです。</span><span class="sxs-lookup"><span data-stu-id="223c6-145">A key aspect of this is that the scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="223c6-146">もう1つの例として、視線がサポートされている**ズームとパン**があります。ユーザーは、フォーカスされている内容を正確に把握することができます。</span><span class="sxs-lookup"><span data-stu-id="223c6-146">Another example is **eye-supported zoom and pan** where the user can feel like diving exactly toward what he or she is focused on.</span></span> <span data-ttu-id="223c6-147">ズームのトリガーとズーム速度の制御は、音声入力または手書き入力によって制御できます。これは、ユーザーがコントロールの感覚を持つことができるようにするために重要です。</span><span class="sxs-lookup"><span data-stu-id="223c6-147">Triggering zoom and controlling zoom speed can be controlled by voice or hand input, which is important for providing the user with the feeling of control while avoiding being overwhelmed.</span></span> <span data-ttu-id="223c6-148">これらの設計の考慮事項については、以下で詳しく説明します。</span><span class="sxs-lookup"><span data-stu-id="223c6-148">We will talk about these design considerations in more detail below.</span></span> <span data-ttu-id="223c6-149">拡大した後、ユーザーは、目を見つめて使用するだけで、自宅などの道をスムーズにたどることができます。</span><span class="sxs-lookup"><span data-stu-id="223c6-149">Once zoomed in, the user can  smoothly follow, for example, the course of a street to explore his or her neighborhood by simply using their eye-gaze.</span></span>
<span data-ttu-id="223c6-150">この種の相互作用に関するデモの例は、「[Mixed Reality Toolkit - Eye-supported Navigation (Mixed Reality Toolkit - 目で支援するナビゲーション)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)」のサンプルを参照してください。</span><span class="sxs-lookup"><span data-stu-id="223c6-150">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="223c6-151">その他の_暗黙的アクション_の使用事例を以下に示します。</span><span class="sxs-lookup"><span data-stu-id="223c6-151">Additional use cases for _implicit actions_ may include:</span></span>
- <span data-ttu-id="223c6-152">**スマート通知:** 通知によって annoyed が表示されます。</span><span class="sxs-lookup"><span data-stu-id="223c6-152">**Smart notifications:** Ever get annoyed by notifications popping up right where you are looking?</span></span> <span data-ttu-id="223c6-153">ユーザーがどのようなことに注目しているかを考慮して、ユーザーが現在使用している場所から通知をオフセットすることで、このエクスペリエンスを向上させることができます。</span><span class="sxs-lookup"><span data-stu-id="223c6-153">Taking into account what a user is paying attention to, you can make this experience better by offsetting notifications from where the user is currently gazing.</span></span> <span data-ttu-id="223c6-154">これにより、取られるが制限され、ユーザーの読み取りが完了すると自動的に破棄されます。</span><span class="sxs-lookup"><span data-stu-id="223c6-154">This limits distractions and automatically dismisses them once the user is finished reading.</span></span> 
- <span data-ttu-id="223c6-155">**Attentive ホログラム:** Gazed 時に微妙に反応するホログラム。</span><span class="sxs-lookup"><span data-stu-id="223c6-155">**Attentive holograms:** Holograms that subtly react when being gazed upon.</span></span> <span data-ttu-id="223c6-156">これは、少し発光した UI 要素、低速の咲きの花から仮想犬までの範囲であり、ユーザーが戻ってきて、その尾を wagging ます。</span><span class="sxs-lookup"><span data-stu-id="223c6-156">This can range from slightly glowing UI elements, a slowly blooming flower to a virtual dog starting to look back at the user and wagging its tail.</span></span> <span data-ttu-id="223c6-157">この相互作用によって、アプリケーションでの接続性と満足度がわかりやすくなる場合があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-157">This interaction might provide an interesting sense of connectivity and satisfaction in your application.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="223c6-158">注意追跡</span><span class="sxs-lookup"><span data-stu-id="223c6-158">Attention tracking</span></span>   
<span data-ttu-id="223c6-159">どのようなユーザーが見ているかについての情報は、設計の使いやすさを評価し、効率的なワークフローの問題を特定するための非常強力なツールです。</span><span class="sxs-lookup"><span data-stu-id="223c6-159">Information about where or what users look at is an immensely powerful tool to assess usability of designs, and to identify problems in efficient workflows.</span></span> <span data-ttu-id="223c6-160">さまざまなアプリケーション領域では、視線追跡の視覚化と分析が一般的な方法です。</span><span class="sxs-lookup"><span data-stu-id="223c6-160">Eye tracking visualization and analytics are a common practice in various application areas.</span></span> <span data-ttu-id="223c6-161">HoloLens 2 では、このことを理解するために新しいディメンションを提供しています。これは、3D ホログラムを実際のコンテキストで配置し、それに従って評価することができるためです。</span><span class="sxs-lookup"><span data-stu-id="223c6-161">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed accordingly.</span></span> <span data-ttu-id="223c6-162">[Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)には、視線追跡データをログに記録して読み込む方法と、それらを視覚化する方法の基本的な例が用意されています。</span><span class="sxs-lookup"><span data-stu-id="223c6-162">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and how to visualize them.</span></span>

<span data-ttu-id="223c6-163">この分野の他の適用方法を以下に示します。</span><span class="sxs-lookup"><span data-stu-id="223c6-163">Other applications in this area may include:</span></span> 
-   <span data-ttu-id="223c6-164">**リモートの視線可視化:** リモートコラボレーターが見ていることを視覚化して、共有の理解を深めます。</span><span class="sxs-lookup"><span data-stu-id="223c6-164">**Remote eye-gaze visualization:** Visualize what remote collaborators are looking at to increase shared understanding.</span></span>
-   <span data-ttu-id="223c6-165">**ユーザー調査研究:** アテンション追跡は、私たちの環境をどのように認識し、どのように連携させるかを理解するのに役立ちます</span><span class="sxs-lookup"><span data-stu-id="223c6-165">**User research studies:** Attention tracking can help better understanding how we perceive and engage with our environment which may help in better human intent models for more instinctual human-computer-interactions.</span></span> 
-   <span data-ttu-id="223c6-166">**トレーニング:** 専門家のビジュアル検索パターンと、医療データを分析したり、運用したりするなどの複雑なタスクについて、専門家のビジュアル検索パターンとそのハンド調整をよりよく理解することにより、初心者のトレーニングが向上しました。</span><span class="sxs-lookup"><span data-stu-id="223c6-166">**Training:** Improved training of novices by better understanding experts' visual search patterns and their hand-eye-coordination for complex tasks, such as for analysis of medical data or while operating machinery.</span></span>
-   <span data-ttu-id="223c6-167">**設計の評価と市場調査:** 目の追跡は、web サイトと製品の設計を評価する際に市場調査を行うための一般的なツールです。</span><span class="sxs-lookup"><span data-stu-id="223c6-167">**Design evaluations and market research:** Eye tracking is a common tool for market research when evaluating website and product designs.</span></span> <span data-ttu-id="223c6-168">HoloLens 2 では、デジタル製品設計のバリエーションを物理環境にマージすることで、これを3D 空間に拡張することができます。</span><span class="sxs-lookup"><span data-stu-id="223c6-168">With HoloLens 2, we can extend this to 3D spaces by merging digital product design variants with the physical environment.</span></span> 

### <a name="additional-use-cases"></a><span data-ttu-id="223c6-169">その他の使用事例</span><span class="sxs-lookup"><span data-stu-id="223c6-169">Additional use cases</span></span>
- <span data-ttu-id="223c6-170">**ゲーム:** スーパーパワーが必要でしたか。</span><span class="sxs-lookup"><span data-stu-id="223c6-170">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="223c6-171">こつをお教えしましょう。</span><span class="sxs-lookup"><span data-stu-id="223c6-171">Here's your chance!</span></span> <span data-ttu-id="223c6-172">ホログラムを levitate することで、ホログラムを作成できます。</span><span class="sxs-lookup"><span data-stu-id="223c6-172">You can levitate holograms by staring at them.</span></span> <span data-ttu-id="223c6-173">お客様の目からレーザービームを撮影してください。 [RoboRaid For HoloLens 2](https://www.microsoft.com/p/roboraid/9nblggh5fv3j)で試してみてください。</span><span class="sxs-lookup"><span data-stu-id="223c6-173">Shoot laser beams from your eyes - try it out in [RoboRaid for HoloLens 2](https://www.microsoft.com/p/roboraid/9nblggh5fv3j).</span></span>
<span data-ttu-id="223c6-174">敵を石にするか、固定します。</span><span class="sxs-lookup"><span data-stu-id="223c6-174">Turn enemies into stone or freeze them.</span></span> <span data-ttu-id="223c6-175">透視能力を使ってビルを探索します。</span><span class="sxs-lookup"><span data-stu-id="223c6-175">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="223c6-176">使い道は想像力次第です。</span><span class="sxs-lookup"><span data-stu-id="223c6-176">Your imagination is the limit!</span></span>
<span data-ttu-id="223c6-177">ユーザーが圧倒されないことに注意してください。詳細については、「[目を見つめた入力のデザインガイドライン](eye-gaze-interaction.md)」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="223c6-177">Beware of not overwhelming the user though - to find out more, check out our [eye-gaze-based input design guidelines](eye-gaze-interaction.md).</span></span>

- <span data-ttu-id="223c6-178">**表現力のあるアバター:** 視線追跡では、ライブ視線追跡データを使用して、ユーザーがどのようなものかを示すアバターの目をアニメーション化することで、より表現力の高い3D アバターを支援します。</span><span class="sxs-lookup"><span data-stu-id="223c6-178">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking data to animate the avatar's eyes that indicate what the user is looking at.</span></span> 

- <span data-ttu-id="223c6-179">**テキスト入力:** 視力の追跡は、特に音声や手の使用が不便な場合に、低労力のテキスト入力の代替手段として使用できます。</span><span class="sxs-lookup"><span data-stu-id="223c6-179">**Text entry:** Eye tracking can be used as an alternative for low-effort text entry, especially when speech or hands are inconvenient to use.</span></span> 

<br>

## <a name="using-eye-gaze-for-interaction"></a><span data-ttu-id="223c6-180">相互作用のための視線の使用</span><span class="sxs-lookup"><span data-stu-id="223c6-180">Using eye-gaze for interaction</span></span>
<span data-ttu-id="223c6-181">高速移動の視点を利用する相互作用を構築するのは困難な場合があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-181">Building an interaction that takes advantage of fast-moving eye targeting can be challenging.</span></span>
<span data-ttu-id="223c6-182">ワンハンドでは、視線入力の使用方法に注意する必要があるほど高速に移動します。そうしないと、ユーザーにとっては膨大な経験や邪魔が見られる可能性があるからです。</span><span class="sxs-lookup"><span data-stu-id="223c6-182">On the one hand, the eyes move so fast that you need to be careful on how to use eye-gaze input, because otherwise user may find the experience overwhelming or distracting.</span></span> <span data-ttu-id="223c6-183">一方で、ユーザーに楽しみな気持ちする真のエクスペリエンスを作成することもできます。</span><span class="sxs-lookup"><span data-stu-id="223c6-183">On the other hand, you can also create truly magical experiences that will excite your users!</span></span> <span data-ttu-id="223c6-184">詳細については、主な利点、課題、および[相互作用の](eye-gaze-interaction.md)ための視線の設計に関する推奨事項の概要に関する記事をご覧ください。</span><span class="sxs-lookup"><span data-stu-id="223c6-184">To help you, check out our overview of key advantages, challenges and design recommendations for [eye-gaze for interaction](eye-gaze-interaction.md).</span></span> 

<br>
 
## <a name="dev-guidance-what-if-eye-tracking-is-not-available"></a><span data-ttu-id="223c6-185">開発ガイダンス: アイトラッキングを利用できない場合はどうすればよいですか。</span><span class="sxs-lookup"><span data-stu-id="223c6-185">Dev guidance: What if eye tracking is not available?</span></span>
<span data-ttu-id="223c6-186">次のようなさまざまな理由により、アプリが目の追跡データを受信しない場合があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-186">There may be situations in which your app will not receive any eye tracking data due to various reasons including but not limited to:</span></span>
* <span data-ttu-id="223c6-187">ユーザーが視線追跡の調整をスキップしました。</span><span class="sxs-lookup"><span data-stu-id="223c6-187">The user skipped the eye tracking calibration.</span></span>
* <span data-ttu-id="223c6-188">ユーザーは調整されましたが、アプリに目の追跡データを使用するアクセス許可を付与しませんでした。</span><span class="sxs-lookup"><span data-stu-id="223c6-188">The user calibrated, but decided to not give permission to your app to use their eye tracking data.</span></span>
* <span data-ttu-id="223c6-189">このユーザーには、システムがまだサポートしていない、固有の眼鏡またはいくつかの目の状態があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-189">The user has unique eyeglasses or some eye condition that the system does not yet support.</span></span>
* <span data-ttu-id="223c6-190">外部要因は、損なわれるの前に髪があることから、HoloLens バイザーや眼鏡での汚れや occlusions の強い日光となど、信頼性の高い視線を追跡します。</span><span class="sxs-lookup"><span data-stu-id="223c6-190">External factors inhibiting reliable eye tracking such as smudges on the HoloLens visor or eyeglasses, intense direct sunlight and occlusions due to hair in front of the eyes.</span></span>

<span data-ttu-id="223c6-191">アプリ開発者にとって、これは、視線追跡データが使用できないユーザーをサポートする方法を考慮する必要があることを意味します。</span><span class="sxs-lookup"><span data-stu-id="223c6-191">For you as an app developer, this means that you need to account for how to support users for whom eye tracking data may not be available.</span></span> <span data-ttu-id="223c6-192">次に、視線追跡が利用可能かどうかを検出する方法と、さまざまなアプリケーションで使用できない場合の対処方法について説明します。</span><span class="sxs-lookup"><span data-stu-id="223c6-192">Below we first explain how to detect whether eye tracking is available and how to address when it is not available for different applications.</span></span>

### <a name="1-how-to-detect-that-eye-tracking-is-available"></a><span data-ttu-id="223c6-193">1. 視線追跡が使用可能であることを検出する方法</span><span class="sxs-lookup"><span data-stu-id="223c6-193">1. How to detect that eye tracking is available</span></span>
<span data-ttu-id="223c6-194">視線追跡データを使用できるかどうかを確認するためのチェックがいくつかあります。</span><span class="sxs-lookup"><span data-stu-id="223c6-194">There are a few checks to determine whether eye tracking data is available.</span></span> <span data-ttu-id="223c6-195">確認...</span><span class="sxs-lookup"><span data-stu-id="223c6-195">Check whether...</span></span>
* <span data-ttu-id="223c6-196">...システムは、視線追跡をまったくサポートしています。</span><span class="sxs-lookup"><span data-stu-id="223c6-196">... the system supports eye tracking at all.</span></span> <span data-ttu-id="223c6-197">次の*メソッド*を呼び出します。 [EyesPose ()](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose.issupported#Windows_Perception_People_EyesPose_IsSupported)です。</span><span class="sxs-lookup"><span data-stu-id="223c6-197">Call the following *method*: [Windows.Perception.People.EyesPose.IsSupported()](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose.issupported#Windows_Perception_People_EyesPose_IsSupported)</span></span>

* <span data-ttu-id="223c6-198">...ユーザーが調整されています。</span><span class="sxs-lookup"><span data-stu-id="223c6-198">... the user is calibrated.</span></span> <span data-ttu-id="223c6-199">次の*プロパティ*を呼び出します。 [EyesPose. IsCalibrationValid](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose.iscalibrationvalid#Windows_Perception_People_EyesPose_IsCalibrationValid)</span><span class="sxs-lookup"><span data-stu-id="223c6-199">Call the following *property*: [Windows.Perception.People.EyesPose.IsCalibrationValid](https://docs.microsoft.com/uwp/api/windows.perception.people.eyespose.iscalibrationvalid#Windows_Perception_People_EyesPose_IsCalibrationValid)</span></span>

* <span data-ttu-id="223c6-200">...ユーザーは、視線追跡データを使用するためのアクセス許可をアプリに付与しました。現在の _' GazeInputAccessStatus '_ を取得します。</span><span class="sxs-lookup"><span data-stu-id="223c6-200">... the user has given your app permission to use their eye tracking data: Retrieve the current _'GazeInputAccessStatus'_.</span></span> <span data-ttu-id="223c6-201">これを行う方法の例については、「[宝石入力へのアクセスの要求](https://docs.microsoft.com/windows/mixed-reality/gaze-in-directX#requesting-access-to-gaze-input)」をご覧ください。</span><span class="sxs-lookup"><span data-stu-id="223c6-201">An example on how to do this is explained at [Requesting access to gaze input](https://docs.microsoft.com/windows/mixed-reality/gaze-in-directX#requesting-access-to-gaze-input).</span></span>

<span data-ttu-id="223c6-202">さらに、次に説明するように、受信した視線追跡データの更新の間にタイムアウトを追加して、またはそれ以外の方法でヘッドから宝石にフォールバックすることで、目の追跡データが古くなっていないことを確認することもできます。</span><span class="sxs-lookup"><span data-stu-id="223c6-202">In addition, you may want to check that your eye tracking data is not stale by adding a timeout between received eye tracking data updates and otherwise fallback to head-gaze as discussed below.</span></span> 

<span data-ttu-id="223c6-203">前述のように、視線追跡データが使用できない理由はいくつかあります。</span><span class="sxs-lookup"><span data-stu-id="223c6-203">As described above, there are several reasons why eye tracking data may not be available.</span></span> <span data-ttu-id="223c6-204">一部のユーザーは、目の追跡データへのアクセスを取り消すことを決定した場合がありますが、ユーザーエクスペリエンスの低下によって、視線追跡データへのアクセスを提供しないというプライバシーに対しては、これが意図していない場合があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-204">While some users may have consciously decided to revoke access to their eye tracking data and are ok with the trade-off of an inferior user experience to the privacy of not providing access to their eye tracking data, in some cases this may be unintentional.</span></span> <span data-ttu-id="223c6-205">そのため、アプリが目の追跡を使用していて、これがエクスペリエンスの重要な部分である場合は、これをユーザーに明確に伝えることをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="223c6-205">Hence, if your app uses eye tracking, and this is an important part of the experience, we recommend clearly communicating this to the user.</span></span> <span data-ttu-id="223c6-206">アプリケーションの可能性を最大限に活用するために、アプリケーションに対して目の追跡が不可欠である理由をユーザーに通知します (一部の拡張機能を一覧表示することもできます)。これにより、ユーザーがどのような機能を提供しているかを理解するのに役立ちます。</span><span class="sxs-lookup"><span data-stu-id="223c6-206">Kindly informing the user why eye tracking is critical for your application (maybe even listing some enhanced features) to experience the full potential of your application can help the user to better understand what they are giving up.</span></span> <span data-ttu-id="223c6-207">ユーザーが、上のチェックに基づいて目の追跡が機能していない理由を特定し、潜在的な問題の迅速なトラブルシューティングを行うための提案を提供します。</span><span class="sxs-lookup"><span data-stu-id="223c6-207">Help the user to identify why eye tracking may not be working (based on the above checks) and offer some suggestions to quickly troubleshoot potential issues.</span></span> <span data-ttu-id="223c6-208">たとえば、システムが目の追跡をサポートしていることを検出できた場合は、ユーザーが調整され、アクセス許可が与えられているにもかかわらず、目の追跡データは表示されません。その場合は、汚れや目の occluded など、その他の問題を指している可能性があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-208">For example, if you can detect that the system supports eye tracking, the user is calibrated and even has given their permission, yet no eye tracking data is received, then this may point to some other issues such as smudges or the eyes being occluded.</span></span> <span data-ttu-id="223c6-209">注意が必要なのは、目の追跡が機能しないユーザーのまれなケースです。</span><span class="sxs-lookup"><span data-stu-id="223c6-209">Please note though that there are rare cases of users for whom eye tracking may simply not work.</span></span> <span data-ttu-id="223c6-210">そのため、では、アプリで目の追跡を有効にするためのアラームを消したり、無効にしたりできるようにすることで、このことを敬意してください。</span><span class="sxs-lookup"><span data-stu-id="223c6-210">Hence, please be respectful of that by allowing to dismiss or even disable reminders for enabling eye tracking in your app.</span></span>

### <a name="2-fallback-for-apps-using-eye-gaze-as-a-primary-input-pointer"></a><span data-ttu-id="223c6-211">2. プライマリ入力ポインターとして視線を使用するアプリのフォールバック</span><span class="sxs-lookup"><span data-stu-id="223c6-211">2. Fallback for apps using eye-gaze as a primary input pointer</span></span>
<span data-ttu-id="223c6-212">アプリがポインター入力として視線を使用してシーン全体のホログラムをすばやく選択していても、目の追跡データが使用できない場合は、頭を見つめて、頭を見つめたカーソルを表示し始めることをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="223c6-212">If your app uses eye-gaze as a pointer input to quickly select holograms across the scene, yet eye tracking data is unavailable, we recommend falling back to head-gaze and start showing the head-gaze cursor.</span></span> <span data-ttu-id="223c6-213">切り替えるかどうかを判断するには、タイムアウト (500 ~ 1500 ミリ秒など) を使用することをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="223c6-213">We recommend using a timeout (e.g., 500–1500 ms) to determine whether to switch or not.</span></span> <span data-ttu-id="223c6-214">これは、システムが短時間の動きやウインクによって追跡が一時的に失われるたびに、カーソルがポップアップされるのを防ぐためです。</span><span class="sxs-lookup"><span data-stu-id="223c6-214">This is to prevent popping up a cursor every time the system may briefly lose tracking due to fast eye motions or winks and blinks.</span></span> <span data-ttu-id="223c6-215">Unity 開発者の場合、ヘッドの自動フォールバックは、Mixed Reality Toolkit で既に処理されています。</span><span class="sxs-lookup"><span data-stu-id="223c6-215">If you are a Unity developer, the automatic fallback to head-gaze is already handled in the Mixed Reality Toolkit.</span></span> <span data-ttu-id="223c6-216">DirectX 開発者は、このスイッチを自分で処理する必要があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-216">If you are a DirectX developer, you need to handle this switch yourself.</span></span>

### <a name="3-fallback-for-other-eye-tracking-specific-applications"></a><span data-ttu-id="223c6-217">3. 他の監視に固有のアプリケーションのフォールバック</span><span class="sxs-lookup"><span data-stu-id="223c6-217">3. Fallback for other eye-tracking-specific applications</span></span>
<span data-ttu-id="223c6-218">アプリは、赤目を明確にするための特別な方法で、たとえば、アバターの目をアニメーション化したり、視覚的な注意に関する正確な情報に依存して目をヒートマップように調整したりすることができます。</span><span class="sxs-lookup"><span data-stu-id="223c6-218">Your app may use eye-gaze in a unique way that is tailored specifically to the eyes - for example, for animating an avatar’s eyes or for eye-based attention heatmaps relying on precise information about visual attention.</span></span> <span data-ttu-id="223c6-219">この場合、明確なフォールバックはありません。</span><span class="sxs-lookup"><span data-stu-id="223c6-219">In this case, there is no clear fallback.</span></span> <span data-ttu-id="223c6-220">視線追跡が使用できない場合は、これらの機能を無効にする必要がある場合があります。</span><span class="sxs-lookup"><span data-stu-id="223c6-220">If eye tracking is not available, these capabilities may simply need to be disabled.</span></span>
<span data-ttu-id="223c6-221">ここでも、機能が動作していないことを認識していない可能性のあるユーザーに、このことを明確に伝えることをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="223c6-221">Again, we recommend to clearly communicate this to the user who may be unaware that the capability is not working.</span></span>

<br>

<span data-ttu-id="223c6-222">このページでは、HoloLens 2 の視線追跡と視線入力の役割を理解するのに役立つ概要が提供されています。</span><span class="sxs-lookup"><span data-stu-id="223c6-222">This page has hopefully provided you with a good overview to get you started understanding the role of eye tracking and eye-gaze input for HoloLens 2.</span></span> <span data-ttu-id="223c6-223">開発を開始するには、ホログラムとの[相互作用のための視線](eye-gaze-interaction.md)の役割についての情報を確認します。また、 [Unity で](https://aka.ms/mrtk-eyes)は目を見つめ、 [DirectX で](gaze-in-directx.md)は目を見つめます。</span><span class="sxs-lookup"><span data-stu-id="223c6-223">To get started developing, check out our information on the role of [eye-gaze for interacting with holograms](eye-gaze-interaction.md), [eye-gaze in Unity](https://aka.ms/mrtk-eyes) and [eye-gaze in DirectX](gaze-in-directx.md).</span></span>


## <a name="see-also"></a><span data-ttu-id="223c6-224">関連項目</span><span class="sxs-lookup"><span data-stu-id="223c6-224">See also</span></span>
* [<span data-ttu-id="223c6-225">調整</span><span class="sxs-lookup"><span data-stu-id="223c6-225">Calibration</span></span>](calibration.md)
* [<span data-ttu-id="223c6-226">快適性</span><span class="sxs-lookup"><span data-stu-id="223c6-226">Comfort</span></span>](comfort.md)
* [<span data-ttu-id="223c6-227">視線に基づく相互作用</span><span class="sxs-lookup"><span data-stu-id="223c6-227">Eye-gaze-based interaction</span></span>](eye-gaze-interaction.md)
* [<span data-ttu-id="223c6-228">DirectX での視線</span><span class="sxs-lookup"><span data-stu-id="223c6-228">Eye-gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="223c6-229">Unity での視線 (Mixed Reality Toolkit)</span><span class="sxs-lookup"><span data-stu-id="223c6-229">Eye-gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="223c6-230">宝石とコミットメント</span><span class="sxs-lookup"><span data-stu-id="223c6-230">Gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="223c6-231">音声入力</span><span class="sxs-lookup"><span data-stu-id="223c6-231">Voice input</span></span>](voice-design.md)


