---
title: 視線
description: HoloLens 2 では、開発者がユーザーに関する情報を使用できるようにすることで、holographic experience 内で新しいレベルのコンテキストと人間の理解を実現できます。
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
keywords: 視線追跡、Mixed Reality、インプット、視線、視線
ms.openlocfilehash: c847f7de2cf4492c89225a88aeaf189f51cfbc40
ms.sourcegitcommit: b0b1b8e1182cce93929d409706cdaa99ff24fdee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/23/2019
ms.locfileid: "68387599"
---
# <a name="eye-gaze-on-hololens-2"></a>HoloLens 2 での視線
HoloLens 2 では、開発者がユーザーに関する情報を使用できるようにすることで、holographic experience 内で新しいレベルのコンテキストと人間の理解を実現できます。 このページでは、さまざまなユースケースの目の追跡や、視線を使用したユーザーインターフェイスの設計時に見られることについて、どのように役立つかを開発者に通知します。 


## <a name="device-support"></a>デバイスのサポート

<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><strong>機能</strong></td>
     <td><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></td>
     <td><strong>HoloLens 2</strong></td>
     <td><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></td>
</tr>
<tr>
     <td>視線</td>
     <td>❌</td>
     <td>✔️</td>
     <td>❌</td>
</tr>
</table>

## <a name="use-cases"></a>使用事例
視線追跡を使用すれば、アプリケーションは、ユーザーが見ている場所をリアルタイムで追跡できます。 次のユースケースでは、mixed reality での視線追跡で可能な相互作用について説明します。
[混合現実のツールキット](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)は、目を通して見やすいように、目を追って見て見やすくするための便利で強力な例をいくつか提供していることに注意してください。ユーザーが見ている内容。 

### <a name="user-intent"></a>ユーザー意図    
ユーザーがどこで見ているかについての情報は、音声、ハンド、コントローラーなど**の他の入力に対し**て強力なコンテキストを提供します。
これは、さまざまなタスクに利用できます。
たとえば、ホログラムを見て「選択」と指示するだけで (「選択」を参照して[ください)、](gaze-and-commit.md)"select" を言い、"put the..." と指示し、ユーザーが配置する場所を調べることによって、シーン全体を**対象**とすることができます。ホログラムと言います。" この例は、「[Mixed Reality Toolkit - Eye-supported Target Selection (Mixed Reality Toolkit - 目で支援するターゲット選択)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)」と「[Mixed Reality Toolkit - Eye-supported Target Positioning (Mixed Reality Toolkit - 目で支援するターゲット配置)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)」に記載されています。

さらに、ユーザーの目的の例として、ユーザーが参照する情報を使用して、埋め込み仮想エージェントや対話型ホログラムによるエンゲージメントを強化することができます。 たとえば、仮想エージェントは、現在表示されているコンテンツに基づいて、使用可能なオプションとその動作を適合させる場合があります。 

### <a name="implicit-actions"></a>暗黙的アクション
暗黙的アクションのカテゴリは、ユーザー意図に密接に関係しています。
これは、ホログラムまたはユーザーインターフェイスの要素が多少 instinctual な方法で対応することで、ユーザーがシステムと対話するのではなく、システムとユーザーが同期しているように感じる可能性があるということではないかもしれません。1つの例として、視線を使用した**自動スクロール**があります。これは、テキストがスクロールし続けているときや、ユーザーの宝石と同期しているときに、テキストを読み上げます。 この重要な点は、スクロール速度がユーザーの読み取り速度に応じて変化することです。
もう1つの例として、視線がサポートされ**ているズームとパン**があります。 ズームのトリガーとズーム速度の制御は、音声入力または手書き入力によって制御できます。これは、ユーザーがコントロールの感覚を持つことができるようにするために重要です。 これらの設計ガイドラインについては、以下で詳しく説明します。 拡大した後、ユーザーは、目を見つめて使用するだけで、自宅などの道をスムーズにたどることができます。
この種の相互作用に関するデモの例は、「[Mixed Reality Toolkit - Eye-supported Navigation (Mixed Reality Toolkit - 目で支援するナビゲーション)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)」のサンプルを参照してください。

_暗黙的なアクション_の追加のユースケースには、次のものが含まれます。
- **スマート通知:** 注目していた場所に通知がポップアップ表示されて不快に思ったことはありませんか? ユーザーがどのようなことに注目しているかを考慮して、ユーザーが現在使用している場所から通知をオフセットすることで、このエクスペリエンスを向上させることができます。 これにより、取られるが制限され、ユーザーの読み取りが完了すると自動的に破棄されます。 
- **気が利くホログラム:** Gazed 時に微妙に反応するホログラム。 これには、わずかに光る UI 要素から、ユーザーに見てみることができるように、または長時間の確認の後にユーザーの目を見つめないようにするために、わずかな咲きの花から仮想ペットまで、さまざまなものがあります。 この相互作用によって、アプリケーションでの接続性と満足度がわかりやすくなる場合があります。

### <a name="attention-tracking"></a>注意追跡   
どのようなユーザーが見ているかについての情報は、設計の使いやすさを評価し、効率的なワークフローの問題を特定するための非常強力なツールです。 さまざまなアプリケーション領域では、視線追跡の視覚化と分析が一般的な方法です。 HoloLens 2 では、このことを理解するために新しいディメンションを提供しています。これは、3D ホログラムを実際のコンテキストで配置し、それに従って評価することができるためです。 [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)には、視線追跡データをログに記録して読み込む方法と、それらを視覚化する方法の基本的な例が用意されています。

この領域の他のアプリケーションには、次のものが含まれます。 
-   **リモートの視線可視化:** リモートコラボレーターが見ていることを視覚化し、命令が正しく認識されているかどうかを確認します。
-   **ユーザー調査研究:** アテンション追跡を使用すると、初心者と専門家のユーザーが視覚的にコンテンツを分析する方法や、医療データを分析したり、運用している場合などの複雑なタスクに対する手作業の調整方法を調べることができます。
-   **トレーニング シミュレーションとパフォーマンス監視:** 実行フローにおけるボトルネックをより効率的に特定することにより、タスクの実行を練習して最適化します。
-   **デザイン評価、宣伝、市場調査:** 視線追跡は、web サイトと製品の設計を評価する際に市場調査を行うための一般的なツールです。

### <a name="additional-use-cases"></a>その他の使用事例
- **ゲーム:** スーパーパワーが欲しくなったことはありませんか? こつをお教えしましょう。 ホログラムを levitate することで、ホログラムを作成できます。 目からレーザー ビームを発射します。 敵を石にするか、固定します。 透視能力を使ってビルを探索します。 使い道は想像力次第です。  

- **表情豊かなアバター:** 視線追跡は、ライブ目の追跡日付を使用して、ユーザーがどのようなものかを示すアバターの目をアニメーション化することで、より表現力の高い3D アバターを支援します。 また、ウインクとまばたきを追加すると、表現力が増します。 

- **テキスト入力:** 視力の追跡は、特に音声や手の使用が不便な場合に、低労力のテキスト入力の代替手段として使用できます。 


## <a name="eye-tracking-api"></a>Eye Tracking API
視線の相互作用に関する特定の設計ガイドラインについて詳しく説明する前に、HoloLens 2 アイ Tracker API が開発者に提供する機能について簡単に説明します。 ここでは、約_30 FPS_でデータを提供する、1つの目を見つめて見つめた出発点と方向を提供します。 

予測される視線は、ca 内にあります。 1.0 ~ 1.5 °、実際のターゲットを中心にした角度。 わずかなずれが想定されるため、この下限値のマージンを考慮する必要があります。 詳細については、後述します。 視線追跡が正しく機能するためには、各ユーザーが視線追跡ユーザー調整を行う必要があります。 

![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)<br>
*2メートル距離で最適なターゲットサイズ*
<br>
<br>
[アイ TRACKING API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose)には、' EyesPose ' を使用してアクセスできます。 

## <a name="eye-gaze-design-guidelines"></a>視線設計ガイドライン
高速に動く目のターゲット設定を利用した対話を構築することは、やりがいのあるものとなり得ます。 このセクションでは、アプリケーションの設計時に考慮する必要がある主な利点と課題についてまとめます。 

### <a name="benefits-of-eye-gaze-input"></a>視線入力の利点
- **高速ポインティング。** 眼筋は、人間の身体の中で最も速く反応する筋肉です。 

- **低労力。** 身体の動きがほとんど必要ありません。 

- **暗黙。** 多くの場合、ユーザーによって "覚えている" ことが示されます。ユーザーの目の動きに関する情報によって、ユーザーがどのターゲットに参加するかをシステムが把握できるようになります。 

- **代替入力チャネル。** 視線を使用すると、ユーザーからの手動による調整に基づく長年の経験に基づいて、手書き入力や音声入力に対する強力なサポート入力を提供できます。

- **視覚的注意。** もう1つの重要な利点は、ユーザーがどのようなことに注目しているかを推測できることです。 これは、さまざまなアプリケーション領域において、より効率的なユーザーインターフェイスとリモート通信のためのソーシャルキューの強化を効果的に評価できるようにするために役立ちます。

簡単に言うと、視線を入力として使用することで、高速で簡単なコンテキスト信号を実現できます。 これは、*音声*入力や*手動*入力などの他の入力と組み合わせてユーザーの意図を確認する場合に特に強力です。


### <a name="challenges-of-eye-gaze-as-an-input"></a>入力としての目を見つめた課題
多くの場合、には多くの責任があります。
Thata を使用して、ユーザーエクスペリエンスの満足度を向上させることができます。スーパーヒーローのような感覚を持っているので、適切に対応していないことを把握しておくことも重要です。 以下では、視線入力を扱うときの対処方法について考慮する必要があるいくつかの*課題*について説明します。 

- **視線が "常時オン" になってい**ます。目の蓋を開くと、環境内での作業が開始されます。 何かが長すぎると結果が得られない場合は、どのような外観にしても、アクションを誤って発行することになります。
このため、ターゲットの選択をトリガーするために、*音声コマンド*、*ハンドジェスチャ*、*ボタンクリック*、または拡張熟考を使用して視線を組み合わせることをお勧めします。
また、このソリューションでは、involuntarily によって何かをトリガーすることなく、ユーザーが自由に検索できるモードを使用することもできます。 ターゲットを見ているだけの場合の視覚的フィードバックと聴覚的フィードバックを設計するときにも、この問題を考慮する必要があります。
一瞬のポップアウト効果やホバー音でユーザーを混乱させないようにしてください。 はらみはキーです。 後で、設計推奨事項について説明するときに、これに関するベスト プラクティスについても説明します。

- **監視と制御**たとえば、壁の写真を正確にためるとします。 写真の縁と周囲を見て、揃っているかどうかを確認します。 次に、画像を移動するための入力として目を見つめて使用する方法を考えてみましょう。 難しいのではないでしょうか。 これは、入力と制御の両方に必要な場合に、視線の2つの役割について説明します。 

- **クリックする前に離れる:** クイックターゲットを選択する場合は、ユーザーの目を見つめていて、手動でクリックする前 (たとえば、放映タップ) に移動できることが調査によって示されています。 そのため、より低速なコントロール入力 (音声、ハンド、コントローラーなど) を使用して、高速の視線信号を同期するには、特別な注意が必要です。

- **小さいターゲット:** 読みやすくするために少しすぎてテキストを読まようとすると、感じていることがわかりますか。 これによって目が疲れてしまうことがあります。目を絞って再調整しようとしているため、疲れてしまうことがあります。
これは、ユーザーが視点を使用してアプリケーションで小さすぎるターゲットを選択する必要がある場合に、ユーザーに対して呼び出す可能性があります。
設計に際しては、ユーザーにとって楽しく快適なエクスペリエンスを作るため、ターゲットを視角で 2 度以上にする (さらに大きい方が好ましい) ことをお勧めします。

- **不規則な視点の移動**この目では、固定から固定への迅速な移動を行います。 記録された目の動きのスキャン パスを見ると、視線が不規則に動いていることがわかります。 *頭の視線入力*や*手の動き*に比べて、目の動きはすばやく、無意識のうちに突然動くことがあります。  

- **追跡の信頼性:** 視線追跡の精度は、目が新しい条件に合わせて調整したときのほんの少しの光の変化で低下する可能性があります。
これは必ずしもアプリケーションの設計に影響を与えるわけではありませんが、精度は2°の制限内にあるため、ユーザーが別の調整を実行することが必要になる場合があります。 


## <a name="design-recommendations"></a>設計の推奨事項
次に示すのは、目を見つめた入力の利点と課題に基づいた、設計に関する特定の推奨事項の一覧です。

1. **視線は、ヘッドを見つめます。**
    - **高速だが不規則な目の動きが入力タスクに合っているかどうかを検討する:** 私たちの視野 (視界) を通じてターゲットをすばやく選択することによって、高速で不規則な視点が非常に優れていますが、smooth input 軌道 (描画や encircling の注釈など) を必要とするタスクには適用できません。 この場合は、手または頭によるポインティングをお勧めします。
  
    - **ユーザーの目を見つめた (スライダーやカーソルなど) に直接接続することは避けてください。**
カーソルの場合は、投影された視線の信号のオフセットがわずかであるため、"fleeing cursor" 効果が生じる可能性があります。 スライダーの場合は、スライダーを目で制御する2つの役割と競合し、オブジェクトが正しい位置にあるかどうかを確認することもできます。 簡単に言うと、ユーザーにとっては特に、ユーザーの信号が不正確になる可能性があります。 
  
2. **視線を他の入力と結合します。** ハンドジェスチャ、音声コマンド、ボタンの押下など、他の入力との視線追跡の統合には、いくつかの利点があります。
    - **自由観察を可能にする:** 私たちの主要な役割は、環境を観察することであるということです。重要なのは、ユーザーが何も (視覚、聴覚などの) フィードバックまたはアクションをトリガーすることなく検索できることです。 
    視線追跡と別の入力コントロールを組み合わせることにより、視線監視と入力制御モードの間でスムーズに遷移できます。
  
    - **強力なコンテキスト プロバイダー:** 音声コマンドを uttering したり、手の形でジェスチャを実行したりしているときに、ユーザーがどこで見ているかについての情報を使用することにより、ビューのフィールド間でシームレスに入力を channeling ことができます。 以下に例を示します。「Put that there (それをあそこに置いて)」と言うときに、単にターゲットと宛先を見るだけで、シーンの中でホログラムを選択して配置する作業を迅速かつ滑らかに行えます。 

    - **マルチモーダル入力を同期する必要性 (「クリック前に離れる」問題):** 長い音声コマンドやハンドジェスチャなど、より複雑な追加入力を使用して迅速な移動を組み合わせることにより、追加の入力コマンドを完了する前に、目を見つめていくリスクが高くなります。 そのため、独自の入力コントロール (手のカスタム ジェスチャなど) を作成する場合は、この入力の開始またはおおよその持続時間をログに記録して、それをユーザーが前に凝視していたものと関連付けてください。
    
3. **視線追跡入力の繊細なフィードバック:** システムが意図したとおりに動作していることを示すためにターゲットを検索するときに、フィードバックを提供すると便利ですが、微妙に保つ必要があります。 これには、徐々にブレンドを行う、視覚的に強調表示する、またはターゲットを少し増やして、ターゲットを見ていることをシステムが正しく検出したことをシステムが検出したことを示すその他の微妙なターゲット動作 (ユーザーの現在のワークフローを不必要に中断しています。 

4. **入力としての不自然な目の動きを強制しない:** アプリケーションのアクションをトリガーするために、ユーザーが特定の目の動き (宝石のジェスチャ) を実行しないようにします。

5. **不正確さを考慮に入れる:** ユーザーにとってわかりやすく、オフセットとジッターという2種類の不正確性を区別します。 オフセットに対処する最も簡単な方法は、操作に十分なサイズのターゲットを提供することです。 2°を超える視覚的な角度を参照として使用することをお勧めします。 たとえば、arm を拡張すると、サムネイルの表示角度が約2°になります。 これが次のガイダンスにつながります。
    - ユーザーが小さいターゲットを選択することを強制しないでください。 ここでは、ターゲットが十分に大きく、システムが適切に設計されている場合に、ユーザーの対話を簡単で魔法のない方法で記述しています。 ターゲットを小さくしすぎると、ユーザーはエクスペリエンスを疲れる、いらだたしいものと表現します。
   

## <a name="see-also"></a>関連項目
* [頭の視線入力とコミット](gaze-and-commit.md)
* [DirectX での頭と視線](gaze-in-directx.md)
* [Unity での視線 (Mixed Reality Toolkit)](https://aka.ms/mrtk-eyes)
* [手のジェスチャ](gestures.md)
* [音声入力](voice-design.md)
* [モーション コントローラー](motion-controllers.md)
* [快適性](comfort.md)
