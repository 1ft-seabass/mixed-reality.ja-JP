---
title: 視線追跡
description: 視線追跡
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: 視線追跡, Mixed Reality, 入力, 目の視線入力
ms.openlocfilehash: 7298a34a946f86aaf789cfe44ad971169fc8ece3
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 06/05/2019
ms.locfileid: "66453702"
---
# <a name="eye-tracking-on-hololens-2"></a><span data-ttu-id="9359a-104">HoloLens 2 上の視線追跡</span><span class="sxs-lookup"><span data-stu-id="9359a-104">Eye tracking on HoloLens 2</span></span>
<span data-ttu-id="9359a-105">HoloLens 2 を使用すると、開発者はユーザーが見ているものの情報を活用する素晴らしい能力を持つことができ、ホログラフィック エクスペリエンスにおけるコンテキストと人間の理解が大きく進みます。</span><span class="sxs-lookup"><span data-stu-id="9359a-105">HoloLens 2 allows for a whole new level of context and human understanding within the holographic experience by providing developers with the incredible ability of using information about what users are looking at.</span></span> <span data-ttu-id="9359a-106">このページでは、開発者がさまざまな使用事例で視線追跡から得られるメリットと、目の視線入力ベースのユーザー インターフェイスを設計する際に注意すべき点について概要を示します。</span><span class="sxs-lookup"><span data-stu-id="9359a-106">This page gives an overview of how developers can benefit from eye tracking for various use cases and what to look out for when designing eye-gaze-based user interfaces.</span></span> 

## <a name="use-cases"></a><span data-ttu-id="9359a-107">使用事例</span><span class="sxs-lookup"><span data-stu-id="9359a-107">Use cases</span></span>
<span data-ttu-id="9359a-108">視線追跡を使用すれば、アプリケーションは、ユーザーが見ている場所をリアルタイムで追跡できます。</span><span class="sxs-lookup"><span data-stu-id="9359a-108">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="9359a-109">このセクションでは、Mixed Reality で視線追跡を使用することで可能になる、潜在的な使用事例や新しい相互作用について説明します。</span><span class="sxs-lookup"><span data-stu-id="9359a-109">This section describes some of the potential use cases and novel interactions that become possible with eye tracking in mixed reality.</span></span>
<span data-ttu-id="9359a-110">作業を始める前に、この先の部分では [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) に何度か言及することをお知らせしておきます。これは、視線追跡を使用する興味深く強力な例がいくつか付属しているためです。たとえば、視線を利用して迅速かつ楽にターゲットを選択することや、ユーザーが見ているものに基づいて自動的にテキストをスクロールすることなどです。</span><span class="sxs-lookup"><span data-stu-id="9359a-110">Before getting started, in the following we will mention the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) several times as it provides several interesting and powerful examples for using eye tracking such as quick and effortless eye-supported target selections and automatically scrolling through text based on where the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="9359a-111">ユーザー意図</span><span class="sxs-lookup"><span data-stu-id="9359a-111">User intent</span></span>    
<span data-ttu-id="9359a-112">ユーザーがどこを見ているかに関する情報は、音声、手、コントローラーなど、**他の入力のための強力なコンテキスト**となります。</span><span class="sxs-lookup"><span data-stu-id="9359a-112">Information about where a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="9359a-113">これは、さまざまなタスクに利用できます。</span><span class="sxs-lookup"><span data-stu-id="9359a-113">This can be used for various tasks.</span></span>
<span data-ttu-id="9359a-114">その範囲としては、たとえば、ホログラムを見て「選択」と言うだけでシーンの中から迅速かつ楽に**ターゲット設定**することや (「[頭の視線入力とコミット](gaze-and-commit.md)」も参照)、「put this... (これを置いて)」と言ってからそのホログラムを配置したい場所を見渡して「...there (あそこへ)」という言うことなどがあります。</span><span class="sxs-lookup"><span data-stu-id="9359a-114">For example, this may range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying "select" (also see [Head-gaze and commit](gaze-and-commit.md)) or by saying "put this...", then looking over to where you want to place the hologram and say "...there".</span></span> <span data-ttu-id="9359a-115">この例は、「[Mixed Reality Toolkit - Eye-supported Target Selection (Mixed Reality Toolkit - 目で支援するターゲット選択)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)」と「[Mixed Reality Toolkit - Eye-supported Target Positioning (Mixed Reality Toolkit - 目で支援するターゲット配置)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)」に記載されています。</span><span class="sxs-lookup"><span data-stu-id="9359a-115">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="9359a-116">ユーザー意図に関する別の例としては、具現化された仮想エージェントや対話型ホログラムとのやり取りを強化するために、ユーザーが何を見ているかについての情報を使う、というものがあります。</span><span class="sxs-lookup"><span data-stu-id="9359a-116">An additional example for user intent may include using information about what users look at to enhance the engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="9359a-117">たとえば、現在視線が向けられているコンテンツに基づき、仮想エージェントは使用可能なオプションや動作を変化させることができます。</span><span class="sxs-lookup"><span data-stu-id="9359a-117">For example, virtual agents may adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="9359a-118">暗黙的アクション</span><span class="sxs-lookup"><span data-stu-id="9359a-118">Implicit actions</span></span>
<span data-ttu-id="9359a-119">暗黙的アクションのカテゴリは、ユーザー意図に密接に関係しています。</span><span class="sxs-lookup"><span data-stu-id="9359a-119">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="9359a-120">この考え方は、ホログラムやユーザー インターフェイス要素がいくらか本能的とも言える仕方で反応するので、システムと対話しているという感覚さえまったく与えることがなく、それでいてシステムとユーザーの間には協調関係がある、というものです。たとえば、大成功した 1 つの例が**目の視線入力ベースの自動スクロール**です。</span><span class="sxs-lookup"><span data-stu-id="9359a-120">The idea is that holograms or user interface elements react in a somewhat instinctual way that may not even feel like you are interacting with the system at all, but rather that the system and the user are in sync. For example, one immensely successful example is **eye-gaze-based auto scroll**.</span></span> <span data-ttu-id="9359a-121">考え方は次のように単純です。ユーザーはテキストを読み、ひたすら読み続けることができます。</span><span class="sxs-lookup"><span data-stu-id="9359a-121">The idea is as simple: The user reads a text and can just keep on reading.</span></span> <span data-ttu-id="9359a-122">テキストは、ユーザーの読む速度に合わせて少しずつ上に移動します。</span><span class="sxs-lookup"><span data-stu-id="9359a-122">The text gradually moves up to keep users in their reading flow.</span></span> <span data-ttu-id="9359a-123">重要な側面は、スクロール速度がユーザーの読む速度に適応していることです。</span><span class="sxs-lookup"><span data-stu-id="9359a-123">A key aspect is that the scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="9359a-124">別の例は、まさに自分が焦点を合わせているものに向かって飛び込むような印象をユーザーに与える、**視線の支援を使ったズームとパン**です。</span><span class="sxs-lookup"><span data-stu-id="9359a-124">Another example is **eye-supported zoom and pan** for which the user can feel like diving exactly toward what he or she is focusing at.</span></span> <span data-ttu-id="9359a-125">ズームのトリガーやズーム速度の管理は、音声や手の入力を使用して制御できます。これは、制御できているという感覚を与えたり、ユーザーの混乱を避けたりする面で重要です (詳細については、後述の設計ガイドラインを参照してください)。</span><span class="sxs-lookup"><span data-stu-id="9359a-125">Triggering the zoom and controlling the zoom speed can be controlled via voice or hand input which is important about providing the feeling of control and avoid overwhelming the user (we will talk about these design guidelines in more detail below).</span></span> <span data-ttu-id="9359a-126">ズームインすれば、ユーザーは、目の視線入力を使用するだけで、たとえば、通り沿いの道を滑らかにたどって近所を探索することができます。</span><span class="sxs-lookup"><span data-stu-id="9359a-126">Once zoomed in, the user can then smoothly follow, for example, the course of a street to explore his or her neighborhood just simply by using their eye gaze.</span></span>
<span data-ttu-id="9359a-127">この種の相互作用に関するデモの例は、「[Mixed Reality Toolkit - Eye-supported Navigation (Mixed Reality Toolkit - 目で支援するナビゲーション)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)」のサンプルを参照してください。</span><span class="sxs-lookup"><span data-stu-id="9359a-127">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="9359a-128">その他の_暗黙的アクション_の使用事例を以下に示します。</span><span class="sxs-lookup"><span data-stu-id="9359a-128">Additional use cases for _implicit actions_ may include:</span></span>
- <span data-ttu-id="9359a-129">**スマート通知:** 注目していた場所に通知がポップアップ表示されて不快に思ったことはありませんか?</span><span class="sxs-lookup"><span data-stu-id="9359a-129">**Smart notifications:** Ever get annoyed by notifications popping up right where you were focusing?</span></span> <span data-ttu-id="9359a-130">ユーザーが現在注目している場所を考慮すれば、改善できます。</span><span class="sxs-lookup"><span data-stu-id="9359a-130">Taking into account where a user is currently paying attention to, you can make it better!</span></span> <span data-ttu-id="9359a-131">気を散らさないように、ユーザーが現在見ている場所を避けて通知を表示し、読み終わったら自動的に消去します。</span><span class="sxs-lookup"><span data-stu-id="9359a-131">Show notifications offset from where the user is currently looking to limit distractions and automatically dismiss them once finished reading.</span></span> 
- <span data-ttu-id="9359a-132">**気が利くホログラム:** 見られているときに繊細に反応するホログラム。</span><span class="sxs-lookup"><span data-stu-id="9359a-132">**Attentive holograms:** Holograms that subtly react when being looked at.</span></span> <span data-ttu-id="9359a-133">これには、UI 要素の輝度をわずかに上げる、ゆっくり花が咲くといったものから、ユーザーの方を振り返る仮想ペットや、長い凝視の後には目の視線入力を避けることまでが含まれます。</span><span class="sxs-lookup"><span data-stu-id="9359a-133">This may range from slightly glowing UI elements, a slowly blooming flower to a virtual pet starting to look back at you or trying to avoid your eye gaze after a prolonged stare.</span></span> <span data-ttu-id="9359a-134">これにより、つながることの面白味や満足感をアプリに盛り込むことができます。</span><span class="sxs-lookup"><span data-stu-id="9359a-134">This may provide an interesting sense of connectivity and satisfaction in your app.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="9359a-135">注意追跡</span><span class="sxs-lookup"><span data-stu-id="9359a-135">Attention tracking</span></span>   
<span data-ttu-id="9359a-136">ユーザーが見ている場所に関する情報は、デザインの有用性を評価し、効率的な作業の流れを阻害している問題を特定するための非常に強力なツールです。</span><span class="sxs-lookup"><span data-stu-id="9359a-136">Information about where users look at is an immensely powerful tool to assess usability of designs and to identify problems in efficient work streams.</span></span> <span data-ttu-id="9359a-137">既に、視線追跡の可視化と分析は、さまざまな適用分野で広く採用されています。</span><span class="sxs-lookup"><span data-stu-id="9359a-137">By now,  eye tracking visualization and analytics are already a common practice in various application areas.</span></span> <span data-ttu-id="9359a-138">HoloLens 2 を使用すれば、3D ホログラムを実際のコンテキスト内に配置して、同時に評価できるため、この認識に新たな次元が加えられます。</span><span class="sxs-lookup"><span data-stu-id="9359a-138">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed alongside.</span></span> <span data-ttu-id="9359a-139">[Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) には、視線追跡データの記録および読み込みとそれらの可視化方法に関する基本的な例が付属しています。</span><span class="sxs-lookup"><span data-stu-id="9359a-139">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and for how to visualize them.</span></span>

<span data-ttu-id="9359a-140">この分野の他の適用方法を以下に示します。</span><span class="sxs-lookup"><span data-stu-id="9359a-140">Other applications in this area may include:</span></span> 
-   <span data-ttu-id="9359a-141">**リモートの目の視線入力の可視化:** たとえば、リモート コラボレーターが見ているものを可視化して、手順が正しく理解され、実行されているかどうかを確認します。</span><span class="sxs-lookup"><span data-stu-id="9359a-141">**Remote eye gaze visualization:** Visualize what remote collaborators are looking at to, for example, ensure whether instructions are correctly understood and followed.</span></span>
-   <span data-ttu-id="9359a-142">**ユーザー調査研究:** 注意追跡は、初級ユーザーと上級ユーザーがコンテンツを視覚的に分析する方法や、複雑な作業における手と目の協調 (医療データの分析や機械の操作など) を探求するために使用できます。</span><span class="sxs-lookup"><span data-stu-id="9359a-142">**User research studies:** Attention tracking can be used to explore the way novice vs. experts users visually analyze content or their hand-eye-coordination for complex tasks (e.g., for analysis of medical data or while operating machinery).</span></span>
-   <span data-ttu-id="9359a-143">**トレーニング シミュレーションとパフォーマンス監視:** 実行フローにおけるボトルネックをより効率的に特定することにより、タスクの実行を練習して最適化します。</span><span class="sxs-lookup"><span data-stu-id="9359a-143">**Training simulations and Performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span>
-   <span data-ttu-id="9359a-144">**デザイン評価、宣伝、市場調査:** 視線追跡は、Web サイトや製品のデザインを評価するための市場調査用の一般的なツールです。</span><span class="sxs-lookup"><span data-stu-id="9359a-144">**Design evaluations, advertisement and marketing research:** Eye tracking is a common tool for market research to evaluate website and product designs.</span></span>

### <a name="additional-use-cases"></a><span data-ttu-id="9359a-145">その他の使用事例</span><span class="sxs-lookup"><span data-stu-id="9359a-145">Additional use cases</span></span>
- <span data-ttu-id="9359a-146">**ゲーム:** スーパーパワーが欲しくなったことはありませんか?</span><span class="sxs-lookup"><span data-stu-id="9359a-146">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="9359a-147">こつをお教えしましょう。</span><span class="sxs-lookup"><span data-stu-id="9359a-147">Here's your chance!</span></span> <span data-ttu-id="9359a-148">ホログラムを凝視して浮き上がらせます。</span><span class="sxs-lookup"><span data-stu-id="9359a-148">Levitate holograms by staring at them.</span></span> <span data-ttu-id="9359a-149">目からレーザー ビームを発射します。</span><span class="sxs-lookup"><span data-stu-id="9359a-149">Shoot laser beams from your eyes.</span></span> <span data-ttu-id="9359a-150">敵を石に変えたり、凍らせたりします。</span><span class="sxs-lookup"><span data-stu-id="9359a-150">Turn enemies into stone or freeze them!</span></span> <span data-ttu-id="9359a-151">透視能力を使ってビルを探索します。</span><span class="sxs-lookup"><span data-stu-id="9359a-151">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="9359a-152">使い道は想像力次第です。</span><span class="sxs-lookup"><span data-stu-id="9359a-152">Your imagination is the limit!</span></span>  

- <span data-ttu-id="9359a-153">**表情豊かなアバター:** 視線追跡のライブ データを使用してユーザーが現在見ているものを示すようにアバターの目をアニメーション化すれば、視線追跡は 3D アバターの表情を豊かにするのに役立ちます。</span><span class="sxs-lookup"><span data-stu-id="9359a-153">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking date to animate the avatar's eyes to indicate what the user is currently looking at.</span></span> <span data-ttu-id="9359a-154">また、ウインクとまばたきを追加すると、表現力が増します。</span><span class="sxs-lookup"><span data-stu-id="9359a-154">It also adds more expressiveness by adding winks and blinks.</span></span> 

- <span data-ttu-id="9359a-155">**テキスト入力:** 視線追跡は、特に声や手が使えない場合の低労力のテキスト入力の興味深い代替手段として使用できます。</span><span class="sxs-lookup"><span data-stu-id="9359a-155">**Text entry:** Eye tracking can be used as an interesting alternative for low-effort text entry especially when speech or hands are inconvenient to use.</span></span> 


## <a name="eye-tracking-api"></a><span data-ttu-id="9359a-156">Eye Tracking API</span><span class="sxs-lookup"><span data-stu-id="9359a-156">Eye tracking API</span></span>
<span data-ttu-id="9359a-157">目の視線入力の相互作用に関する特定の設計ガイドラインの詳細に入る前に、HoloLens 2 Eye Tracker が提供している機能について簡単に説明します。</span><span class="sxs-lookup"><span data-stu-id="9359a-157">Before going into detail about the specific design guidelines for eye-gaze interaction, we want to briefly point to the capabilities that the HoloLens 2 Eye Tracker is providing.</span></span> <span data-ttu-id="9359a-158">[Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) は `Windows.Perception.People.EyesPose` 経由でアクセスできます。</span><span class="sxs-lookup"><span data-stu-id="9359a-158">The [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) is accessible through: `Windows.Perception.People.EyesPose`.</span></span> <span data-ttu-id="9359a-159">この API は、目の視線入力の 1 本の光線 (視線入力の原点と方向) を開発者に提供します。</span><span class="sxs-lookup"><span data-stu-id="9359a-159">It provides a single eye gaze ray (gaze origin and direction) to developers.</span></span>
<span data-ttu-id="9359a-160">アイ トラッカーは、約 _30 FPS_ でデータを提供します。</span><span class="sxs-lookup"><span data-stu-id="9359a-160">The eye tracker provides data at about _30 FPS_.</span></span>
<span data-ttu-id="9359a-161">予測される目の視線入力は、</span><span class="sxs-lookup"><span data-stu-id="9359a-161">The predicted eye gaze lies within ca.</span></span> <span data-ttu-id="9359a-162">実際に見ているターゲットを中心とした視角で約 1.0 - 1.5 度以内に収まっています。</span><span class="sxs-lookup"><span data-stu-id="9359a-162">1.0 - 1.5 degrees in visual angle around the actual looked at target.</span></span> <span data-ttu-id="9359a-163">わずかなずれが想定されるため、この下限値のマージンを考慮する必要があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-163">As slight imprecisions are expected, you should plan for some margin around this lower bound value.</span></span> <span data-ttu-id="9359a-164">詳細については、後述します。</span><span class="sxs-lookup"><span data-stu-id="9359a-164">We will discuss this more below.</span></span> <span data-ttu-id="9359a-165">視線追跡が正しく機能するためには、各ユーザーが視線追跡ユーザー調整を行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-165">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="9359a-166">![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="9359a-166">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="9359a-167">*2 m の距離での最適なターゲット サイズ*</span><span class="sxs-lookup"><span data-stu-id="9359a-167">*Optimal target size at 2 meter distance*</span></span>


## <a name="eye-gaze-design-guidelines"></a><span data-ttu-id="9359a-168">目の視線入力設計ガイドライン</span><span class="sxs-lookup"><span data-stu-id="9359a-168">Eye gaze design guidelines</span></span>
<span data-ttu-id="9359a-169">高速に動く目のターゲット設定を利用した対話を構築することは、やりがいのあるものとなり得ます。</span><span class="sxs-lookup"><span data-stu-id="9359a-169">Building an interaction that takes advantage of fast moving eye targeting can be challenging.</span></span> <span data-ttu-id="9359a-170">このセクションでは、アプリの設計時に考慮すべき主なメリットと課題をまとめます。</span><span class="sxs-lookup"><span data-stu-id="9359a-170">In this section, we summarize the key advantages and challenges to take into account when designing your app.</span></span> 

### <a name="benefits-of-eye-gaze-input"></a><span data-ttu-id="9359a-171">目の視線入力で入力を行うメリット</span><span class="sxs-lookup"><span data-stu-id="9359a-171">Benefits of eye gaze input</span></span>
- <span data-ttu-id="9359a-172">**高速ポインティング。**</span><span class="sxs-lookup"><span data-stu-id="9359a-172">**High speed pointing.**</span></span> <span data-ttu-id="9359a-173">眼筋は、人間の身体の中で最も速く反応する筋肉です。</span><span class="sxs-lookup"><span data-stu-id="9359a-173">The eye muscle is the fastest reacting muscle in our body.</span></span> 

- <span data-ttu-id="9359a-174">**低労力。**</span><span class="sxs-lookup"><span data-stu-id="9359a-174">**Low effort.**</span></span> <span data-ttu-id="9359a-175">身体の動きがほとんど必要ありません。</span><span class="sxs-lookup"><span data-stu-id="9359a-175">Barely any physical movements are necessary.</span></span> 

- <span data-ttu-id="9359a-176">**暗黙。**</span><span class="sxs-lookup"><span data-stu-id="9359a-176">**Implicitness.**</span></span> <span data-ttu-id="9359a-177">ユーザーから「読心術」と言われることもしばしばであり、ユーザーの目の動きの情報から、ユーザーが操作しようと思っているターゲットがシステムに伝わります。</span><span class="sxs-lookup"><span data-stu-id="9359a-177">Often described by users as "mind reading", information about a user's eye movements lets the system know which target the user plans to engage with.</span></span> 

- <span data-ttu-id="9359a-178">**代替入力チャネル。**</span><span class="sxs-lookup"><span data-stu-id="9359a-178">**Alternative input channel.**</span></span> <span data-ttu-id="9359a-179">目の視線入力は、手と音声による入力を強力に支援できます。これは、手と目の協調を基にしたユーザーの長年の経験に立脚したものです。</span><span class="sxs-lookup"><span data-stu-id="9359a-179">Eye gaze can provide a powerful supporting input for hand and voice input building on years of experience from users based on their hand-eye coordination.</span></span>

- <span data-ttu-id="9359a-180">**視覚的注意。**</span><span class="sxs-lookup"><span data-stu-id="9359a-180">**Visual attention.**</span></span> <span data-ttu-id="9359a-181">もう 1 つの重要なメリットは、ユーザーが注目しているものを推測できる可能性です。</span><span class="sxs-lookup"><span data-stu-id="9359a-181">Another important benefit is the possibility to infer what a user's is paying attention to.</span></span> <span data-ttu-id="9359a-182">これはさまざまな適用分野に役立つ可能性があり、その範囲は、さまざまなデザインを効果的に評価することから、より賢いユーザー インターフェイスや遠隔コミュニケーションのための拡大された社会的手掛かりを支援することにまで及びます。</span><span class="sxs-lookup"><span data-stu-id="9359a-182">This can help in various application areas ranging from more effectively evaluating different designs to aiding in smarter User Interfaces and enhanced social cues for remote communication.</span></span>

<span data-ttu-id="9359a-183">簡単に言えば、目の視線入力を入力として使用することにより、高速で簡単なコンテキスト信号を提供できる可能性があります。これは、特に、*音声*入力や*手*入力などの他の入力と組み合わせてユーザーの意図を確認する場合に効果を発揮します。</span><span class="sxs-lookup"><span data-stu-id="9359a-183">In a nutshell, using eye gaze as an input potentially offers a fast and effortless contextual signal - This is particularly powerful in combination with other inputs such as *voice* and *manual* input to confirm the user's intent.</span></span>


### <a name="challenges-of-eye-gaze-as-an-input"></a><span data-ttu-id="9359a-184">入力としての目の視線入力の課題</span><span class="sxs-lookup"><span data-stu-id="9359a-184">Challenges of eye gaze as an input</span></span>
<span data-ttu-id="9359a-185">能力が高いほど、責任も重くなります。目の視線入力を使用すれば、スーパーヒーローになったような魅力的なユーザー エクスペリエンスを創造できますが、その欠点について知っておくことも、正しく説明する責任を果たす上で重要です。</span><span class="sxs-lookup"><span data-stu-id="9359a-185">With lots of power, comes lots of responsibility: While eye gaze can be used to create magical user experiences feeling like a superhero, it is also important to know what it is not good at to account for this appropriately.</span></span> <span data-ttu-id="9359a-186">ここでは、目の視線入力による入力を使用して作業する場合に考慮すべき*課題*とその解決方法について説明します。</span><span class="sxs-lookup"><span data-stu-id="9359a-186">In the following, we discuss some *challenges* to take into account and how to address them when working with eye gaze input:</span></span> 

- <span data-ttu-id="9359a-187">**目の視線入力は「常時オン」** まぶたを開いた瞬間に、目は環境内の物の凝視を開始します。</span><span class="sxs-lookup"><span data-stu-id="9359a-187">**Your eye gaze is "always on"** The moment you open your eye lids, your eyes start fixating things in your environment.</span></span> <span data-ttu-id="9359a-188">見たものすべてに反応したり、長く見続けたせいで偶発的にアクションが発行されたりすることによって、予想外のことが起きる場合があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-188">Reacting to every look you make and potentially accidentally issuing actions because you looked at something for too long would result in a terrible experience!</span></span>
<span data-ttu-id="9359a-189">これが、目の視線入力と、*音声コマンド*、*手のジェスチャ*、*ボタン クリック*、または長期間のドウェルを組み合わせてターゲットの選択をトリガーすることが推奨されている理由です。</span><span class="sxs-lookup"><span data-stu-id="9359a-189">This is why we recommend combining eye gaze with a *voice command*, *hand gesture*, *button click* or extended dwell to trigger the selection of a target.</span></span>
<span data-ttu-id="9359a-190">この解決策は、ユーザーが無意識に何かをトリガーして混乱を覚えることなく、自由に周りを見回すことができるモードも可能にします。</span><span class="sxs-lookup"><span data-stu-id="9359a-190">This solution also allows for a mode in which the user can freely look around without the overwhelming feeling of involuntarily triggering something.</span></span> <span data-ttu-id="9359a-191">ターゲットを見ているだけの場合の視覚的フィードバックと聴覚的フィードバックを設計するときにも、この問題を考慮する必要があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-191">This issue should also be taken into account when designing visual and auditory feedback when merely looking at a target.</span></span>
<span data-ttu-id="9359a-192">一瞬のポップアウト効果やホバー音でユーザーを混乱させないようにしてください。</span><span class="sxs-lookup"><span data-stu-id="9359a-192">Do not overwhelm the user with immediate pop-out effects or hover sounds.</span></span> <span data-ttu-id="9359a-193">繊細さが重要です。</span><span class="sxs-lookup"><span data-stu-id="9359a-193">Subtlety is key!</span></span> <span data-ttu-id="9359a-194">後で、設計推奨事項について説明するときに、これに関するベスト プラクティスについても説明します。</span><span class="sxs-lookup"><span data-stu-id="9359a-194">We will discuss some best practices for this further below when talking about design recommendations.</span></span>

- <span data-ttu-id="9359a-195">**観察と制御** 壁に写真をきれいに並べたい場合を想像してください。</span><span class="sxs-lookup"><span data-stu-id="9359a-195">**Observation vs. control** Imagine you want to precisely align a photograph at your wall.</span></span> <span data-ttu-id="9359a-196">写真の縁と周囲を見て、揃っているかどうかを確認します。</span><span class="sxs-lookup"><span data-stu-id="9359a-196">You look at its borders and its surroundings to see if it aligns well.</span></span> <span data-ttu-id="9359a-197">今度は、それを行うのと同時に、写真を動かすための入力として自分の目の視線入力を用いようとする場面を想像してみてください。</span><span class="sxs-lookup"><span data-stu-id="9359a-197">Now imagine how you would do that when at the same time you want to use your eye gaze as an input to move the picture.</span></span> <span data-ttu-id="9359a-198">難しいのではないでしょうか。</span><span class="sxs-lookup"><span data-stu-id="9359a-198">Difficult, isn't it?</span></span> <span data-ttu-id="9359a-199">これは、目の視線入力が、入力と制御の両方に必要になると、その役割が二重になることを示します。</span><span class="sxs-lookup"><span data-stu-id="9359a-199">This describes the double role of eye gaze when it is required both for input and control.</span></span> 

- <span data-ttu-id="9359a-200">**クリックする前に離れる:** 調査によると、迅速なターゲット選択の場合、手動クリック (エアタップなど) が完了する前にユーザーの目の視線入力が動いてしまう可能性があることがわかりました。</span><span class="sxs-lookup"><span data-stu-id="9359a-200">**Leave before click:** For quick target selections, research has shown that a user's eye gaze may move on before concluding a manual click (e.g., an airtap).</span></span> <span data-ttu-id="9359a-201">そのため、低速のコントロール入力 (音声、手、コントローラーなど) と高速の目の視線入力信号の同期には特別な注意を払う必要があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-201">Hence, special attention must be paid to synchronizing the fast eye gaze signal with slower control input (e.g., voice, hands, controller).</span></span>

- <span data-ttu-id="9359a-202">**小さいターゲット:** 小さすぎてよく見えないテキストを必死で読もうとするときの気持ちをご存知でしょうか。</span><span class="sxs-lookup"><span data-stu-id="9359a-202">**Small targets:** Do you know the feeling when you try to read text that is just a bit too small to comfortably read?</span></span> <span data-ttu-id="9359a-203">この暗い気持ちのせいで、焦点を合わせようとして目を酷使することになり、疲れを感じたり、心が折れたりします。</span><span class="sxs-lookup"><span data-stu-id="9359a-203">This straining feeling on your eyes that cause you to feel tired and worn out because you try to readjust your eyes to focus better?</span></span>
<span data-ttu-id="9359a-204">これが、目によるターゲット設定を使用したアプリで小さすぎるターゲットを選択させたときにユーザーに与える可能性のある気持ちです。</span><span class="sxs-lookup"><span data-stu-id="9359a-204">This is a feeling you may invoke in your users when forcing them to select too small targets in your app using eye targeting.</span></span>
<span data-ttu-id="9359a-205">設計に際しては、ユーザーにとって楽しく快適なエクスペリエンスを作るため、ターゲットを視角で 2 度以上にする (さらに大きい方が好ましい) ことをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="9359a-205">For your design, to create a pleasant and comfortable experience for your users, we recommend that targets should be at least 2° in visual angle, preferably larger.</span></span>

- <span data-ttu-id="9359a-206">**不規則な目の視線入力の動き** 人間の目はすばやく移動し、あらゆるものを次々に凝視します。</span><span class="sxs-lookup"><span data-stu-id="9359a-206">**Ragged eye gaze movements** Our eyes perform rapid movements from fixation to fixation.</span></span> <span data-ttu-id="9359a-207">記録された目の動きのスキャン パスを見ると、視線が不規則に動いていることがわかります。</span><span class="sxs-lookup"><span data-stu-id="9359a-207">If you look at scan paths of recorded eye movements, you can see that they look ragged.</span></span> <span data-ttu-id="9359a-208">*頭の視線入力*や*手の動き*に比べて、目の動きはすばやく、無意識のうちに突然動くことがあります。</span><span class="sxs-lookup"><span data-stu-id="9359a-208">Your eyes move quickly and in spontaneous jumps in comparison to *head gaze* or *hand motions*.</span></span>  

- <span data-ttu-id="9359a-209">**追跡の信頼性:** 視線追跡の精度は、目が新しい条件に合わせて調整したときのほんの少しの光の変化で低下する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-209">**Tracking reliability:** Eye tracking accuracy may degrade a little in changing light as your eye adjust to the new conditions.</span></span>
<span data-ttu-id="9359a-210">これはアプリの設計に必ずしも影響しませんが、精度は前述の 2 度の制限内にする必要があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-210">While this should not necessarily affect your app design, as the accuracy should be within the above mentioned limitation of 2°.</span></span> <span data-ttu-id="9359a-211">これは、ユーザーがもう一度調整を行わなければならない場合もあるということです。</span><span class="sxs-lookup"><span data-stu-id="9359a-211">It may mean that the user has to run another calibration.</span></span> 


### <a name="design-recommendations"></a><span data-ttu-id="9359a-212">設計の推奨事項</span><span class="sxs-lookup"><span data-stu-id="9359a-212">Design recommendations</span></span>
<span data-ttu-id="9359a-213">ここでは、説明した目の視線入力による入力のメリットと課題に基づき、設計に関する特定の推奨事項を一覧表示します。</span><span class="sxs-lookup"><span data-stu-id="9359a-213">In the following, we list specific design recommendations based on the described advantages and challenges for eye gaze input:</span></span>

1. <span data-ttu-id="9359a-214">**目の視線入力 != 頭の視線入力:**</span><span class="sxs-lookup"><span data-stu-id="9359a-214">**Eye gaze != Head gaze:**</span></span>
    - <span data-ttu-id="9359a-215">**高速だが不規則な目の動きが入力タスクに合っているかどうかを検討する:** 高速で不規則な目の動きは視界全体から迅速にターゲット選択する場合に適していますが、滑らかな入力軌道が必要なタスク (描画や注釈を囲むなど) にはあまり適していません。</span><span class="sxs-lookup"><span data-stu-id="9359a-215">**Consider whether fast yet ragged eye movements fit your input task:** While our fast and ragged eye movements are great to quickly select targets across our Field of View, it is less applicable for tasks that require smooth input trajectories (e.g., for drawing or encircling annotations).</span></span> <span data-ttu-id="9359a-216">この場合は、手または頭によるポインティングをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="9359a-216">In this case, hand or head pointing should be preferred.</span></span>
  
    - <span data-ttu-id="9359a-217">**ユーザーの目の視線入力に何か (スライダーやカーソルなど) を直接に取り付けるのは避けてください。**</span><span class="sxs-lookup"><span data-stu-id="9359a-217">**Avoid attaching something directly to the user’s eye gaze (e.g., a slider or cursor).**</span></span>
<span data-ttu-id="9359a-218">カーソルの場合は、投影された目の視線入力信号のわずかなオフセットのせいで「逃げるカーソル」効果が発生する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-218">In case of a cursor, this may result in the “fleeing cursor” effect due to slight offsets in the projected eye gaze signal.</span></span> <span data-ttu-id="9359a-219">スライダーの場合は、目でスライダーを制御しながら、対象物が正しい位置にあるかどうかも確認したいという二重の役割に抵触します。</span><span class="sxs-lookup"><span data-stu-id="9359a-219">In case of a slider, it conflicts with the double role of controlling the slider with your eyes while also wanting to check whether the object is at the correct location.</span></span> <span data-ttu-id="9359a-220">簡単に言えば、ユーザーは、特に自分に関する信号が不正確な場合、たちまち混乱したり、集中できなくなったりする可能性があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-220">In a nutshell, users may quickly feel overwhelmed and distracted, especially if the signal is imprecise for that user.</span></span> 
  
2. <span data-ttu-id="9359a-221">**目の視線入力と他の入力を組み合わせる:** 視線追跡と、手のジェスチャ、音声コマンド、ボタン押下などの他の入力との統合には、いくつかのメリットがあります。</span><span class="sxs-lookup"><span data-stu-id="9359a-221">**Combine eye gaze with other inputs:** The integration of Eye Tracking with other inputs, such as hand gestures, voice commands or button presses, serves several advantages:</span></span>
    - <span data-ttu-id="9359a-222">**自由観察を可能にする:** 目の主要な役割が環境を観察することだとすると、ユーザーが (視覚や聴覚などの) フィードバックやアクションをトリガーすることなく周囲を見回すことができるようにすることが重要です。</span><span class="sxs-lookup"><span data-stu-id="9359a-222">**Allow for free observation:** Given that the main role of our eyes is to observe our environment, it is important to allow users to look around without triggering any (visual, auditory, ...) feedback or actions.</span></span> 
    <span data-ttu-id="9359a-223">別の入力コントロールと ET を組み合わせることにより、ET 観察モードと入力コントロール モード間のスムーズな遷移が可能になります。</span><span class="sxs-lookup"><span data-stu-id="9359a-223">Combining ET with another input control allows for smoothly transitioning between ET observation and input control modes.</span></span>
  
    - <span data-ttu-id="9359a-224">**強力なコンテキスト プロバイダー:** 音声コマンドを発音したり、手のジェスチャを実行したりするときに、ユーザーが見ている場所に関する情報を使用すれば、視界全体での入力のチャネル処理が楽に行えます。</span><span class="sxs-lookup"><span data-stu-id="9359a-224">**Powerful context provider:** Using information about where the user is looking at while uttering a voice command or performing a hand gesture allows for effortlessly channeling the input across the field-of-view.</span></span> <span data-ttu-id="9359a-225">たとえば、次のものがあります。「Put that there (それをあそこに置いて)」と言うときに、単にターゲットと宛先を見るだけで、シーンの中でホログラムを選択して配置する作業を迅速かつ滑らかに行えます。</span><span class="sxs-lookup"><span data-stu-id="9359a-225">Examples include: “Put that there” to quickly and fluently select and position a hologram across the scene by simply looking at a target and destination.</span></span> 

    - <span data-ttu-id="9359a-226">**マルチモーダル入力を同期する必要性 (「クリック前に離れる」問題):** 迅速な目の動きと、より複雑な追加入力 (長い音声コマンドや手のジェスチャなど) を組み合わせると、追加の入力コマンドが完了する前に目の視線入力が動いてしまう可能性があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-226">**Need for synchronizing multimodal inputs (“leave before click” issue):** Combining rapid eye movements with more complex additional inputs (e.g., long voice commands or hand gestures) bears the risk of moving on with your eye gaze before finishing the additional input command.</span></span> <span data-ttu-id="9359a-227">そのため、独自の入力コントロール (手のカスタム ジェスチャなど) を作成する場合は、この入力の開始またはおおよその持続時間をログに記録して、それをユーザーが前に凝視していたものと関連付けてください。</span><span class="sxs-lookup"><span data-stu-id="9359a-227">Hence, if you create your own input controls (e.g., custom hand gestures), make sure to log the onset of this input or approximate duration to correlate it with what a user had fixated on in the past.</span></span>
    
3. <span data-ttu-id="9359a-228">**視線追跡入力の繊細なフィードバック:** ターゲットが見つめられたときにフィードバックを提供するのは有用ですが (システムが意図どおりに機能していることを示すため)、繊細さを保つ必要があります。</span><span class="sxs-lookup"><span data-stu-id="9359a-228">**Subtle feedback for eye tracking input:** It is useful to provide feedback if a target is looked at (to indicate that the system is working as intended) but should be kept subtle.</span></span> <span data-ttu-id="9359a-229">これにはビジュアル ハイライトのゆっくりしたブレンドインまたはブレンドアウトや、他の繊細なターゲットの動作が含まれます。たとえば、低速モーション (ターゲットが若干大きくなる) などを使って、ユーザーがターゲットを見つめたことがシステムによって正しく検出されたことを示します。しかし、ユーザーの現在のワークフローを不必要に中断させることは避けます。</span><span class="sxs-lookup"><span data-stu-id="9359a-229">This may include slowly blending in/out visual highlights or perform other subtle target behaviors, such as slow motions (e.g., slightly increasing the target) to indicate that the system correctly detected that the user is looking at a target, however, without unnecessarily interrupting the user’s current workflow.</span></span> 

4. <span data-ttu-id="9359a-230">**入力としての不自然な目の動きを強制しない:** アプリ内のアクションをトリガーするために特定の目の動き (視線入力ジェスチャ) を実行するようにユーザーに強制しないでください。</span><span class="sxs-lookup"><span data-stu-id="9359a-230">**Avoid enforcing unnatural eye movements as input:** Do not force users to perform specific eye movements (gaze gestures) to trigger actions in your app.</span></span>

5. <span data-ttu-id="9359a-231">**不正確さを考慮に入れる:** ユーザーに容易に気付かれる 2 種類の不正確さが特定されています。オフセットとジッターです。</span><span class="sxs-lookup"><span data-stu-id="9359a-231">**Account for imprecisions:** We distinguish two types of imprecisions which are noticeable to users: Offset and Jitter.</span></span> <span data-ttu-id="9359a-232">オフセットを解消する簡単な方法は、操作する対象として十分大きいターゲットを用意することです (視角で 2 度以上 – 参考までにお知らせすると、腕を伸ばしたとき、親指の爪の視角は約 2 度です (1))。</span><span class="sxs-lookup"><span data-stu-id="9359a-232">The easiest way to address offsets is to provide sufficiently large targets to interact with (> 2° in visual angle – as reference: your thumbnail is about 2° in visual angle when you stretch out your arm (1)).</span></span> <span data-ttu-id="9359a-233">これが次のガイダンスにつながります。</span><span class="sxs-lookup"><span data-stu-id="9359a-233">This leads to the following guidance:</span></span>
    - <span data-ttu-id="9359a-234">小さいターゲットを選択するようにユーザーに強制しないでください。調査では、ターゲットが十分大きい (かつシステムが適切に設計されている) 場合は、ユーザーが操作が簡単で魔法のようだと答えていることがわかりました。</span><span class="sxs-lookup"><span data-stu-id="9359a-234">Do not force users to select tiny targets: Research has shown that if targets are sufficiently large (and the system is designed well), users describe the interaction as effortless and magical.</span></span> <span data-ttu-id="9359a-235">ターゲットを小さくしすぎると、ユーザーはエクスペリエンスを疲れる、いらだたしいものと表現します。</span><span class="sxs-lookup"><span data-stu-id="9359a-235">If targets become too small, users describe the experience as fatiguing and frustrating.</span></span>
   

## <a name="see-also"></a><span data-ttu-id="9359a-236">関連項目</span><span class="sxs-lookup"><span data-stu-id="9359a-236">See also</span></span>
* [<span data-ttu-id="9359a-237">頭の視線入力とコミット</span><span class="sxs-lookup"><span data-stu-id="9359a-237">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="9359a-238">DirectX でのヘッド視線入力とアイ視線入力</span><span class="sxs-lookup"><span data-stu-id="9359a-238">Head and eye gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="9359a-239">Unity (Mixed Reality Toolkit) の目の視線入力</span><span class="sxs-lookup"><span data-stu-id="9359a-239">Eye gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="9359a-240">手のジェスチャ</span><span class="sxs-lookup"><span data-stu-id="9359a-240">Hand gestures</span></span>](gestures.md)
* [<span data-ttu-id="9359a-241">音声入力</span><span class="sxs-lookup"><span data-stu-id="9359a-241">Voice input</span></span>](voice-design.md)
* [<span data-ttu-id="9359a-242">モーション コントローラー</span><span class="sxs-lookup"><span data-stu-id="9359a-242">Motion controllers</span></span>](motion-controllers.md)
* [<span data-ttu-id="9359a-243">快適性</span><span class="sxs-lookup"><span data-stu-id="9359a-243">Comfort</span></span>](comfort.md)
