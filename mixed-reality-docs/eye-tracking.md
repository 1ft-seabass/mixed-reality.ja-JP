---
title: 視線
description: HoloLens 2 を使用すると、開発者はユーザーが見ているものについての情報を使用できるので、ホログラフィック エクスペリエンスにおけるコンテキストと人間の理解が大きく進みます。
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
keywords: 視線追跡、Mixed Reality、インプット、視線
ms.openlocfilehash: 51779b7b210522aa4d19b5a32d7df6ccb2cb3a35
ms.sourcegitcommit: ff330a7e36e5ff7ae0e9a08c0e99eb7f3f81361f
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/28/2019
ms.locfileid: "70122065"
---
# <a name="eye-gaze-on-hololens-2"></a><span data-ttu-id="33468-104">HoloLens 2 での視線</span><span class="sxs-lookup"><span data-stu-id="33468-104">Eye-gaze on HoloLens 2</span></span>
<span data-ttu-id="33468-105">HoloLens 2 を使用すると、開発者はユーザーが見ているものについての情報を使用できるので、ホログラフィック エクスペリエンスにおけるコンテキストと人間の理解が大きく進みます。</span><span class="sxs-lookup"><span data-stu-id="33468-105">HoloLens 2 allows for a new level of context and human understanding within the holographic experience by providing developers with the ability of using information about what users are looking at.</span></span> <span data-ttu-id="33468-106">このページでは、さまざまなユースケースの目の追跡や、視線を使用したユーザーインターフェイスの設計時に見られることについて、どのように役立つかを開発者に通知します。</span><span class="sxs-lookup"><span data-stu-id="33468-106">This page tells developers how they can benefit from eye tracking for various use cases as well as what to look for when designing eye-gaze-based user interfaces.</span></span> 


## <a name="device-support"></a><span data-ttu-id="33468-107">デバイスのサポート</span><span class="sxs-lookup"><span data-stu-id="33468-107">Device support</span></span>

<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><span data-ttu-id="33468-108"><strong>機能</strong></span><span class="sxs-lookup"><span data-stu-id="33468-108"><strong>Feature</strong></span></span></td>
     <td><span data-ttu-id="33468-109"><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></span><span class="sxs-lookup"><span data-stu-id="33468-109"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
     <td><span data-ttu-id="33468-110"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="33468-110"><strong>HoloLens 2</strong></span></span></td>
     <td><span data-ttu-id="33468-111"><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></span><span class="sxs-lookup"><span data-stu-id="33468-111"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
</tr>
<tr>
     <td><span data-ttu-id="33468-112">視線</span><span class="sxs-lookup"><span data-stu-id="33468-112">Eye-gaze</span></span></td>
     <td><span data-ttu-id="33468-113">❌</span><span class="sxs-lookup"><span data-stu-id="33468-113">❌</span></span></td>
     <td><span data-ttu-id="33468-114">✔️</span><span class="sxs-lookup"><span data-stu-id="33468-114">✔️</span></span></td>
     <td><span data-ttu-id="33468-115">❌</span><span class="sxs-lookup"><span data-stu-id="33468-115">❌</span></span></td>
</tr>
</table>

## <a name="use-cases"></a><span data-ttu-id="33468-116">使用事例</span><span class="sxs-lookup"><span data-stu-id="33468-116">Use cases</span></span>
<span data-ttu-id="33468-117">視線追跡を使用すれば、アプリケーションは、ユーザーが見ている場所をリアルタイムで追跡できます。</span><span class="sxs-lookup"><span data-stu-id="33468-117">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="33468-118">次のユースケースでは、mixed reality での視線追跡で可能な相互作用について説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-118">The following use cases describe some interactions that are possible with eye tracking in mixed reality.</span></span>
<span data-ttu-id="33468-119">[混合現実のツールキット](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)は、目を通して見やすいように、目を追って見て見やすくするための便利で強力な例をいくつか提供していることに注意してください。ユーザーが見ている内容。</span><span class="sxs-lookup"><span data-stu-id="33468-119">Keep in mind that the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) is useful for providing several interesting and powerful examples for using eye tracking, such as quick and effortless eye-supported target selections as well as automatically scrolling through text based on what the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="33468-120">ユーザー意図</span><span class="sxs-lookup"><span data-stu-id="33468-120">User intent</span></span>    
<span data-ttu-id="33468-121">ユーザーがどこで見ているかについての情報は、音声、ハンド、コントローラーなど**の他の入力に対し**て強力なコンテキストを提供します。</span><span class="sxs-lookup"><span data-stu-id="33468-121">Information about where and what a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="33468-122">これは、さまざまなタスクに利用できます。</span><span class="sxs-lookup"><span data-stu-id="33468-122">This can be used for various tasks.</span></span>
<span data-ttu-id="33468-123">たとえば、ホログラムを見て「選択」と指示するだけで (「選択」を参照して[ください)、](gaze-and-commit.md)"select" を言い、 *"put the this..."* を言い、ユーザーがどこにいるかを調べることによって、シーン全体を**対象**とすることができます。ホログラムを配置したい*と言います。"*</span><span class="sxs-lookup"><span data-stu-id="33468-123">For example, this can range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying "select" (also see [Head-gaze and commit](gaze-and-commit.md)) or by saying *"put this..."*, then looking over to where the user wants to place the hologram and say *"...there"*.</span></span> <span data-ttu-id="33468-124">この例は、「[Mixed Reality Toolkit - Eye-supported Target Selection (Mixed Reality Toolkit - 目で支援するターゲット選択)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)」と「[Mixed Reality Toolkit - Eye-supported Target Positioning (Mixed Reality Toolkit - 目で支援するターゲット配置)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)」に記載されています。</span><span class="sxs-lookup"><span data-stu-id="33468-124">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="33468-125">さらに、ユーザーの目的の例として、ユーザーが参照する情報を使用して、埋め込み仮想エージェントや対話型ホログラムによるエンゲージメントを強化することができます。</span><span class="sxs-lookup"><span data-stu-id="33468-125">Additionally, an example for user intent might include using information about what users look at to enhance engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="33468-126">たとえば、仮想エージェントは、現在表示されているコンテンツに基づいて、使用可能なオプションとその動作を適合させる場合があります。</span><span class="sxs-lookup"><span data-stu-id="33468-126">For instance, virtual agents might adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="33468-127">暗黙的アクション</span><span class="sxs-lookup"><span data-stu-id="33468-127">Implicit actions</span></span>
<span data-ttu-id="33468-128">暗黙的アクションのカテゴリは、ユーザー意図に密接に関係しています。</span><span class="sxs-lookup"><span data-stu-id="33468-128">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="33468-129">これは、ホログラムまたはユーザーインターフェイスの要素が多少 instinctual な方法で対応することで、ユーザーがシステムと対話するのではなく、システムとユーザーが同期しているように感じる可能性があるということではないかもしれません。一例として、ユーザーがテキストを読み取ったときに自動的にスクロールが開始され、ユーザーがテキストボックスの一番下に移動したときに自動的にスクロールが開始されるような**自動スクロール**があります。</span><span class="sxs-lookup"><span data-stu-id="33468-129">The idea is that holograms or user interface elements react in a somewhat instinctual way that may not even feel like the user is interacting with the system at all, but rather that the system and the user are in sync. One example is **eye-gaze-based auto scroll** where the user can read a long text which automatically starts scrolling once the user gets to the bottom of the textbox to keep the user in the flow of reading without lifting a finger.</span></span>  
<span data-ttu-id="33468-130">この重要な点は、スクロール速度がユーザーの読み取り速度に適応することです。</span><span class="sxs-lookup"><span data-stu-id="33468-130">A key aspect of this is that the scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="33468-131">もう1つの例として、視線がサポートされている**ズームとパン**があります。ユーザーは、フォーカスされている内容を正確に把握することができます。</span><span class="sxs-lookup"><span data-stu-id="33468-131">Another example is **eye-supported zoom and pan** where the user can feel like diving exactly toward what he or she is focused on.</span></span> <span data-ttu-id="33468-132">ズームのトリガーとズーム速度の制御は、音声入力または手書き入力によって制御できます。これは、ユーザーがコントロールの感覚を持つことができるようにするために重要です。</span><span class="sxs-lookup"><span data-stu-id="33468-132">Triggering zoom and controlling zoom speed can be controlled by voice or hand input, which is important for providing the user with the feeling of control while avoiding being overwhelmed.</span></span> <span data-ttu-id="33468-133">これらの設計ガイドラインについては、以下で詳しく説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-133">We will talk about these design guidelines in more detail below.</span></span> <span data-ttu-id="33468-134">拡大した後、ユーザーは、目を見つめて使用するだけで、自宅などの道をスムーズにたどることができます。</span><span class="sxs-lookup"><span data-stu-id="33468-134">Once zoomed in, the user can  smoothly follow, for example, the course of a street to explore his or her neighborhood by simply using their eye-gaze.</span></span>
<span data-ttu-id="33468-135">この種の相互作用に関するデモの例は、「[Mixed Reality Toolkit - Eye-supported Navigation (Mixed Reality Toolkit - 目で支援するナビゲーション)](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)」のサンプルを参照してください。</span><span class="sxs-lookup"><span data-stu-id="33468-135">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="33468-136">_暗黙的なアクション_の追加のユースケースには、次のものが含まれます。</span><span class="sxs-lookup"><span data-stu-id="33468-136">Additional use cases for _implicit actions_ can include:</span></span>
- <span data-ttu-id="33468-137">**スマート通知:** 注目していた場所に通知がポップアップ表示されて不快に思ったことはありませんか?</span><span class="sxs-lookup"><span data-stu-id="33468-137">**Smart notifications:** Ever get annoyed by notifications popping up right where you were focusing?</span></span> <span data-ttu-id="33468-138">ユーザーがどのようなことに注目しているかを考慮して、ユーザーが現在使用している場所から通知をオフセットすることで、このエクスペリエンスを向上させることができます。</span><span class="sxs-lookup"><span data-stu-id="33468-138">Taking into account what a user is paying attention to, you can make this experience better by offsetting notifications from where the user is currently gazing.</span></span> <span data-ttu-id="33468-139">これにより、取られるが制限され、ユーザーの読み取りが完了すると自動的に破棄されます。</span><span class="sxs-lookup"><span data-stu-id="33468-139">This limits distractions and automatically dismisses them once the user is finished reading.</span></span> 
- <span data-ttu-id="33468-140">**気が利くホログラム:** Gazed 時に微妙に反応するホログラム。</span><span class="sxs-lookup"><span data-stu-id="33468-140">**Attentive holograms:** Holograms that subtly react when being gazed upon.</span></span> <span data-ttu-id="33468-141">これには、わずかに光る UI 要素から、ユーザーに見てみることができるように、または長時間の確認の後にユーザーの目を見つめないようにするために、わずかな咲きの花から仮想ペットまで、さまざまなものがあります。</span><span class="sxs-lookup"><span data-stu-id="33468-141">This can range from slightly glowing UI elements to a slowly blooming flower to a virtual pet starting to look back at the user or trying to avoid the user's eye-gaze after a prolonged stare.</span></span> <span data-ttu-id="33468-142">この相互作用によって、アプリケーションでの接続性と満足度がわかりやすくなる場合があります。</span><span class="sxs-lookup"><span data-stu-id="33468-142">This interaction might provide an interesting sense of connectivity and satisfaction in your application.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="33468-143">注意追跡</span><span class="sxs-lookup"><span data-stu-id="33468-143">Attention tracking</span></span>   
<span data-ttu-id="33468-144">どのようなユーザーが見ているかについての情報は、設計の使いやすさを評価し、効率的なワークフローの問題を特定するための非常強力なツールです。</span><span class="sxs-lookup"><span data-stu-id="33468-144">Information about where or what users look at is an immensely powerful tool to assess usability of designs, and to identify problems in efficient workflows.</span></span> <span data-ttu-id="33468-145">さまざまなアプリケーション領域では、視線追跡の視覚化と分析が一般的な方法です。</span><span class="sxs-lookup"><span data-stu-id="33468-145">Eye tracking visualization and analytics are a common practice in various application areas.</span></span> <span data-ttu-id="33468-146">HoloLens 2 では、このことを理解するために新しいディメンションを提供しています。これは、3D ホログラムを実際のコンテキストで配置し、それに従って評価することができるためです。</span><span class="sxs-lookup"><span data-stu-id="33468-146">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed accordingly.</span></span> <span data-ttu-id="33468-147">[Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)には、視線追跡データをログに記録して読み込む方法と、それらを視覚化する方法の基本的な例が用意されています。</span><span class="sxs-lookup"><span data-stu-id="33468-147">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and  how to visualize them.</span></span>

<span data-ttu-id="33468-148">この領域の他のアプリケーションには、次のものが含まれます。</span><span class="sxs-lookup"><span data-stu-id="33468-148">Other applications in this area can include:</span></span> 
-   <span data-ttu-id="33468-149">**リモートの視線可視化:** リモートコラボレーターが見ていることを視覚化し、命令が正しく認識されているかどうかを確認します。</span><span class="sxs-lookup"><span data-stu-id="33468-149">**Remote eye-gaze visualization:** Visualize what remote collaborators are looking at to ensure whether instructions are correctly understood and followed.</span></span>
-   <span data-ttu-id="33468-150">**ユーザー調査研究:** アテンション追跡を使用すると、初心者と専門家のユーザーが視覚的にコンテンツを分析する方法や、医療データを分析したり、運用している場合などの複雑なタスクに対する手作業の調整方法を調べることができます。</span><span class="sxs-lookup"><span data-stu-id="33468-150">**User research studies:** Attention tracking can be used to explore the way novice vs. expert users visually analyze content or how their hand-eye-coordination for complex tasks, such as for analysis of medical data or while operating machinery.</span></span>
-   <span data-ttu-id="33468-151">**トレーニング シミュレーションとパフォーマンス監視:** 実行フローにおけるボトルネックをより効率的に特定することにより、タスクの実行を練習して最適化します。</span><span class="sxs-lookup"><span data-stu-id="33468-151">**Training simulations and Performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span>
-   <span data-ttu-id="33468-152">**デザイン評価、宣伝、市場調査:** 目の追跡は、web サイトと製品の設計を評価する際に市場調査を行うための一般的なツールです。</span><span class="sxs-lookup"><span data-stu-id="33468-152">**Design evaluations, advertisement and marketing research:** Eye tracking is a common tool for market research when evaluating website and product designs.</span></span>

### <a name="additional-use-cases"></a><span data-ttu-id="33468-153">その他の使用事例</span><span class="sxs-lookup"><span data-stu-id="33468-153">Additional use cases</span></span>
- <span data-ttu-id="33468-154">**ゲーム:** スーパーパワーが欲しくなったことはありませんか?</span><span class="sxs-lookup"><span data-stu-id="33468-154">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="33468-155">こつをお教えしましょう。</span><span class="sxs-lookup"><span data-stu-id="33468-155">Here's your chance!</span></span> <span data-ttu-id="33468-156">ホログラムを levitate することで、ホログラムを作成できます。</span><span class="sxs-lookup"><span data-stu-id="33468-156">You can levitate holograms by staring at them.</span></span> <span data-ttu-id="33468-157">目からレーザー ビームを発射します。</span><span class="sxs-lookup"><span data-stu-id="33468-157">Shoot laser beams from your eyes.</span></span> <span data-ttu-id="33468-158">敵を石にするか、固定します。</span><span class="sxs-lookup"><span data-stu-id="33468-158">Turn enemies into stone or freeze them.</span></span> <span data-ttu-id="33468-159">透視能力を使ってビルを探索します。</span><span class="sxs-lookup"><span data-stu-id="33468-159">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="33468-160">使い道は想像力次第です。</span><span class="sxs-lookup"><span data-stu-id="33468-160">Your imagination is the limit!</span></span>  

- <span data-ttu-id="33468-161">**表情豊かなアバター:** 視線追跡では、ライブ視線追跡データを使用して、ユーザーがどのようなものかを示すアバターの目をアニメーション化することで、より表現力の高い3D アバターを支援します。</span><span class="sxs-lookup"><span data-stu-id="33468-161">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking data to animate the avatar's eyes that indicate what the user is looking at.</span></span> 

- <span data-ttu-id="33468-162">**テキスト入力:** 視力の追跡は、特に音声や手の使用が不便な場合に、低労力のテキスト入力の代替手段として使用できます。</span><span class="sxs-lookup"><span data-stu-id="33468-162">**Text entry:** Eye tracking can be used as an alternative for low-effort text entry, especially when speech or hands are inconvenient to use.</span></span> 


## <a name="available-eye-tracking-data"></a><span data-ttu-id="33468-163">利用可能なアイトラッキングデータ</span><span class="sxs-lookup"><span data-stu-id="33468-163">Available eye tracking data</span></span>
<span data-ttu-id="33468-164">視線の相互作用に関する特定の設計ガイドラインについて詳しく説明する前に、HoloLens 2[目の追跡 API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose)によって提供される機能について簡単に説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-164">Before going into detail about the specific design guidelines for eye-gaze interaction, we want to briefly point out the capabilities that the HoloLens 2 [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) provides.</span></span> <span data-ttu-id="33468-165">開発者は約_30 FPS (60 Hz)_ で、1つの目を見つめた射線 (宝石の出発点と向き) にアクセスできます。</span><span class="sxs-lookup"><span data-stu-id="33468-165">Developers get access to a single eye-gaze ray (gaze origin and direction) at approximately _30 FPS (60 Hz)_.</span></span>
<span data-ttu-id="33468-166">視線追跡データへのアクセス方法の詳細については、「開発者ガイド」を参照してください。 [DirectX での視線](gaze-in-directx.md)と、 [Unity で](https://aka.ms/mrtk-eyes)の視線の使用に関するガイドを参照してください。</span><span class="sxs-lookup"><span data-stu-id="33468-166">For more detailed information about how to access eye tracking data, please refer to our developer guides on using [eye-gaze in DirectX](gaze-in-directx.md) and [eye-gaze in Unity](https://aka.ms/mrtk-eyes).</span></span>

<span data-ttu-id="33468-167">予測された視線は、実際のターゲットを中心に約1.5 °の範囲で表示されます (次の図を参照してください)。</span><span class="sxs-lookup"><span data-stu-id="33468-167">The predicted eye-gaze is approximately within 1.5 degrees in visual angle around the actual target (see the illustration below).</span></span> <span data-ttu-id="33468-168">わずかな不正確性が想定されているため、開発者はこの下限値の周りにいくらかの余白を計画する必要があります (たとえば、2.0-3.0 度では、より快適なエクスペリエンスが得られる可能性があります)。</span><span class="sxs-lookup"><span data-stu-id="33468-168">As slight imprecisions are expected, developers should plan for some margin around this lower bound value (e.g., 2.0-3.0 degrees may result in a much more comfortable experience).</span></span> <span data-ttu-id="33468-169">以下では、小規模なターゲットの選択に対処する方法について説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-169">We will discuss how to address the selection of small targets in more detail below.</span></span> <span data-ttu-id="33468-170">視線追跡が正しく機能するためには、各ユーザーが視線追跡ユーザー調整を行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="33468-170">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="33468-171">![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="33468-171">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="33468-172">*2メートル距離で最適なターゲットサイズ*</span><span class="sxs-lookup"><span data-stu-id="33468-172">*Optimal target size at a 2-meter distance*</span></span>

## <a name="calibration"></a><span data-ttu-id="33468-173">目盛り</span><span class="sxs-lookup"><span data-stu-id="33468-173">Calibration</span></span> 
<span data-ttu-id="33468-174">視線追跡を正確に機能させるには、各ユーザーが、一連の holographic ターゲットを確認する必要がある、[目の追跡ユーザーの調整](calibration.md)を行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="33468-174">For eye tracking to work accurately, each user is required to go through an [eye tracking user calibration](calibration.md) for which the user has to look at a set of holographic targets.</span></span> <span data-ttu-id="33468-175">これにより、デバイスはシステムを調整して、より快適で品質の高い閲覧エクスペリエンスをユーザーに提供し、同時に正確な視点を追跡することができます。</span><span class="sxs-lookup"><span data-stu-id="33468-175">This allows the device to adjust the system for a more comfortable and higher quality viewing experience for the user and to ensure accurate eye tracking at the same time.</span></span> <span data-ttu-id="33468-176">視線追跡はほとんどのユーザーに対して機能しますが、ユーザーが正常に調整できない場合もあります。</span><span class="sxs-lookup"><span data-stu-id="33468-176">Eye tracking should work for most users, but there are cases in which a user might be unable to calibrate successfully.</span></span>
<span data-ttu-id="33468-177">調整の詳細については、[調整](calibration.md)を確認してください。</span><span class="sxs-lookup"><span data-stu-id="33468-177">To learn more about the calibration, please check [Calibration](calibration.md).</span></span>

## <a name="eye-gaze-input-design-guidelines"></a><span data-ttu-id="33468-178">視線入力のデザインガイドライン</span><span class="sxs-lookup"><span data-stu-id="33468-178">Eye-gaze input design guidelines</span></span>
<span data-ttu-id="33468-179">高速移動の視点を利用する相互作用を構築するのは困難な場合があります。</span><span class="sxs-lookup"><span data-stu-id="33468-179">Building an interaction that takes advantage of fast-moving eye targeting can be challenging.</span></span> <span data-ttu-id="33468-180">このセクションでは、アプリケーションの設計時に考慮する必要がある主な利点と課題をまとめます。</span><span class="sxs-lookup"><span data-stu-id="33468-180">In this section, we summarize the key advantages and challenges to consider when designing your application.</span></span> 

### <a name="benefits-of-eye-gaze-input"></a><span data-ttu-id="33468-181">視線入力の利点</span><span class="sxs-lookup"><span data-stu-id="33468-181">Benefits of eye-gaze input</span></span>
- <span data-ttu-id="33468-182">**高速ポインティング。**</span><span class="sxs-lookup"><span data-stu-id="33468-182">**High speed pointing.**</span></span> <span data-ttu-id="33468-183">視線筋肉は、人間の本文で最も速く反応する筋肉です。</span><span class="sxs-lookup"><span data-stu-id="33468-183">The eye muscle is the fastest reacting muscle in the human body.</span></span> 

- <span data-ttu-id="33468-184">**低労力。**</span><span class="sxs-lookup"><span data-stu-id="33468-184">**Low effort.**</span></span> <span data-ttu-id="33468-185">身体の動きがほとんど必要ありません。</span><span class="sxs-lookup"><span data-stu-id="33468-185">Barely any physical movements are necessary.</span></span> 

- <span data-ttu-id="33468-186">**暗黙。**</span><span class="sxs-lookup"><span data-stu-id="33468-186">**Implicitness.**</span></span> <span data-ttu-id="33468-187">多くの場合、ユーザーによって "覚えている" ことが示されます。ユーザーの目の動きに関する情報によって、ユーザーがどのターゲットに参加するかをシステムが把握できるようになります。</span><span class="sxs-lookup"><span data-stu-id="33468-187">Often described by users as "mind reading", information about a user's eye movements lets the system know which target the user plans to engage.</span></span> 

- <span data-ttu-id="33468-188">**代替入力チャネル。**</span><span class="sxs-lookup"><span data-stu-id="33468-188">**Alternative input channel.**</span></span> <span data-ttu-id="33468-189">視線を使用すると、ユーザーからの手動による調整に基づく長年の経験に基づいて、手書き入力や音声入力に対する強力なサポート入力を提供できます。</span><span class="sxs-lookup"><span data-stu-id="33468-189">Eye-gaze can provide a powerful supporting input for hand and voice input building on years of experience from users based on their hand-eye coordination.</span></span>

- <span data-ttu-id="33468-190">**視覚的注意。**</span><span class="sxs-lookup"><span data-stu-id="33468-190">**Visual attention.**</span></span> <span data-ttu-id="33468-191">もう1つの重要な利点は、ユーザーがどのようなことに注目しているかを推測できることです。</span><span class="sxs-lookup"><span data-stu-id="33468-191">Another important benefit is the possibility to infer what a user is paying attention to.</span></span> <span data-ttu-id="33468-192">これは、さまざまなアプリケーション領域において、より効率的なユーザーインターフェイスとリモート通信のためのソーシャルキューの強化を効果的に評価できるようにするために役立ちます。</span><span class="sxs-lookup"><span data-stu-id="33468-192">This can help in various application areas ranging from more effectively evaluating different designs to aiding in smarter user interfaces and enhanced social cues for remote communication.</span></span>

<span data-ttu-id="33468-193">簡単に言うと、視線を入力として使用することで、高速で簡単なコンテキスト信号を実現できます。</span><span class="sxs-lookup"><span data-stu-id="33468-193">In a nutshell, using eye-gaze as an input offers a fast and effortless contextual signal.</span></span> <span data-ttu-id="33468-194">これは、*音声*入力や*手動*入力などの他の入力と組み合わせてユーザーの意図を確認する場合に特に強力です。</span><span class="sxs-lookup"><span data-stu-id="33468-194">This is particularly powerful when combined with other inputs such as *voice* and *manual* input to confirm the user's intent.</span></span>


### <a name="challenges-of-eye-gaze-as-an-input"></a><span data-ttu-id="33468-195">入力としての目を見つめた課題</span><span class="sxs-lookup"><span data-stu-id="33468-195">Challenges of eye-gaze as an input</span></span>
<span data-ttu-id="33468-196">多くの場合、には多くの責任があります。</span><span class="sxs-lookup"><span data-stu-id="33468-196">With lots of power, comes lots of responsibility.</span></span>
<span data-ttu-id="33468-197">視線を使用して、スーパーヒーローのようなユーザーエクスペリエンスを実現することができますが、適切に対応できないことを把握しておくことも重要です。</span><span class="sxs-lookup"><span data-stu-id="33468-197">While eye-gaze can be used to create satisfying user experiences that makes you feel like a superhero, it is also important to know what it is not good at to appropriately account for this.</span></span> <span data-ttu-id="33468-198">以下では、注意すべきいくつかの*課題*について説明します。また、視線入力を操作するときの対処方法についても説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-198">The following discusses some *challenges* to consider as well as how to address them when working with eye-gaze input:</span></span> 

- <span data-ttu-id="33468-199">**視線が "常時オン" になってい**ます。目の蓋を開くと、環境内での作業が開始されます。</span><span class="sxs-lookup"><span data-stu-id="33468-199">**Your eye-gaze is "always on"** The moment you open your eye lids, your eyes start fixating on things in the environment.</span></span> <span data-ttu-id="33468-200">何かが長すぎると結果が得られない場合は、どのような外観にしても、アクションを誤って発行することになります。</span><span class="sxs-lookup"><span data-stu-id="33468-200">Reacting to every look you make and accidentally issuing actions because you looked at something for too long would result in an unsatisfying experience.</span></span>
<span data-ttu-id="33468-201">そのため、ターゲットの選択をトリガーするには、*音声コマンド*、*ハンドジェスチャ*、*ボタンクリック*、または拡張熟考を使用して、視線を組み合わせることをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="33468-201">Therefore we recommend combining eye-gaze with a *voice command*, *hand gesture*, *button click* or extended dwell to trigger the selection of a target.</span></span>
<span data-ttu-id="33468-202">また、このソリューションでは、involuntarily によって何かをトリガーすることなく、ユーザーが自由に検索できるモードを使用することもできます。</span><span class="sxs-lookup"><span data-stu-id="33468-202">This solution also allows for a mode in which the user can freely look around without being overwhelmed by involuntarily triggering something.</span></span> <span data-ttu-id="33468-203">また、この問題は、単にターゲットを見ているときにビジュアルと聴覚のフィードバックをデザインする場合にも考慮する必要があります。</span><span class="sxs-lookup"><span data-stu-id="33468-203">This issue should also be considered when designing visual and auditory feedback when merely looking at a target.</span></span>
<span data-ttu-id="33468-204">一瞬のポップアウト効果やホバー音でユーザーを混乱させないようにしてください。</span><span class="sxs-lookup"><span data-stu-id="33468-204">Do not overwhelm the user with immediate pop-out effects or hover sounds.</span></span> <span data-ttu-id="33468-205">はらみはキーです。</span><span class="sxs-lookup"><span data-stu-id="33468-205">Subtlety is key.</span></span> <span data-ttu-id="33468-206">後で、設計推奨事項について説明するときに、これに関するベスト プラクティスについても説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-206">We will discuss some best practices for this further below when talking about design recommendations.</span></span>

- <span data-ttu-id="33468-207">**監視と制御**たとえば、壁の写真を正確にためるとします。</span><span class="sxs-lookup"><span data-stu-id="33468-207">**Observation vs. control** Imagine that you want to precisely straighten a photograph on your wall.</span></span> <span data-ttu-id="33468-208">写真の縁と周囲を見て、揃っているかどうかを確認します。</span><span class="sxs-lookup"><span data-stu-id="33468-208">You look at its borders and its surroundings to see if it aligns well.</span></span> <span data-ttu-id="33468-209">次に、画像を移動するための入力として目を見つめて使用する方法を考えてみましょう。</span><span class="sxs-lookup"><span data-stu-id="33468-209">Now imagine how you would do that when you want to use your eye-gaze as an input to move the picture.</span></span> <span data-ttu-id="33468-210">難しいのではないでしょうか。</span><span class="sxs-lookup"><span data-stu-id="33468-210">Difficult, isn't it?</span></span> <span data-ttu-id="33468-211">これは、入力と制御の両方に必要な場合に、視線の2つの役割について説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-211">This describes the double role of eye-gaze when it is required both for input and control.</span></span> 

- <span data-ttu-id="33468-212">**クリックする前に離れる:** クイックターゲットを選択する場合は、ユーザーの目を見つめていて、手動でクリックする前 (たとえば、放映タップ) に移動できることが調査によって示されています。</span><span class="sxs-lookup"><span data-stu-id="33468-212">**Leave before click:** For quick target selections, research has shown that a user's eye-gaze can move on before concluding a manual click (e.g., an airtap).</span></span> <span data-ttu-id="33468-213">そのため、より低速なコントロール入力 (音声、ハンド、コントローラーなど) を使用して、高速の視線信号を同期するには、特別な注意が必要です。</span><span class="sxs-lookup"><span data-stu-id="33468-213">Hence, special attention must be paid to synchronizing the fast eye-gaze signal with slower control input (e.g., voice, hands, controller).</span></span>

- <span data-ttu-id="33468-214">**小さいターゲット:** 少し小さすぎて読むことができないテキストを読み込んだときに感じていることがわかりますか。</span><span class="sxs-lookup"><span data-stu-id="33468-214">**Small targets:** Do you know the feeling when you try to read text that is just a bit too small to read comfortably?</span></span> <span data-ttu-id="33468-215">これによって目が疲れてしまうことがあります。目を絞って再調整しようとしているため、疲れてしまうことがあります。</span><span class="sxs-lookup"><span data-stu-id="33468-215">This straining feeling on your eyes can cause you to feel tired and worn out because you try to readjust your eyes to focus better.</span></span>
<span data-ttu-id="33468-216">これは、ユーザーが視点を使用してアプリケーションで小さすぎるターゲットを選択する必要がある場合に、ユーザーに対して呼び出す可能性があります。</span><span class="sxs-lookup"><span data-stu-id="33468-216">This is a feeling you might invoke in your users when forcing them to select targets that are too small in your application using eye targeting.</span></span>
<span data-ttu-id="33468-217">設計に際しては、ユーザーにとって楽しく快適なエクスペリエンスを作るため、ターゲットを視角で 2 度以上にする (さらに大きい方が好ましい) ことをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="33468-217">For your design, to create a pleasant and comfortable experience for your users, we recommend that targets should be at least 2° in visual angle, preferably larger.</span></span>

- <span data-ttu-id="33468-218">**不規則な視点の移動**この目では、固定から固定への迅速な移動を行います。</span><span class="sxs-lookup"><span data-stu-id="33468-218">**Ragged eye-gaze movements** Our eyes perform rapid movements from fixation to fixation.</span></span> <span data-ttu-id="33468-219">記録された目の動きのスキャン パスを見ると、視線が不規則に動いていることがわかります。</span><span class="sxs-lookup"><span data-stu-id="33468-219">If you look at scan paths of recorded eye movements, you can see that they look ragged.</span></span> <span data-ttu-id="33468-220">視線は、頭の中や*手の動き*と比較して、すばやく、自然*な*ジャンプに移動します。</span><span class="sxs-lookup"><span data-stu-id="33468-220">Your eyes move quickly and in spontaneous jumps in comparison to *head-gaze* or *hand motions*.</span></span>  

- <span data-ttu-id="33468-221">**追跡の信頼性:** 視線追跡の精度は、目が新しい条件に合わせて調整したときのほんの少しの光の変化で低下する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="33468-221">**Tracking reliability:** Eye tracking accuracy may degrade a little in changing light as your eye adjust to the new conditions.</span></span>
<span data-ttu-id="33468-222">これは必ずしもアプリケーションの設計に影響を与えるわけではありませんが、精度は2°の制限内にあるため、ユーザーが再調整する必要がある場合があります。</span><span class="sxs-lookup"><span data-stu-id="33468-222">While this should not necessarily affect your application design, as the accuracy should be within the 2° limitation, i might be necessary for the user to calibrate again.</span></span> 


## <a name="design-recommendations"></a><span data-ttu-id="33468-223">設計の推奨事項</span><span class="sxs-lookup"><span data-stu-id="33468-223">Design recommendations</span></span>
<span data-ttu-id="33468-224">次に示すのは、目を見つめた入力の利点と課題に基づいた、設計に関する特定の推奨事項の一覧です。</span><span class="sxs-lookup"><span data-stu-id="33468-224">The following is a list of specific design recommendations based on the described advantages and challenges for eye-gaze input:</span></span>

1. <span data-ttu-id="33468-225">**視線は、ヘッド見つめと同じではありません。**</span><span class="sxs-lookup"><span data-stu-id="33468-225">**Eye-gaze is not the same as Head-gaze:**</span></span>
    - <span data-ttu-id="33468-226">**高速だが不規則な目の動きが入力タスクに合っているかどうかを検討する:** ビューのフィールド全体でターゲットをすばやく選択すると、高速で不規則な目の動きが非常に優れていますが、smooth input 軌道 (描画や encircling の注釈など) を必要とするタスクには適用できません。</span><span class="sxs-lookup"><span data-stu-id="33468-226">**Consider whether fast yet ragged eye movements fit your input task:** While our fast and ragged eye movements are great at quickly selecting targets across our field of view, it is less applicable for tasks that require smooth input trajectories (e.g., drawing or encircling annotations).</span></span> <span data-ttu-id="33468-227">この場合は、手または頭によるポインティングをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="33468-227">In this case, hand or head pointing should be preferred.</span></span>
  
    - <span data-ttu-id="33468-228">**ユーザーの目を見つめた (スライダーやカーソルなど) に直接接続することは避けてください。**</span><span class="sxs-lookup"><span data-stu-id="33468-228">**Avoid attaching something directly to the user’s eye-gaze (e.g., a slider or cursor).**</span></span>
<span data-ttu-id="33468-229">カーソルの場合は、投影された視線の信号のオフセットがわずかであるため、"fleeing cursor" 効果が生じる可能性があります。</span><span class="sxs-lookup"><span data-stu-id="33468-229">In case of a cursor, this may result in the “fleeing cursor” effect due to slight offsets in the projected eye-gaze signal.</span></span> <span data-ttu-id="33468-230">スライダーの場合は、スライダーを目で制御する2つの役割と競合し、オブジェクトが正しい位置にあるかどうかを確認することもできます。</span><span class="sxs-lookup"><span data-stu-id="33468-230">In case of a slider, it can conflict with the double role of controlling the slider with your eyes while also wanting to check whether the object is at the correct location.</span></span> <span data-ttu-id="33468-231">簡単に言うと、ユーザーにとっては特に、ユーザーの信号が不正確になる可能性があります。</span><span class="sxs-lookup"><span data-stu-id="33468-231">In a nutshell, users could become overwhelmed and distracted, especially if the signal is imprecise for that user.</span></span> 
  
2. <span data-ttu-id="33468-232">**視線を他の入力と結合します。** ハンドジェスチャ、音声コマンド、ボタンの押下など、他の入力との視線追跡の統合には、いくつかの利点があります。</span><span class="sxs-lookup"><span data-stu-id="33468-232">**Combine eye-gaze with other inputs:** The integration of eye tracking with other inputs, such as hand gestures, voice commands or button presses, serves several advantages:</span></span>
    - <span data-ttu-id="33468-233">**自由観察を可能にする:** 私たちの主要な役割は、環境を観察することであるということです。重要なのは、ユーザーが何も (視覚、聴覚などの) フィードバックまたはアクションをトリガーすることなく検索できることです。</span><span class="sxs-lookup"><span data-stu-id="33468-233">**Allow for free observation:** Given that the main role of our eyes is to observe our environment, it is important users are allowed to look around without triggering any (visual, auditory, etc.) feedback or actions.</span></span> 
    <span data-ttu-id="33468-234">視線追跡と別の入力コントロールを組み合わせることにより、視線監視と入力制御モードの間でスムーズに遷移できます。</span><span class="sxs-lookup"><span data-stu-id="33468-234">Combining eye tracking with another input control allows smooth transitioning between eye tracking observation and input control modes.</span></span>
  
    - <span data-ttu-id="33468-235">**強力なコンテキスト プロバイダー:** 音声コマンドを uttering したり、手の形でジェスチャを実行したりしているときに、ユーザーがどこで見ているかについての情報を使用することにより、ビューのフィールド間でシームレスに入力を channeling ことができます。</span><span class="sxs-lookup"><span data-stu-id="33468-235">**Powerful context provider:** Using information about where and what the user is looking at while uttering a voice command or performing a hand gesture allows seamlessly channeling the input across the field-of-view.</span></span> <span data-ttu-id="33468-236">以下に例を示します。「Put that there (それをあそこに置いて)」と言うときに、単にターゲットと宛先を見るだけで、シーンの中でホログラムを選択して配置する作業を迅速かつ滑らかに行えます。</span><span class="sxs-lookup"><span data-stu-id="33468-236">For example: “Put that there” to quickly and fluently select and position a hologram across the scene by simply looking at a target and destination.</span></span> 

    - <span data-ttu-id="33468-237">**マルチモーダル入力を同期する必要性 (「クリック前に離れる」問題):** 長い音声コマンドやハンドジェスチャなど、より複雑な追加入力を使用して迅速な移動を組み合わせることにより、追加の入力コマンドを完了する前に、目を見つめていくリスクが高くなります。</span><span class="sxs-lookup"><span data-stu-id="33468-237">**Need for synchronizing multimodal inputs (“leave before click” issue):** Combining rapid eye movements with more complex additional inputs, such as long voice commands or hand gestures, bears the risk of continuing your eye-gaze before finishing the additional input command.</span></span> <span data-ttu-id="33468-238">したがって、独自の入力コントロール (カスタムハンドジェスチャなど) を作成する場合は、この入力の先頭またはおおよその期間を記録して、ユーザーが過去に見た内容と関連付けられるようにしてください。</span><span class="sxs-lookup"><span data-stu-id="33468-238">Hence, if you create your own input controls (e.g., custom hand gestures), make sure to log the onset of this input or approximate duration to correlate it with what a user had looked at in the past.</span></span>
    
3. <span data-ttu-id="33468-239">**視線追跡入力の繊細なフィードバック:** システムが意図したとおりに動作していることを示すためにターゲットを検索するときにフィードバックを提供すると便利ですが、微妙に保つ必要があります。</span><span class="sxs-lookup"><span data-stu-id="33468-239">**Subtle feedback for eye tracking input:** It's useful to provide feedback when a target is looked at to indicate that the system is working as intended but should be kept subtle.</span></span> <span data-ttu-id="33468-240">これには、徐々にブレンドを行う、視覚的に強調表示する、またはターゲットのサイズを少し増やした場合など、他の微妙なターゲットの動作を実行することがありますユーザーの現在のワークフローを不必要に中断しています。</span><span class="sxs-lookup"><span data-stu-id="33468-240">This can include slowly blending, in and out, visual highlights or perform other subtle target behaviors, such as slow motions, such as slightly increasing the target size, to indicate that the system correctly detected that the user is looking at a target without unnecessarily interrupting the user’s current workflow.</span></span> 

4. <span data-ttu-id="33468-241">**入力としての不自然な目の動きを強制しない:** アプリケーションのアクションをトリガーするために、ユーザーが特定の目の動き (宝石のジェスチャ) を実行しないようにします。</span><span class="sxs-lookup"><span data-stu-id="33468-241">**Avoid enforcing unnatural eye movements as input:** Do not force users to perform specific eye movements (gaze gestures) to trigger actions in your application.</span></span>

5. <span data-ttu-id="33468-242">**不正確さを考慮に入れる:** ユーザーにとってわかりやすく、オフセットとジッターという2種類の不正確性を区別します。</span><span class="sxs-lookup"><span data-stu-id="33468-242">**Account for imprecisions:** We distinguish two types of imprecisions which are noticeable to users: offset and jitter.</span></span> <span data-ttu-id="33468-243">オフセットに対処する最も簡単な方法は、操作に十分なサイズのターゲットを提供することです。</span><span class="sxs-lookup"><span data-stu-id="33468-243">The easiest way to address an offset is to provide sufficiently large targets to interact with.</span></span> <span data-ttu-id="33468-244">2°を超える視覚的な角度を参照として使用することをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="33468-244">It is suggested that you use a visual angle greater than 2° as a reference.</span></span> <span data-ttu-id="33468-245">たとえば、arm を拡張すると、サムネイルの表示角度が約2°になります。</span><span class="sxs-lookup"><span data-stu-id="33468-245">For instance, your thumbnail is about 2° in visual angle when you stretch out your arm.</span></span> <span data-ttu-id="33468-246">これが次のガイダンスにつながります。</span><span class="sxs-lookup"><span data-stu-id="33468-246">This leads to the following guidance:</span></span>
    - <span data-ttu-id="33468-247">ユーザーが小さいターゲットを選択することを強制しないでください。</span><span class="sxs-lookup"><span data-stu-id="33468-247">Do not force users to select tiny targets.</span></span> <span data-ttu-id="33468-248">ここでは、ターゲットが十分に大きく、システムが適切に設計されている場合に、ユーザーの対話を簡単で魔法のない方法で記述しています。</span><span class="sxs-lookup"><span data-stu-id="33468-248">Research has shown that if targets are sufficiently large, and that the system is designed well, users describe their interactions as effortless and magical.</span></span> <span data-ttu-id="33468-249">ターゲットを小さくしすぎると、ユーザーはエクスペリエンスを疲れる、いらだたしいものと表現します。</span><span class="sxs-lookup"><span data-stu-id="33468-249">If targets become too small, users describe the experience as fatiguing and frustrating.</span></span>
  
## <a name="dev-guidance-what-if-eye-tracking-is-not-available"></a><span data-ttu-id="33468-250">開発ガイダンス:視線追跡を利用できない場合はどうすればよいですか。</span><span class="sxs-lookup"><span data-stu-id="33468-250">Dev guidance: What if eye tracking is not available?</span></span>
<span data-ttu-id="33468-251">次のようなさまざまな理由により、アプリが目の追跡データを受信しない場合があります。</span><span class="sxs-lookup"><span data-stu-id="33468-251">There may be situations in which your app will not receive any eye tracking data due to various reasons including but not limited to:</span></span>
* <span data-ttu-id="33468-252">ユーザーが視線追跡の調整をスキップしました。</span><span class="sxs-lookup"><span data-stu-id="33468-252">The user skipped the eye tracking calibration.</span></span>
* <span data-ttu-id="33468-253">ユーザーは調整されましたが、アプリに目の追跡データを使用するアクセス許可を付与しませんでした。</span><span class="sxs-lookup"><span data-stu-id="33468-253">The user calibrated, but decided to not give permission to your app to use their eye tracking data.</span></span>
* <span data-ttu-id="33468-254">このユーザーには、システムがまだサポートしていない、固有の眼鏡またはいくつかの目の状態があります。</span><span class="sxs-lookup"><span data-stu-id="33468-254">The user has unique eyeglasses or some eye condition that the system does not yet support.</span></span>
* <span data-ttu-id="33468-255">外部要因は、損なわれるの前に髪があることから、HoloLens バイザーや眼鏡での汚れや occlusions の強い日光となど、信頼性の高い視線を追跡します。</span><span class="sxs-lookup"><span data-stu-id="33468-255">External factors inhibiting reliable eye tracking such as smudges on the HoloLens visor or eyeglasses, intense direct sunlight and occlusions due to hair in front of the eyes.</span></span>

<span data-ttu-id="33468-256">アプリ開発者にとって、これは、視線追跡データが使用できないユーザーをサポートする方法を考慮する必要があることを意味します。</span><span class="sxs-lookup"><span data-stu-id="33468-256">For you as an app developer, this means that you need to account for how to support users for whom eye tracking data may not be available.</span></span> <span data-ttu-id="33468-257">次に、視線追跡が利用可能かどうかを検出する方法と、さまざまなアプリケーションで使用できない場合の対処方法について説明します。</span><span class="sxs-lookup"><span data-stu-id="33468-257">Below we first explain how to detect whether eye tracking is available and how to address when it is not available for different applications.</span></span>

### <a name="1-how-to-detect-that-eye-tracking-is-available"></a><span data-ttu-id="33468-258">1. 視線追跡が使用可能であることを検出する方法</span><span class="sxs-lookup"><span data-stu-id="33468-258">1. How to detect that eye tracking is available</span></span>
<span data-ttu-id="33468-259">視線追跡データを使用できるかどうかを確認するためのチェックがいくつかあります。</span><span class="sxs-lookup"><span data-stu-id="33468-259">There are a few checks to determine whether eye tracking data is available.</span></span> <span data-ttu-id="33468-260">確認...</span><span class="sxs-lookup"><span data-stu-id="33468-260">Check whether...</span></span>
* <span data-ttu-id="33468-261">...システムは、視線追跡をまったくサポートしています。</span><span class="sxs-lookup"><span data-stu-id="33468-261">... the system supports eye tracking at all.</span></span> <span data-ttu-id="33468-262">次の*メソッド*を呼び出します。[EyesPose () をサポートします。](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose.issupported#Windows_Perception_People_EyesPose_IsSupported)</span><span class="sxs-lookup"><span data-stu-id="33468-262">Call the following *method*: [Windows.Perception.People.EyesPose.IsSupported()](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose.issupported#Windows_Perception_People_EyesPose_IsSupported)</span></span>

* <span data-ttu-id="33468-263">...ユーザーが調整されています。</span><span class="sxs-lookup"><span data-stu-id="33468-263">... the user is calibrated.</span></span> <span data-ttu-id="33468-264">次の*プロパティ*を呼び出します。[EyesPose. IsCalibrationValid (Windows)](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose.iscalibrationvalid#Windows_Perception_People_EyesPose_IsCalibrationValid)</span><span class="sxs-lookup"><span data-stu-id="33468-264">Call the following *property*: [Windows.Perception.People.EyesPose.IsCalibrationValid](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose.iscalibrationvalid#Windows_Perception_People_EyesPose_IsCalibrationValid)</span></span>

* <span data-ttu-id="33468-265">...ユーザーは、目の追跡データを使用するためのアクセス許可をアプリに付与しています。現在の _' GazeInputAccessStatus '_ を取得します。</span><span class="sxs-lookup"><span data-stu-id="33468-265">... the user has given your app permission to use their eye tracking data: Retrieve the current _'GazeInputAccessStatus'_.</span></span> <span data-ttu-id="33468-266">これを行う方法の例については、「[宝石入力へのアクセスの要求](https://docs.microsoft.com/en-us/windows/mixed-reality/gaze-in-directX#requesting-access-to-gaze-input)」をご覧ください。</span><span class="sxs-lookup"><span data-stu-id="33468-266">An example on how to do this is explained at [Requesting access to gaze input](https://docs.microsoft.com/en-us/windows/mixed-reality/gaze-in-directX#requesting-access-to-gaze-input).</span></span>

<span data-ttu-id="33468-267">さらに、次に説明するように、受信した視線追跡データの更新の間にタイムアウトを追加して、またはそれ以外の方法でヘッドから宝石にフォールバックすることで、目の追跡データが古くなっていないことを確認することもできます。</span><span class="sxs-lookup"><span data-stu-id="33468-267">In addition, you may want to check that your eye tracking data is not stale by adding a timeout between received eye tracking data updates and otherwise fallback to head-gaze as discussed below.</span></span> 

<span data-ttu-id="33468-268">前述のように、視線追跡データが使用できない理由はいくつかあります。</span><span class="sxs-lookup"><span data-stu-id="33468-268">As described above, there are several reasons why eye tracking data may not be available.</span></span> <span data-ttu-id="33468-269">一部のユーザーは、目の追跡データへのアクセスを取り消すことを決定した場合がありますが、ユーザーエクスペリエンスの低下によって、視線追跡データへのアクセスを提供しないというプライバシーに対しては、これが意図していない場合があります。</span><span class="sxs-lookup"><span data-stu-id="33468-269">While some users may have consciously decided to revoke access to their eye tracking data and are ok with the trade-off of an inferior user experience to the privacy of not providing access to their eye tracking data, in some cases this may be unintentional.</span></span> <span data-ttu-id="33468-270">そのため、アプリが目の追跡を使用していて、これがエクスペリエンスの重要な部分である場合は、これをユーザーに明確に伝えることをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="33468-270">Hence, if your app uses eye tracking, and this is an important part of the experience, we recommend clearly communicating this to the user.</span></span> <span data-ttu-id="33468-271">アプリケーションの可能性を最大限に活用するために、アプリケーションに対して目の追跡が不可欠である理由をユーザーに通知します (一部の拡張機能を一覧表示することもできます)。これにより、ユーザーがどのような機能を提供しているかを理解するのに役立ちます。</span><span class="sxs-lookup"><span data-stu-id="33468-271">Kindly informing the user why eye tracking is critical for your application (maybe even listing some enhanced features) to experience the full potential of your application can help the user to better understand what they are giving up.</span></span> <span data-ttu-id="33468-272">ユーザーが、上のチェックに基づいて目の追跡が機能していない理由を特定し、潜在的な問題の迅速なトラブルシューティングを行うための提案を提供します。</span><span class="sxs-lookup"><span data-stu-id="33468-272">Help the user to identify why eye tracking may not be working (based on the above checks) and offer some suggestions to quickly troubleshoot potential issues.</span></span> <span data-ttu-id="33468-273">たとえば、システムが目の追跡をサポートしていることを検出できた場合は、ユーザーが調整され、アクセス許可が与えられているにもかかわらず、目の追跡データは表示されません。その場合は、汚れや目の occluded など、その他の問題を指している可能性があります。</span><span class="sxs-lookup"><span data-stu-id="33468-273">For example, if you can detect that the system supports eye tracking, the user is calibrated and even has given their permission, yet no eye tracking data is received, then this may point to some other issues such as smudges or the eyes being occluded.</span></span> <span data-ttu-id="33468-274">注意が必要なのは、目の追跡が機能しないユーザーのまれなケースです。</span><span class="sxs-lookup"><span data-stu-id="33468-274">Please note though that there are rare cases of users for whom eye tracking may simply not work.</span></span> <span data-ttu-id="33468-275">そのため、では、アプリで目の追跡を有効にするためのアラームを消したり、無効にしたりできるようにすることで、このことを敬意してください。</span><span class="sxs-lookup"><span data-stu-id="33468-275">Hence, please be respectful of that by allowing to dismiss or even disable reminders for enabling eye tracking in your app.</span></span>

### <a name="2-fallback-for-apps-using-eye-gaze-as-a-primary-input-pointer"></a><span data-ttu-id="33468-276">2. プライマリ入力ポインターとして視線を使用するアプリのフォールバック</span><span class="sxs-lookup"><span data-stu-id="33468-276">2. Fallback for apps using eye-gaze as a primary input pointer</span></span>
<span data-ttu-id="33468-277">アプリがポインター入力として視線を使用してシーン全体のホログラムをすばやく選択していても、目の追跡データが使用できない場合は、頭を見つめて、頭を見つめたカーソルを表示し始めることをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="33468-277">If your app uses eye-gaze as a pointer input to quickly select holograms across the scene, yet eye tracking data is unavailable, we recommend falling back to head-gaze and start showing the head-gaze cursor.</span></span> <span data-ttu-id="33468-278">切り替えるかどうかを判断するには、タイムアウト (500 ~ 1500 ミリ秒など) を使用することをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="33468-278">We recommend using a timeout (e.g., 500–1500 ms) to determine whether to switch or not.</span></span> <span data-ttu-id="33468-279">これは、システムが短時間の動きやウインクによって追跡が一時的に失われるたびに、カーソルがポップアップされるのを防ぐためです。</span><span class="sxs-lookup"><span data-stu-id="33468-279">This is to prevent popping up a cursor every time the system may briefly lose tracking due to fast eye motions or winks and blinks.</span></span> <span data-ttu-id="33468-280">Unity 開発者の場合、ヘッドの自動フォールバックは、Mixed Reality Toolkit で既に処理されています。</span><span class="sxs-lookup"><span data-stu-id="33468-280">If you are a Unity developer, the automatic fallback to head-gaze is already handled in the Mixed Reality Toolkit.</span></span> <span data-ttu-id="33468-281">DirectX 開発者は、このスイッチを自分で処理する必要があります。</span><span class="sxs-lookup"><span data-stu-id="33468-281">If you are a DirectX developer, you need to handle this switch yourself.</span></span>

### <a name="3-fallback-for-other-eye-tracking-specific-applications"></a><span data-ttu-id="33468-282">3.その他の視点を特定するアプリケーションのフォールバック</span><span class="sxs-lookup"><span data-stu-id="33468-282">3. Fallback for other eye-tracking-specific applications</span></span>
<span data-ttu-id="33468-283">アプリは、赤目を明確にするための特別な方法で、たとえば、アバターの目をアニメーション化したり、視覚的な注意に関する正確な情報に依存して目をヒートマップように調整したりすることができます。</span><span class="sxs-lookup"><span data-stu-id="33468-283">Your app may use eye-gaze in a unique way that is tailored specifically to the eyes - for example, for animating an avatar’s eyes or for eye-based attention heatmaps relying on precise information about visual attention.</span></span> <span data-ttu-id="33468-284">この場合、明確なフォールバックはありません。</span><span class="sxs-lookup"><span data-stu-id="33468-284">In this case, there is no clear fallback.</span></span> <span data-ttu-id="33468-285">視線追跡が使用できない場合は、これらの機能を無効にする必要がある場合があります。</span><span class="sxs-lookup"><span data-stu-id="33468-285">If eye tracking is not available, these capabilities may simply need to be disabled.</span></span> 

<br>

<span data-ttu-id="33468-286">このページでは、HoloLens 2 の視線追跡と視線入力の役割を理解するのに役立つ概要が提供されています。</span><span class="sxs-lookup"><span data-stu-id="33468-286">This page has hopefully provided you with a good overview to get you started understanding the role of eye tracking and eye-gaze input for HoloLens 2.</span></span> <span data-ttu-id="33468-287">開発を開始するには、 [Unity](https://aka.ms/mrtk-eyes)と、 [DirectX で](gaze-in-directx.md)の視線に関する情報を確認してください。</span><span class="sxs-lookup"><span data-stu-id="33468-287">To get started developing, check out our information on [eye-gaze in Unity](https://aka.ms/mrtk-eyes) and [eye-gaze in DirectX](gaze-in-directx.md).</span></span>


## <a name="see-also"></a><span data-ttu-id="33468-288">関連項目</span><span class="sxs-lookup"><span data-stu-id="33468-288">See also</span></span>
* [<span data-ttu-id="33468-289">DirectX での視線</span><span class="sxs-lookup"><span data-stu-id="33468-289">Eye-gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="33468-290">Unity での視線 (Mixed Reality Toolkit)</span><span class="sxs-lookup"><span data-stu-id="33468-290">Eye-gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="33468-291">調整</span><span class="sxs-lookup"><span data-stu-id="33468-291">Calibration</span></span>](calibration.md)
* [<span data-ttu-id="33468-292">頭の視線入力とコミット</span><span class="sxs-lookup"><span data-stu-id="33468-292">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="33468-293">手のジェスチャ</span><span class="sxs-lookup"><span data-stu-id="33468-293">Hand gestures</span></span>](gestures.md)
* [<span data-ttu-id="33468-294">音声入力</span><span class="sxs-lookup"><span data-stu-id="33468-294">Voice input</span></span>](voice-design.md)
* [<span data-ttu-id="33468-295">モーション コントローラー</span><span class="sxs-lookup"><span data-stu-id="33468-295">Motion controllers</span></span>](motion-controllers.md)
* [<span data-ttu-id="33468-296">快適性</span><span class="sxs-lookup"><span data-stu-id="33468-296">Comfort</span></span>](comfort.md)
