---
title: 視線
description: 視線
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: 目の追跡、実際には、入力、目の視線の混在
ms.openlocfilehash: 7298a34a946f86aaf789cfe44ad971169fc8ece3
ms.sourcegitcommit: 60060386305eabfac2758a2c861a43c36286b151
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 05/31/2019
ms.locfileid: "66453702"
---
# <a name="eye-tracking-on-hololens-2"></a><span data-ttu-id="8484d-104">HoloLens 2 の視線</span><span class="sxs-lookup"><span data-stu-id="8484d-104">Eye tracking on HoloLens 2</span></span>
<span data-ttu-id="8484d-105">HoloLens 2 を使うと、開発者に検索内容ユーザーに関する情報を使用しての驚異的な機能に提供するにより、まったく新しいレベルのコンテキストとホログラフィック操作内で人間について理解します。</span><span class="sxs-lookup"><span data-stu-id="8484d-105">HoloLens 2 allows for a whole new level of context and human understanding within the holographic experience by providing developers with the incredible ability of using information about what users are looking at.</span></span> <span data-ttu-id="8484d-106">このページは、さまざまなユース ケースの追跡を目が開発者にできる利用し、目視線入力ベースのユーザー インターフェイスを設計するとき確認する項目の概要を示します。</span><span class="sxs-lookup"><span data-stu-id="8484d-106">This page gives an overview of how developers can benefit from eye tracking for various use cases and what to look out for when designing eye-gaze-based user interfaces.</span></span> 

## <a name="use-cases"></a><span data-ttu-id="8484d-107">使用事例</span><span class="sxs-lookup"><span data-stu-id="8484d-107">Use cases</span></span>
<span data-ttu-id="8484d-108">追跡の目では、リアルタイムでユーザーを検索する場所を追跡するためにアプリケーションを使用できます。</span><span class="sxs-lookup"><span data-stu-id="8484d-108">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="8484d-109">このセクションでは、いくつかの潜在的なユース ケースと複合現実での視線を可能になる斬新な相互作用について説明します。</span><span class="sxs-lookup"><span data-stu-id="8484d-109">This section describes some of the potential use cases and novel interactions that become possible with eye tracking in mixed reality.</span></span>
<span data-ttu-id="8484d-110">始める前に、以下では伝え、 [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)数回興味深く強力な例をいくつかの迅速かつ簡単な目でサポートされているターゲットなどの目の追跡を使用して提供選択と基に、ユーザーを検索する場所にテキストを自動的にスクロールします。</span><span class="sxs-lookup"><span data-stu-id="8484d-110">Before getting started, in the following we will mention the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) several times as it provides several interesting and powerful examples for using eye tracking such as quick and effortless eye-supported target selections and automatically scrolling through text based on where the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="8484d-111">ユーザーの意図</span><span class="sxs-lookup"><span data-stu-id="8484d-111">User intent</span></span>    
<span data-ttu-id="8484d-112">ユーザーを検索する場所に関する情報を提供できる、強力かつ**他の入力のコンテキスト**音声、手、およびコント ローラーなど。</span><span class="sxs-lookup"><span data-stu-id="8484d-112">Information about where a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="8484d-113">これは、さまざまなタスクを使用できます。</span><span class="sxs-lookup"><span data-stu-id="8484d-113">This can be used for various tasks.</span></span>
<span data-ttu-id="8484d-114">たとえば、これの範囲から迅速かつ容易に**を対象とする**ホログラム見て"select"というだけで、シーンの間で (も参照してください[ヘッド注視し、コミット](gaze-and-commit.md)) 言っておきたい"put この..."、または、ホログラムを配置し、"... とする場所を取り除いた方そういうこと"です。</span><span class="sxs-lookup"><span data-stu-id="8484d-114">For example, this may range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying "select" (also see [Head-gaze and commit](gaze-and-commit.md)) or by saying "put this...", then looking over to where you want to place the hologram and say "...there".</span></span> <span data-ttu-id="8484d-115">この例が記載[Mixed Reality Toolkit - 目でサポートされているターゲットを選択](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)と[混合現実 Toolkit - ターゲットの位置をその目でサポートされている](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)します。</span><span class="sxs-lookup"><span data-stu-id="8484d-115">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="8484d-116">ユーザーの意図に関する他の例では、embodied 仮想エージェントと対話型ホログラム エンゲージメントを強化するためにユーザーを見て何についての情報を使用してを含めることができます。</span><span class="sxs-lookup"><span data-stu-id="8484d-116">An additional example for user intent may include using information about what users look at to enhance the engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="8484d-117">たとえば、仮想エージェントが使用可能なオプションを変更でき、現在に基づいてその動作は、コンテンツを表示します。</span><span class="sxs-lookup"><span data-stu-id="8484d-117">For example, virtual agents may adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="8484d-118">暗黙的な操作</span><span class="sxs-lookup"><span data-stu-id="8484d-118">Implicit actions</span></span>
<span data-ttu-id="8484d-119">暗黙的なアクションのカテゴリは、ユーザーの意図に密接に関連します。</span><span class="sxs-lookup"><span data-stu-id="8484d-119">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="8484d-120">考え方としてはホログラムまたはユーザー インターフェイス要素がやや instinctual も感じられない、まったくなく、システムと対話するように、システムと、ユーザーが同期されているな形で対応します。たとえば、非常に成功した 1 つの例は**目視線入力ベースの自動スクロール**します。</span><span class="sxs-lookup"><span data-stu-id="8484d-120">The idea is that holograms or user interface elements react in a somewhat instinctual way that may not even feel like you are interacting with the system at all, but rather that the system and the user are in sync. For example, one immensely successful example is **eye-gaze-based auto scroll**.</span></span> <span data-ttu-id="8484d-121">考え方はだけです。ユーザーは、テキストを読み取って、できるだけでを読み続けてください。</span><span class="sxs-lookup"><span data-stu-id="8484d-121">The idea is as simple: The user reads a text and can just keep on reading.</span></span> <span data-ttu-id="8484d-122">テキストは、読み取りフローでユーザーを保持する段階的に移動します。</span><span class="sxs-lookup"><span data-stu-id="8484d-122">The text gradually moves up to keep users in their reading flow.</span></span> <span data-ttu-id="8484d-123">重要な側面は、対応、ユーザーの読み取り速度へのスクロール速度です。</span><span class="sxs-lookup"><span data-stu-id="8484d-123">A key aspect is that the scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="8484d-124">別の例は**目でサポートされているズームとパン**のユーザーが何かに焦点を合わせてに正確に急降下のように感じることができますが。</span><span class="sxs-lookup"><span data-stu-id="8484d-124">Another example is **eye-supported zoom and pan** for which the user can feel like diving exactly toward what he or she is focusing at.</span></span> <span data-ttu-id="8484d-125">ズームをトリガーして、ズームの速度を制御する音声を使用して制御できますまたはコントロールの感情を提供することの重要なは、入力を渡すし、(これは、これらについて以下で設計のガイドラインについて説明されます)、ユーザーの混乱を回避します。</span><span class="sxs-lookup"><span data-stu-id="8484d-125">Triggering the zoom and controlling the zoom speed can be controlled via voice or hand input which is important about providing the feeling of control and avoid overwhelming the user (we will talk about these design guidelines in more detail below).</span></span> <span data-ttu-id="8484d-126">したらズームインすると、ユーザーことができますし、スムーズにに従って、たとえば、面している通り、目視線の先を使用して単純に自分のコンピューターを探索するの。</span><span class="sxs-lookup"><span data-stu-id="8484d-126">Once zoomed in, the user can then smoothly follow, for example, the course of a street to explore his or her neighborhood just simply by using their eye gaze.</span></span>
<span data-ttu-id="8484d-127">この種の相互作用のデモの例が記載されて、 [Mixed Reality Toolkit - 目でサポートされているナビゲーション](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)サンプル。</span><span class="sxs-lookup"><span data-stu-id="8484d-127">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="8484d-128">追加のケースを使用して、_暗黙的なアクション_含めることができます。</span><span class="sxs-lookup"><span data-stu-id="8484d-128">Additional use cases for _implicit actions_ may include:</span></span>
- <span data-ttu-id="8484d-129">**スマート通知:** これまで取得するが重点を置いて右をポップアップ通知によって嫌がるでしょうか。</span><span class="sxs-lookup"><span data-stu-id="8484d-129">**Smart notifications:** Ever get annoyed by notifications popping up right where you were focusing?</span></span> <span data-ttu-id="8484d-130">ユーザーに注意してくださいの支払いを現在を考慮して、行うことができますより優れた!</span><span class="sxs-lookup"><span data-stu-id="8484d-130">Taking into account where a user is currently paying attention to, you can make it better!</span></span> <span data-ttu-id="8484d-131">表示通知、ユーザーが混乱を避けることを制限し、1 回自動的に消去する検索して現在位置からオフセットが読み取りを終了します。</span><span class="sxs-lookup"><span data-stu-id="8484d-131">Show notifications offset from where the user is currently looking to limit distractions and automatically dismiss them once finished reading.</span></span> 
- <span data-ttu-id="8484d-132">**注意深いホログラム:** 微妙で参照されるときを react ホログラムします。</span><span class="sxs-lookup"><span data-stu-id="8484d-132">**Attentive holograms:** Holograms that subtly react when being looked at.</span></span> <span data-ttu-id="8484d-133">Blooming 緩やかに変化の花を仮想ペットの開始にさかのぼって確認するまたは長時間にわたるまなざし後に、目視線の先を回避しようとしています。 を少し発光の UI 要素から範囲この可能性があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-133">This may range from slightly glowing UI elements, a slowly blooming flower to a virtual pet starting to look back at you or trying to avoid your eye gaze after a prolonged stare.</span></span> <span data-ttu-id="8484d-134">これにより、接続と、アプリで満足度の興味深い意味が提供することがあります。</span><span class="sxs-lookup"><span data-stu-id="8484d-134">This may provide an interesting sense of connectivity and satisfaction in your app.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="8484d-135">注意の追跡</span><span class="sxs-lookup"><span data-stu-id="8484d-135">Attention tracking</span></span>   
<span data-ttu-id="8484d-136">ユーザーを検索する場所に関する情報は、設計の使いやすさを評価して効率的なワーク ストリームで問題を識別するために非常に強力なツールです。</span><span class="sxs-lookup"><span data-stu-id="8484d-136">Information about where users look at is an immensely powerful tool to assess usability of designs and to identify problems in efficient work streams.</span></span> <span data-ttu-id="8484d-137">ここまでで、視覚エフェクトと分析の視線はさまざまなアプリケーションの領域での一般的な方法で、既にいます。</span><span class="sxs-lookup"><span data-stu-id="8484d-137">By now,  eye tracking visualization and analytics are already a common practice in various application areas.</span></span> <span data-ttu-id="8484d-138">HoloLens の 2 3D ホログラムを実際のコンテキスト内に配置し、評価と共にようにこのことを理解する新しいディメンションを提供します。</span><span class="sxs-lookup"><span data-stu-id="8484d-138">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed alongside.</span></span> <span data-ttu-id="8484d-139">[Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)なログ記録と監視追跡データを読み込み、それらを視覚化する方法の基本的な例を提供します。</span><span class="sxs-lookup"><span data-stu-id="8484d-139">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and for how to visualize them.</span></span>

<span data-ttu-id="8484d-140">この領域では、他のアプリケーションが含まれます。</span><span class="sxs-lookup"><span data-stu-id="8484d-140">Other applications in this area may include:</span></span> 
-   <span data-ttu-id="8484d-141">**リモート監視の視線の先の視覚化:** リモート コラボレーター検索内容にするなどの視覚化を手順が正しく認識され、後にかどうかを確認します。</span><span class="sxs-lookup"><span data-stu-id="8484d-141">**Remote eye gaze visualization:** Visualize what remote collaborators are looking at to, for example, ensure whether instructions are correctly understood and followed.</span></span>
-   <span data-ttu-id="8484d-142">**ユーザーの研究:** 注意追跡は、初心者のエキスパート ユーザーとは、コンテンツや、手の目-調整 (またはメカニズムの動作中の医療データを分析) などの複雑なタスクに視覚的に分析する方法の詳細を使用できます。</span><span class="sxs-lookup"><span data-stu-id="8484d-142">**User research studies:** Attention tracking can be used to explore the way novice vs. experts users visually analyze content or their hand-eye-coordination for complex tasks (e.g., for analysis of medical data or while operating machinery).</span></span>
-   <span data-ttu-id="8484d-143">**トレーニングのシミュレーションとパフォーマンスの監視:** 実践しより効率的に実行フローのボトルネックを識別することによってタスクの実行を最適化します。</span><span class="sxs-lookup"><span data-stu-id="8484d-143">**Training simulations and Performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span>
-   <span data-ttu-id="8484d-144">**評価版ソフトウェア、提供情報および市場調査を設計します。** 視線は、web サイトと製品の設計を評価する市場調査の一般的なツールです。</span><span class="sxs-lookup"><span data-stu-id="8484d-144">**Design evaluations, advertisement and marketing research:** Eye tracking is a common tool for market research to evaluate website and product designs.</span></span>

### <a name="additional-use-cases"></a><span data-ttu-id="8484d-145">追加のユース ケース</span><span class="sxs-lookup"><span data-stu-id="8484d-145">Additional use cases</span></span>
- <span data-ttu-id="8484d-146">**ゲームの場合:** 優れたオプションがあるたいと思ったでしょうか。</span><span class="sxs-lookup"><span data-stu-id="8484d-146">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="8484d-147">機会になります。</span><span class="sxs-lookup"><span data-stu-id="8484d-147">Here's your chance!</span></span> <span data-ttu-id="8484d-148">それらでホログラムを levitate します。</span><span class="sxs-lookup"><span data-stu-id="8484d-148">Levitate holograms by staring at them.</span></span> <span data-ttu-id="8484d-149">レーザー ビーム目から撮影してください。</span><span class="sxs-lookup"><span data-stu-id="8484d-149">Shoot laser beams from your eyes.</span></span> <span data-ttu-id="8484d-150">石に敵を有効にするか、固定します。</span><span class="sxs-lookup"><span data-stu-id="8484d-150">Turn enemies into stone or freeze them!</span></span> <span data-ttu-id="8484d-151">建物を探索するのにには、x 線画像構想を使用します。</span><span class="sxs-lookup"><span data-stu-id="8484d-151">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="8484d-152">想像力を上限であります。</span><span class="sxs-lookup"><span data-stu-id="8484d-152">Your imagination is the limit!</span></span>  

- <span data-ttu-id="8484d-153">**表現力豊かなアバターは:** 目を示す必要な情報、ユーザーが現在のアバターの目をアニメーション化する日付を追跡するライブの目を使用して、表現力豊かな 3D アバターで追跡します。</span><span class="sxs-lookup"><span data-stu-id="8484d-153">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking date to animate the avatar's eyes to indicate what the user is currently looking at.</span></span> <span data-ttu-id="8484d-154">ウインクと点滅を追加することで、複数の表現力も追加されます。</span><span class="sxs-lookup"><span data-stu-id="8484d-154">It also adds more expressiveness by adding winks and blinks.</span></span> 

- <span data-ttu-id="8484d-155">**テキストを入力します。** 視線は音声認識または手が使用する便利な場合に特に低エフォート テキスト エントリの興味深い方法としては使用できます。</span><span class="sxs-lookup"><span data-stu-id="8484d-155">**Text entry:** Eye tracking can be used as an interesting alternative for low-effort text entry especially when speech or hands are inconvenient to use.</span></span> 


## <a name="eye-tracking-api"></a><span data-ttu-id="8484d-156">目追跡 API</span><span class="sxs-lookup"><span data-stu-id="8484d-156">Eye tracking API</span></span>
<span data-ttu-id="8484d-157">目視線の相互作用に関する特定のデザインのガイドラインについて詳細に入る前に、HoloLens 2 の目の追跡ツールを提供する機能を簡単にポイントします。</span><span class="sxs-lookup"><span data-stu-id="8484d-157">Before going into detail about the specific design guidelines for eye-gaze interaction, we want to briefly point to the capabilities that the HoloLens 2 Eye Tracker is providing.</span></span> <span data-ttu-id="8484d-158">[目追跡 API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose)を通じてアクセス:`Windows.Perception.People.EyesPose`します。</span><span class="sxs-lookup"><span data-stu-id="8484d-158">The [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) is accessible through: `Windows.Perception.People.EyesPose`.</span></span> <span data-ttu-id="8484d-159">1 つ目の視線の先レイ (視線の先の原点と方向) を開発者に提供します。</span><span class="sxs-lookup"><span data-stu-id="8484d-159">It provides a single eye gaze ray (gaze origin and direction) to developers.</span></span>
<span data-ttu-id="8484d-160">目の追跡ツールに関するデータを提供する_30 FPS_します。</span><span class="sxs-lookup"><span data-stu-id="8484d-160">The eye tracker provides data at about _30 FPS_.</span></span>
<span data-ttu-id="8484d-161">予測目視線の先が ca 内に存在します。</span><span class="sxs-lookup"><span data-stu-id="8484d-161">The predicted eye gaze lies within ca.</span></span> <span data-ttu-id="8484d-162">1.0-1.5 度、実際の周囲のビジュアルの角度では、ターゲットで検索します。</span><span class="sxs-lookup"><span data-stu-id="8484d-162">1.0 - 1.5 degrees in visual angle around the actual looked at target.</span></span> <span data-ttu-id="8484d-163">少し不正確性は、想定どおり、この下限の境界値をいくつかの余白の幅を立てる必要があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-163">As slight imprecisions are expected, you should plan for some margin around this lower bound value.</span></span> <span data-ttu-id="8484d-164">これについてより下です。</span><span class="sxs-lookup"><span data-stu-id="8484d-164">We will discuss this more below.</span></span> <span data-ttu-id="8484d-165">視線を正確に機能、ユーザーの調整の視線を経由する各ユーザーが必要です。</span><span class="sxs-lookup"><span data-stu-id="8484d-165">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="8484d-166">![2 つのメーター距離にある最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="8484d-166">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="8484d-167">*2 つのメーター距離にある最適なターゲット サイズ*</span><span class="sxs-lookup"><span data-stu-id="8484d-167">*Optimal target size at 2 meter distance*</span></span>


## <a name="eye-gaze-design-guidelines"></a><span data-ttu-id="8484d-168">目視線の先のデザイン ガイドライン</span><span class="sxs-lookup"><span data-stu-id="8484d-168">Eye gaze design guidelines</span></span>
<span data-ttu-id="8484d-169">高速移動の目を対象とするを利用する相互作用の構築は困難なことができます。</span><span class="sxs-lookup"><span data-stu-id="8484d-169">Building an interaction that takes advantage of fast moving eye targeting can be challenging.</span></span> <span data-ttu-id="8484d-170">このセクションで、主な利点と、アプリを設計するときに考慮に入れての課題を要約します。</span><span class="sxs-lookup"><span data-stu-id="8484d-170">In this section, we summarize the key advantages and challenges to take into account when designing your app.</span></span> 

### <a name="benefits-of-eye-gaze-input"></a><span data-ttu-id="8484d-171">視線入力の目の利点</span><span class="sxs-lookup"><span data-stu-id="8484d-171">Benefits of eye gaze input</span></span>
- <span data-ttu-id="8484d-172">**高速ポイントしています。**</span><span class="sxs-lookup"><span data-stu-id="8484d-172">**High speed pointing.**</span></span> <span data-ttu-id="8484d-173">目の威力は、当社の本文で最速 reacting 威力です。</span><span class="sxs-lookup"><span data-stu-id="8484d-173">The eye muscle is the fastest reacting muscle in our body.</span></span> 

- <span data-ttu-id="8484d-174">**少ない。**</span><span class="sxs-lookup"><span data-stu-id="8484d-174">**Low effort.**</span></span> <span data-ttu-id="8484d-175">ほとんどの物理的な動きは、必要があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-175">Barely any physical movements are necessary.</span></span> 

- <span data-ttu-id="8484d-176">**Implicitness します。**</span><span class="sxs-lookup"><span data-stu-id="8484d-176">**Implicitness.**</span></span> <span data-ttu-id="8484d-177">多くの場合のようにユーザーが「に注意してください資料」として、ユーザーの目の動作についての情報には、ターゲットと連絡を取るユーザー プランをシステムことができます。</span><span class="sxs-lookup"><span data-stu-id="8484d-177">Often described by users as "mind reading", information about a user's eye movements lets the system know which target the user plans to engage with.</span></span> 

- <span data-ttu-id="8484d-178">**代替の入力チャネル。**</span><span class="sxs-lookup"><span data-stu-id="8484d-178">**Alternative input channel.**</span></span> <span data-ttu-id="8484d-179">目の視線入力は、手の形と音声入力からのビルド長年の経験、手の目の調整に基づくユーザーの強力なサポートの入力を指定できます。</span><span class="sxs-lookup"><span data-stu-id="8484d-179">Eye gaze can provide a powerful supporting input for hand and voice input building on years of experience from users based on their hand-eye coordination.</span></span>

- <span data-ttu-id="8484d-180">**Visual 注意します。**</span><span class="sxs-lookup"><span data-stu-id="8484d-180">**Visual attention.**</span></span> <span data-ttu-id="8484d-181">もう 1 つの重要なメリットは、どのようなユーザーが注意を推測することです。</span><span class="sxs-lookup"><span data-stu-id="8484d-181">Another important benefit is the possibility to infer what a user's is paying attention to.</span></span> <span data-ttu-id="8484d-182">これから詳細はよりスマートなユーザー インターフェイスに役立つようにさまざまなデザインを効果的に評価するまで、さまざまなアプリケーション領域で参照し、リモート通信にソーシャル的な手掛かりが強化されています。</span><span class="sxs-lookup"><span data-stu-id="8484d-182">This can help in various application areas ranging from more effectively evaluating different designs to aiding in smarter User Interfaces and enhanced social cues for remote communication.</span></span>

<span data-ttu-id="8484d-183">簡単に言うと、目視線の先を使用して入力は、高速で簡単なコンテキスト信号の可能性がある提供していますこれは、他の入力と組み合わせて非常に強力ななど*音声*と*手動*への入力。ユーザーの意図を確認します。</span><span class="sxs-lookup"><span data-stu-id="8484d-183">In a nutshell, using eye gaze as an input potentially offers a fast and effortless contextual signal - This is particularly powerful in combination with other inputs such as *voice* and *manual* input to confirm the user's intent.</span></span>


### <a name="challenges-of-eye-gaze-as-an-input"></a><span data-ttu-id="8484d-184">目の課題の視線入力として</span><span class="sxs-lookup"><span data-stu-id="8484d-184">Challenges of eye gaze as an input</span></span>
<span data-ttu-id="8484d-185">電源の多くは、多くの責任には。目視線の先を使用して、superhero ように感じて魔法のようなユーザー エクスペリエンスを作成することが、内容は良くありませんでアカウントにこれを適切に把握しておくがもできます。</span><span class="sxs-lookup"><span data-stu-id="8484d-185">With lots of power, comes lots of responsibility: While eye gaze can be used to create magical user experiences feeling like a superhero, it is also important to know what it is not good at to account for this appropriately.</span></span> <span data-ttu-id="8484d-186">以下では、説明*課題*アカウントと目視線入力を使用する場合、その対処方法を考慮します。</span><span class="sxs-lookup"><span data-stu-id="8484d-186">In the following, we discuss some *challenges* to take into account and how to address them when working with eye gaze input:</span></span> 

- <span data-ttu-id="8484d-187">**目、視線の先が「常時オン」** 目 lids を開く時点目は、環境内の固着の手順を開始します。</span><span class="sxs-lookup"><span data-stu-id="8484d-187">**Your eye gaze is "always on"** The moment you open your eye lids, your eyes start fixating things in your environment.</span></span> <span data-ttu-id="8484d-188">すべての対応を紹介しましたので、誤って可能性のあるアクションを発行および検索の時間が長すぎる悲惨な経験では結果は!</span><span class="sxs-lookup"><span data-stu-id="8484d-188">Reacting to every look you make and potentially accidentally issuing actions because you looked at something for too long would result in a terrible experience!</span></span>
<span data-ttu-id="8484d-189">目視線の先を組み合わせることをお勧めします。 このため、*音声指示コマンド*、*ジェスチャを渡す*、*ボタンをクリック*またはターゲットの選択範囲をトリガーするドウェルを拡張します。</span><span class="sxs-lookup"><span data-stu-id="8484d-189">This is why we recommend combining eye gaze with a *voice command*, *hand gesture*, *button click* or extended dwell to trigger the selection of a target.</span></span>
<span data-ttu-id="8484d-190">このソリューションを自由に確認できます膨大な感じのものを誤ってトリガーすることがなくモードもできます。</span><span class="sxs-lookup"><span data-stu-id="8484d-190">This solution also allows for a mode in which the user can freely look around without the overwhelming feeling of involuntarily triggering something.</span></span> <span data-ttu-id="8484d-191">単にターゲットを見ると、視覚や聴覚的フィードバックを設計するときに、この問題もに考慮する必要があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-191">This issue should also be taken into account when designing visual and auditory feedback when merely looking at a target.</span></span>
<span data-ttu-id="8484d-192">即時のポップアウト効果を使用したユーザーの重荷となって、サウンドのマウス ポインターを移動したりしないでください。</span><span class="sxs-lookup"><span data-stu-id="8484d-192">Do not overwhelm the user with immediate pop-out effects or hover sounds.</span></span> <span data-ttu-id="8484d-193">注意が重要です。</span><span class="sxs-lookup"><span data-stu-id="8484d-193">Subtlety is key!</span></span> <span data-ttu-id="8484d-194">設計に関する推奨事項について説明するときのベスト プラクティス下さらにこれを説明します。</span><span class="sxs-lookup"><span data-stu-id="8484d-194">We will discuss some best practices for this further below when talking about design recommendations.</span></span>

- <span data-ttu-id="8484d-195">**コントロールと観測**壁に写真を正確に配置するとします。</span><span class="sxs-lookup"><span data-stu-id="8484d-195">**Observation vs. control** Imagine you want to precisely align a photograph at your wall.</span></span> <span data-ttu-id="8484d-196">境界線とも配置を確認するには、その環境を表示します。</span><span class="sxs-lookup"><span data-stu-id="8484d-196">You look at its borders and its surroundings to see if it aligns well.</span></span> <span data-ttu-id="8484d-197">ここではその方法を入力として、目視線の先を使用して、画像を移動すると同時にする場合を想像してください。</span><span class="sxs-lookup"><span data-stu-id="8484d-197">Now imagine how you would do that when at the same time you want to use your eye gaze as an input to move the picture.</span></span> <span data-ttu-id="8484d-198">難しいでしょう。</span><span class="sxs-lookup"><span data-stu-id="8484d-198">Difficult, isn't it?</span></span> <span data-ttu-id="8484d-199">これにより、両方の入力および制御するために必要な場合に、目の視線入力の double の役割を説明します。</span><span class="sxs-lookup"><span data-stu-id="8484d-199">This describes the double role of eye gaze when it is required both for input and control.</span></span> 

- <span data-ttu-id="8484d-200">**クリックする前に、このままにします。** クイック ターゲットの選択内容の調査によれば、手動のクリックを終える前に、ユーザーの目の視線入力可能性がありますに移動 (airtap など)。</span><span class="sxs-lookup"><span data-stu-id="8484d-200">**Leave before click:** For quick target selections, research has shown that a user's eye gaze may move on before concluding a manual click (e.g., an airtap).</span></span> <span data-ttu-id="8484d-201">そのため、低速のコントロールの入力 (音声、手、コント ローラーなど) と高速の目の視線入力信号を同期する特別な注意を支払う必要があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-201">Hence, special attention must be paid to synchronizing the fast eye gaze signal with slower control input (e.g., voice, hands, controller).</span></span>

- <span data-ttu-id="8484d-202">**小規模のターゲット:** 少し読み快適に小さすぎるだけテキストを読み取るしようとするは、感情をわかりますか。</span><span class="sxs-lookup"><span data-stu-id="8484d-202">**Small targets:** Do you know the feeling when you try to read text that is just a bit too small to comfortably read?</span></span> <span data-ttu-id="8484d-203">優れたフォーカスに目を再調整しようとするため、疲れているとして古いを感じる可能性がありますを目でのこの負担をかけず感じでしょうか。</span><span class="sxs-lookup"><span data-stu-id="8484d-203">This straining feeling on your eyes that cause you to feel tired and worn out because you try to readjust your eyes to focus better?</span></span>
<span data-ttu-id="8484d-204">これは、気を強制的に目を対象とするを使用して、アプリで小さすぎるターゲットを選択するときに、ユーザーに呼び出す可能性があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-204">This is a feeling you may invoke in your users when forcing them to select too small targets in your app using eye targeting.</span></span>
<span data-ttu-id="8484d-205">設計には、ユーザーに対しては、快適になり、快適なエクスペリエンスを作成するをお勧めするターゲットでは、可能であれば大きなビジュアルの角度で少なくとも 2 度必要があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-205">For your design, to create a pleasant and comfortable experience for your users, we recommend that targets should be at least 2° in visual angle, preferably larger.</span></span>

- <span data-ttu-id="8484d-206">**目の視線の動きを不規則**人間の目は固定から固定化迅速な動きを実行します。</span><span class="sxs-lookup"><span data-stu-id="8484d-206">**Ragged eye gaze movements** Our eyes perform rapid movements from fixation to fixation.</span></span> <span data-ttu-id="8484d-207">記録された目の移動のスキャンのパスを確認する場合、幅合わせしないので、見た目が確認できます。</span><span class="sxs-lookup"><span data-stu-id="8484d-207">If you look at scan paths of recorded eye movements, you can see that they look ragged.</span></span> <span data-ttu-id="8484d-208">迅速かつ自然なジャンプで比較する、目が移動*ヘッド注視*または*モーションを渡す*します。</span><span class="sxs-lookup"><span data-stu-id="8484d-208">Your eyes move quickly and in spontaneous jumps in comparison to *head gaze* or *hand motions*.</span></span>  

- <span data-ttu-id="8484d-209">**信頼性を追跡するには。** 新しい条件に目を調整するように光を変更するのに視線の精度が少しが低下する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-209">**Tracking reliability:** Eye tracking accuracy may degrade a little in changing light as your eye adjust to the new conditions.</span></span>
<span data-ttu-id="8484d-210">アプリの設計は必ずしもは影響が、精度とする必要があります内に 2 度の上記の制限。</span><span class="sxs-lookup"><span data-stu-id="8484d-210">While this should not necessarily affect your app design, as the accuracy should be within the above mentioned limitation of 2°.</span></span> <span data-ttu-id="8484d-211">もう 1 つの調整を実行するユーザーが持っている可能性があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-211">It may mean that the user has to run another calibration.</span></span> 


### <a name="design-recommendations"></a><span data-ttu-id="8484d-212">設計に関する推奨事項</span><span class="sxs-lookup"><span data-stu-id="8484d-212">Design recommendations</span></span>
<span data-ttu-id="8484d-213">次に、説明の利点に基づく特定のデザインの推奨事項を一覧表示し、目の課題の視線入力。</span><span class="sxs-lookup"><span data-stu-id="8484d-213">In the following, we list specific design recommendations based on the described advantages and challenges for eye gaze input:</span></span>

1. <span data-ttu-id="8484d-214">**目の視線入力! = ヘッド視線の先。**</span><span class="sxs-lookup"><span data-stu-id="8484d-214">**Eye gaze != Head gaze:**</span></span>
    - <span data-ttu-id="8484d-215">**かどうかまだ高速不規則目の動きに合わせて、入力タスクを検討してください。** 目を高速と不規則な動きが優れたをすばやく、ビューのフィールドの間でのターゲットを選択する (たとえば、描画または注釈を encircling) 滑らかな入力の軌道を必要とするタスクの小さい適用することは。</span><span class="sxs-lookup"><span data-stu-id="8484d-215">**Consider whether fast yet ragged eye movements fit your input task:** While our fast and ragged eye movements are great to quickly select targets across our Field of View, it is less applicable for tasks that require smooth input trajectories (e.g., for drawing or encircling annotations).</span></span> <span data-ttu-id="8484d-216">この場合、手動または先頭を指すことをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="8484d-216">In this case, hand or head pointing should be preferred.</span></span>
  
    - <span data-ttu-id="8484d-217">**何かをユーザーの目注視 (スライダーやカーソルなど) に直接アタッチしないようにします。**</span><span class="sxs-lookup"><span data-stu-id="8484d-217">**Avoid attaching something directly to the user’s eye gaze (e.g., a slider or cursor).**</span></span>
<span data-ttu-id="8484d-218">カーソルが発生した場合、射影された目視線入力信号にわずかなオフセットにより「勝てないカーソル」効果であります。</span><span class="sxs-lookup"><span data-stu-id="8484d-218">In case of a cursor, this may result in the “fleeing cursor” effect due to slight offsets in the projected eye gaze signal.</span></span> <span data-ttu-id="8484d-219">スライダーが発生した場合もオブジェクトが、正しい位置にあるかどうかを確認する必要があるときに、目のスライダーを制御するための二重ロールと競合します。</span><span class="sxs-lookup"><span data-stu-id="8484d-219">In case of a slider, it conflicts with the double role of controlling the slider with your eyes while also wanting to check whether the object is at the correct location.</span></span> <span data-ttu-id="8484d-220">簡単に言うと、ユーザーことがあります迅速にないと思われる圧倒と気が散って、特にかどうか、シグナルそのユーザーの正確です。</span><span class="sxs-lookup"><span data-stu-id="8484d-220">In a nutshell, users may quickly feel overwhelmed and distracted, especially if the signal is imprecise for that user.</span></span> 
  
2. <span data-ttu-id="8484d-221">**目視線の先を他の入力に組み合わせます。** 目の追跡の手のジェスチャ、音声コマンドやボタンの押下などの他の入力との統合には、いくつかの利点があります。</span><span class="sxs-lookup"><span data-stu-id="8484d-221">**Combine eye gaze with other inputs:** The integration of Eye Tracking with other inputs, such as hand gestures, voice commands or button presses, serves several advantages:</span></span>
    - <span data-ttu-id="8484d-222">**監視を無料で許可します。** いずれかをトリガーすることがなく、少し調べてみるようにする重要な人間の目のメインのロールでは、この環境を監視して、(視覚、音声、...) からのフィードバックやアクション。</span><span class="sxs-lookup"><span data-stu-id="8484d-222">**Allow for free observation:** Given that the main role of our eyes is to observe our environment, it is important to allow users to look around without triggering any (visual, auditory, ...) feedback or actions.</span></span> 
    <span data-ttu-id="8484d-223">別の入力コントロールと ET を組み合わせると ET 監視し、入力コントロール モード間でスムーズに移行できます。</span><span class="sxs-lookup"><span data-stu-id="8484d-223">Combining ET with another input control allows for smoothly transitioning between ET observation and input control modes.</span></span>
  
    - <span data-ttu-id="8484d-224">**強力なコンテキスト プロバイダー:** 音声コマンドを uttering 中に、ユーザーが見るまたはビューのフィールドの間で、入力を簡単に道筋では、手のジェスチャを実行する場所に関する情報を使用します。</span><span class="sxs-lookup"><span data-stu-id="8484d-224">**Powerful context provider:** Using information about where the user is looking at while uttering a voice command or performing a hand gesture allows for effortlessly channeling the input across the field-of-view.</span></span> <span data-ttu-id="8484d-225">たとえば、次のものがあります。「追加することがあります」すばやくスムーズ選択し、シーン全体を見ただけで、ターゲットとなる目的でホログラムを配置します。</span><span class="sxs-lookup"><span data-stu-id="8484d-225">Examples include: “Put that there” to quickly and fluently select and position a hologram across the scene by simply looking at a target and destination.</span></span> 

    - <span data-ttu-id="8484d-226">**マルチ モーダルな入力値 ("leave をクリックする前に"の問題) を同期する必要があります。** 複雑な追加の入力 (長い音声コマンドや手のジェスチャなど) との迅速な目の動きを組み合わせると、入力の追加のコマンドを終了する前に、目視線の先で移動のリスクがあります。</span><span class="sxs-lookup"><span data-stu-id="8484d-226">**Need for synchronizing multimodal inputs (“leave before click” issue):** Combining rapid eye movements with more complex additional inputs (e.g., long voice commands or hand gestures) bears the risk of moving on with your eye gaze before finishing the additional input command.</span></span> <span data-ttu-id="8484d-227">そのため、独自の入力コントロール (カスタムの手のジェスチャなど) を作成する場合にどのようなユーザーいたことに執着して過去に関連付けるこの入力または概数の期間のログに記録することを確認すること。</span><span class="sxs-lookup"><span data-stu-id="8484d-227">Hence, if you create your own input controls (e.g., custom hand gestures), make sure to log the onset of this input or approximate duration to correlate it with what a user had fixated on in the past.</span></span>
    
3. <span data-ttu-id="8484d-228">**視線入力の微妙なフィードバック:** ターゲット (システムを意図したとおりに動作していることを示します) を考えましたが、微妙に保持する必要があります、フィードバックを提供すると便利です。</span><span class="sxs-lookup"><span data-stu-id="8484d-228">**Subtle feedback for eye tracking input:** It is useful to provide feedback if a target is looked at (to indicate that the system is working as intended) but should be kept subtle.</span></span> <span data-ttu-id="8484d-229">緩やかに変化ブレンド/ビジュアルの強調表示出力を含めることも低速のモーションなどの他のターゲットの微妙な動作を実行 (例: 少し増やすターゲット) を示す、システムは、ユーザーは、ターゲットを見ることを正しく検出されるなしで不必要にユーザーの現在のワークフローを中断します。</span><span class="sxs-lookup"><span data-stu-id="8484d-229">This may include slowly blending in/out visual highlights or perform other subtle target behaviors, such as slow motions (e.g., slightly increasing the target) to indicate that the system correctly detected that the user is looking at a target, however, without unnecessarily interrupting the user’s current workflow.</span></span> 

4. <span data-ttu-id="8484d-230">**不自然な目の動きを適用する入力として使用しないでください。** アクションをトリガーするアプリで特定の目の動き (視線入力ジェスチャ) を実行するユーザーは求めません。</span><span class="sxs-lookup"><span data-stu-id="8484d-230">**Avoid enforcing unnatural eye movements as input:** Do not force users to perform specific eye movements (gaze gestures) to trigger actions in your app.</span></span>

5. <span data-ttu-id="8484d-231">**不正確性のアカウント:** 2 種類のユーザーに顕著なである不正確性を識別します。オフセットやジッターします。</span><span class="sxs-lookup"><span data-stu-id="8484d-231">**Account for imprecisions:** We distinguish two types of imprecisions which are noticeable to users: Offset and Jitter.</span></span> <span data-ttu-id="8484d-232">アドレス オフセットに最も簡単な方法は、対話する十分な大きさのターゲットを提供することです。 (> 2 度参照として – visual の角度に: arm (1) を拡張する場合は、サムネイル、ビジュアルの角度で約 2 度)。</span><span class="sxs-lookup"><span data-stu-id="8484d-232">The easiest way to address offsets is to provide sufficiently large targets to interact with (> 2° in visual angle – as reference: your thumbnail is about 2° in visual angle when you stretch out your arm (1)).</span></span> <span data-ttu-id="8484d-233">これは、次のガイダンスにつながります。</span><span class="sxs-lookup"><span data-stu-id="8484d-233">This leads to the following guidance:</span></span>
    - <span data-ttu-id="8484d-234">小さなターゲットを選択するユーザーは求めません。調査は、ターゲットは、十分な大きさ (および、システムが適切に設計されて) 場合として作業を簡単かつ魔法のような相互作用についてユーザーに説明することが示されています。</span><span class="sxs-lookup"><span data-stu-id="8484d-234">Do not force users to select tiny targets: Research has shown that if targets are sufficiently large (and the system is designed well), users describe the interaction as effortless and magical.</span></span> <span data-ttu-id="8484d-235">ターゲットが小さすぎるになると、ユーザーは辛いものとして、苛立つエクスペリエンスについて説明します。</span><span class="sxs-lookup"><span data-stu-id="8484d-235">If targets become too small, users describe the experience as fatiguing and frustrating.</span></span>
   

## <a name="see-also"></a><span data-ttu-id="8484d-236">関連項目</span><span class="sxs-lookup"><span data-stu-id="8484d-236">See also</span></span>
* [<span data-ttu-id="8484d-237">頭の視線入力とコミット</span><span class="sxs-lookup"><span data-stu-id="8484d-237">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="8484d-238">DirectX でのヘッド視線入力とアイ視線入力</span><span class="sxs-lookup"><span data-stu-id="8484d-238">Head and eye gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="8484d-239">Unity (Mixed Reality Toolkit) で、目の視線入力</span><span class="sxs-lookup"><span data-stu-id="8484d-239">Eye gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="8484d-240">手のジェスチャ</span><span class="sxs-lookup"><span data-stu-id="8484d-240">Hand gestures</span></span>](gestures.md)
* [<span data-ttu-id="8484d-241">音声入力</span><span class="sxs-lookup"><span data-stu-id="8484d-241">Voice input</span></span>](voice-design.md)
* [<span data-ttu-id="8484d-242">モーション コントローラー</span><span class="sxs-lookup"><span data-stu-id="8484d-242">Motion controllers</span></span>](motion-controllers.md)
* [<span data-ttu-id="8484d-243">快適性</span><span class="sxs-lookup"><span data-stu-id="8484d-243">Comfort</span></span>](comfort.md)
