---
title: 頭の視線入力とコミット
description: 頭の視線入力とコミットの入力モデルの概要
author: caseymeekhof
ms.author: cmeekhof
ms.date: 03/31/2019
ms.topic: article
keywords: Mixed Reality, 視線入力, 視線入力ターゲット設定, 対話, 設計
ms.openlocfilehash: aeca5ceacf5ae350aa06cb58cc68162f885f6d78
ms.sourcegitcommit: b0b1b8e1182cce93929d409706cdaa99ff24fdee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/23/2019
ms.locfileid: "68387682"
---
# <a name="head-gaze-and-commit"></a><span data-ttu-id="e72e7-104">頭の視線入力とコミット</span><span class="sxs-lookup"><span data-stu-id="e72e7-104">Head-gaze and commit</span></span>
<span data-ttu-id="e72e7-105">ヘッドを見つめてコミットすると、入力モデルになります。このモデルでは、前方 (ヘッド方向) の方向にあるオブジェクトをターゲットにし、ハンドジェスチャのエアタップや音声コマンドの選択などの2番目の入力で操作します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-105">Head-gaze and commit is an input model that involves targeting an object with the direction of your head pointing forward (head-direction), and then acting on it with a secondary input, such as the hand gesture air tap or the voice command Select.</span></span> <span data-ttu-id="e72e7-106">これは間接的な操作を伴う外部入力モデルと見なされます。つまり、腕を超えるコンテンツとの対話に最適です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-106">It is considered a far input model with indirect manipulation, meaning it is best used for interacting with content that is beyond arms reach.</span></span>

## <a name="device-support"></a><span data-ttu-id="e72e7-107">デバイスのサポート</span><span class="sxs-lookup"><span data-stu-id="e72e7-107">Device support</span></span>

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><span data-ttu-id="e72e7-108"><strong>入力モデル</strong></span><span class="sxs-lookup"><span data-stu-id="e72e7-108"><strong>Input model</strong></span></span></td>
        <td><span data-ttu-id="e72e7-109"><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></span><span class="sxs-lookup"><span data-stu-id="e72e7-109"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
        <td><span data-ttu-id="e72e7-110"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="e72e7-110"><strong>HoloLens 2</strong></span></span></td>
        <td><span data-ttu-id="e72e7-111"><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></span><span class="sxs-lookup"><span data-stu-id="e72e7-111"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
    </tr>
     <tr>
        <td><span data-ttu-id="e72e7-112">頭の視線入力とコミット</span><span class="sxs-lookup"><span data-stu-id="e72e7-112">Head-gaze and commit</span></span></td>
        <td><span data-ttu-id="e72e7-113">✔️ 推奨</span><span class="sxs-lookup"><span data-stu-id="e72e7-113">✔️ Recommended</span></span></td>
        <td><span data-ttu-id="e72e7-114">✔️ 推奨 (3 番目の選択肢 -<a href="interaction-fundamentals.md">他のオプションを参照</a>)</span><span class="sxs-lookup"><span data-stu-id="e72e7-114">✔️ Recommended (third choice - <a href="interaction-fundamentals.md">See the other options</a>)</span></span></td>
        <td><span data-ttu-id="e72e7-115">➕ 代替オプション</span><span class="sxs-lookup"><span data-stu-id="e72e7-115">➕ Alternate option</span></span></td>
    </tr>
</table>

## <a name="head-gaze"></a><span data-ttu-id="e72e7-116">頭の視線入力</span><span class="sxs-lookup"><span data-stu-id="e72e7-116">Head-gaze</span></span>
<span data-ttu-id="e72e7-117">Mixed Reality ヘッドセットは、ユーザーの頭の位置と向きを利用して、その頭の方向ベクトルを決定します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-117">Mixed reality headsets use the position and orientation of the user's head to determine their head direction vector.</span></span> <span data-ttu-id="e72e7-118">これは、ユーザーの目と目の間から直接、まっすぐ前を指し示すレーザーと考えることができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-118">You can think of this as a laser that points straight ahead from directly between the user's eyes.</span></span> <span data-ttu-id="e72e7-119">これはユーザーが目を向けている場所の非常に粗い近似値です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-119">This is a fairly coarse approximation of where the user is looking.</span></span> <span data-ttu-id="e72e7-120">アプリケーションでは、この射線と仮想または実世界のオブジェクトを交差させ、その位置にカーソルを描画して、現在対象としているものをユーザーに知らせることができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-120">Your application can intersect this ray with virtual or real-world objects, and draw a cursor at that location to let the user know what they are currently targeting.</span></span>

<span data-ttu-id="e72e7-121">「HoloLens 2」のように、一部の mixed reality ヘッドセットには、目を見つめたベクトルを作り出す視線追跡システムが含まれています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-121">In addition to head gaze, some mixed reality headsets, like HoloLens 2, include eye tracking systems that produce an eye-gaze vector.</span></span> <span data-ttu-id="e72e7-122">これにより、ユーザーが目を向けている場所のきめ細かい測定値が得られます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-122">This provides a fine-grained measurement of where the user is looking.</span></span> <span data-ttu-id="e72e7-123">視線を使用して、宝石とコミットの相互作用を構築することができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-123">It is possible to build gaze and commit interactions using eye gaze.</span></span> <span data-ttu-id="e72e7-124">しかし、これには設計上の制約がまったく異なるものがあります。これについては、この[記事](eye-tracking.md)で個別に説明します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-124">But this comes with a very different set of design constraints, which will be covered separately in the [eye-gaze article](eye-tracking.md).</span></span>

## <a name="commit"></a><span data-ttu-id="e72e7-125">確定</span><span class="sxs-lookup"><span data-stu-id="e72e7-125">Commit</span></span>
<span data-ttu-id="e72e7-126">オブジェクトまたは UI 要素をターゲットにした後、ユーザーは2次入力を使用して操作またはクリックできます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-126">After targeting an object or UI element, the user can interact or click on it using a secondary input.</span></span> <span data-ttu-id="e72e7-127">これは、モデルのコミット ステップと呼ばれています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-127">This is known as the commit step of the model.</span></span> <span data-ttu-id="e72e7-128">以下のコミット方法がサポートされています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-128">The following commit methods are supported:</span></span>

- <span data-ttu-id="e72e7-129">エアタップジェスチャ</span><span class="sxs-lookup"><span data-stu-id="e72e7-129">Air tap gesture</span></span>
- <span data-ttu-id="e72e7-130">音声コマンド、選択、または対象となる音声コマンドの1つを読み上げます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-130">Speak the voice command, Select, or one of the targeted voice commands</span></span>
- <span data-ttu-id="e72e7-131">[HoloLens Clicker](hardware-accessories.md#hololens-clicker)で1つのボタンを押す</span><span class="sxs-lookup"><span data-stu-id="e72e7-131">Press a single button on a [HoloLens Clicker](hardware-accessories.md#hololens-clicker)</span></span>
- <span data-ttu-id="e72e7-132">Xbox ゲームパッドで [A] ボタンを押す</span><span class="sxs-lookup"><span data-stu-id="e72e7-132">Press the 'A' button on an Xbox gamepad</span></span>
- <span data-ttu-id="e72e7-133">Xbox adaptive コントローラーで [A] ボタンを押す</span><span class="sxs-lookup"><span data-stu-id="e72e7-133">Press the 'A' button on an Xbox adaptive controller</span></span>

### <a name="head-gaze-and-air-tap-gesture"></a><span data-ttu-id="e72e7-134">頭の視線入力とエアタップ ジェスチャ</span><span class="sxs-lookup"><span data-stu-id="e72e7-134">Head-gaze and air tap gesture</span></span>
<span data-ttu-id="e72e7-135">エアタップは、手をまっすぐにしてタップするジェスチャです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-135">Air tap is a tapping gesture with the hand held upright.</span></span> <span data-ttu-id="e72e7-136">エアタップを実行するには、インデックスの指を準備完了の位置まで上げて、親指でピンチし、インデックスを作成して、リリースまでさかのぼってください。</span><span class="sxs-lookup"><span data-stu-id="e72e7-136">To perform an air tap, raise your index finger to the ready position, then pinch with your thumb, and raise your index finger back up to release.</span></span> <span data-ttu-id="e72e7-137">HoloLens (第1世代) では、無線タップが最も一般的な2番目の入力です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-137">On HoloLens (1st Gen), air tap is the most common secondary input.</span></span>

![準備完了位置の指と、タップまたはクリックの動き](images/readyandpress.jpg)<br>

<span data-ttu-id="e72e7-139">無線タップは、HoloLens 2 でも使用できます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-139">Air tap is also available on HoloLens 2.</span></span> <span data-ttu-id="e72e7-140">これは、元のバージョンからは緩和されています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-140">It has been relaxed from the original version.</span></span> <span data-ttu-id="e72e7-141">ほとんどすべての種類の pinches は、手が垂直で維持されている限り、サポートされるようになりました。</span><span class="sxs-lookup"><span data-stu-id="e72e7-141">Nearly all types of pinches are now supported as long as the hand is upright and holding still.</span></span> <span data-ttu-id="e72e7-142">これによりユーザーは、はるかに簡単にジェスチャを学習したり実行したりできます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-142">This makes it much easier for users to learn and perform the gesture.</span></span> <span data-ttu-id="e72e7-143">この新しいエアタップでは、古いものが同じ API を使用して置き換えられるため、HoloLens 2 の再コンパイル後に既存のアプリケーションの新しい動作が自動的に行われます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-143">This new air tap replaces the old one through the same API, so existing applications will have the new behavior automatically after recompiling for HoloLens 2.</span></span>

### <a name="head-gaze-and-select-voice-command"></a><span data-ttu-id="e72e7-144">頭の視線入力と [選択] 音声コマンド</span><span class="sxs-lookup"><span data-stu-id="e72e7-144">Head-gaze and "Select" voice command</span></span>
<span data-ttu-id="e72e7-145">音声コマンド処理は、mixed reality の主要な相互作用メソッドの1つです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-145">Voice commanding is one of the primary interaction methods in mixed reality.</span></span> <span data-ttu-id="e72e7-146">システムを制御するための非常に強力な機能を提供します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-146">It provides a very powerful hands-free mechanism to control the system.</span></span> <span data-ttu-id="e72e7-147">さまざまな種類の音声操作モデルがあります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-147">There are diferent types of voice interaction models:</span></span>

- <span data-ttu-id="e72e7-148">クリック処理を実行する、または2次入力としてコミットする汎用コマンド選択。</span><span class="sxs-lookup"><span data-stu-id="e72e7-148">The generic command Select that performs a click actuation or commit as a secondary input.</span></span>
- <span data-ttu-id="e72e7-149">Close などのオブジェクトコマンドや、拡大したオブジェクトコマンドは、アクションをセカンダリ入力として実行してコミットします。</span><span class="sxs-lookup"><span data-stu-id="e72e7-149">Object commands like Close or Make it bigger performs and commits to an action as a secondary input.</span></span>
- <span data-ttu-id="e72e7-150">[開始] などのグローバルな commnads は、ターゲットを必要としません。</span><span class="sxs-lookup"><span data-stu-id="e72e7-150">Global commnads like Go to start don't require a target.</span></span>
- <span data-ttu-id="e72e7-151">対話ユーザーインターフェイスまたは Cortana のようなエンティティには、AI 自然言語機能があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-151">Conversation user interfaces or entities like Cortana have an AI natural language capability.</span></span>
- <span data-ttu-id="e72e7-152">カスタム コマンド</span><span class="sxs-lookup"><span data-stu-id="e72e7-152">Custom commnads</span></span>

<span data-ttu-id="e72e7-153">使用可能なコマンドとその使用方法の詳細については[、「comprenhesive](voice-design.md) 」を参照してください。</span><span class="sxs-lookup"><span data-stu-id="e72e7-153">To find more details as well as a comprenhesive list of available commands and how to use them, check out our [voice commanding](voice-design.md) guidance.</span></span>


### <a name="head-gaze-and-hololens-clicker"></a><span data-ttu-id="e72e7-154">頭の視線入力と HoloLens クリッカー</span><span class="sxs-lookup"><span data-stu-id="e72e7-154">Head-gaze and HoloLens Clicker</span></span>
<span data-ttu-id="e72e7-155">HoloLens Clicker は、HoloLens 専用に構築された最初の周辺機器です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-155">The HoloLens Clicker is the first peripheral device built specifically for HoloLens.</span></span> <span data-ttu-id="e72e7-156">これは HoloLens (第1世代) Development Edition に含まれています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-156">It is included with HoloLens (1st Gen) Development Edition.</span></span> <span data-ttu-id="e72e7-157">HoloLens Clicker を使用すると、ユーザーは最小限の手でクリックし、2番目の入力としてコミットできます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-157">The HoloLens Clicker lets a user click with minimal hand motion, and commit as a secondary input.</span></span> <span data-ttu-id="e72e7-158">HoloLens Clicker は、Bluetooth 低エネルギー (BTLE) を使用して HoloLens (第1世代) または HoloLens 2 に接続します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-158">The HoloLens Clicker connects to HoloLens (1st Gen) or HoloLens 2 using Bluetooth Low Energy (BTLE).</span></span>

<span data-ttu-id="e72e7-159">![HoloLens クリッカー](images/hololens-clicker-500px.jpg)</span><span class="sxs-lookup"><span data-stu-id="e72e7-159">![HoloLens Clicker](images/hololens-clicker-500px.jpg)</span></span><br>
<span data-ttu-id="e72e7-160">*HoloLens クリッカー*</span><span class="sxs-lookup"><span data-stu-id="e72e7-160">*HoloLens Clicker*</span></span>

<span data-ttu-id="e72e7-161">デバイスのペアリングの詳細と手順については、[こちら](hardware-accessories.md#pairing-bluetooth-accessories)を参照してください</span><span class="sxs-lookup"><span data-stu-id="e72e7-161">More information and instructions to pair the device can be found [here](hardware-accessories.md#pairing-bluetooth-accessories)</span></span>




### <a name="head-gaze-and-xbox-wireless-controller"></a><span data-ttu-id="e72e7-162">頭の視線入力と Xbox ワイヤレス コントローラー</span><span class="sxs-lookup"><span data-stu-id="e72e7-162">Head-gaze and Xbox Wireless Controller</span></span>
<span data-ttu-id="e72e7-163">Xbox ワイヤレスコントローラーは、"A" ボタンを使用して、2番目の入力としてクリックを実行します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-163">The Xbox Wireless Controller performs a click actuation as a secondary input by using the 'A' button.</span></span> <span data-ttu-id="e72e7-164">このデバイスは、システムのナビゲーションや制御に役立つ既定のアクションのセットにマップされています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-164">The device is mapped to a default set of actions that help navigate and controll the system.</span></span> <span data-ttu-id="e72e7-165">コントローラーをカスタマイズする場合は、Xbox Accesories アプリケーションを使用して Xbox ワイヤレスコントローラーを構成します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-165">If you want to customize the controller, use the Xbox Accesories application to configure your Xbox Wireless Controller.</span></span>

<span data-ttu-id="e72e7-166">![Xbox ワイヤレス コントローラー](images/xboxcontroller.jpg)</span><span class="sxs-lookup"><span data-stu-id="e72e7-166">![Xbox Wireless Controller](images/xboxcontroller.jpg)</span></span><br>
<span data-ttu-id="e72e7-167">*Xbox ワイヤレス コントローラー*</span><span class="sxs-lookup"><span data-stu-id="e72e7-167">*Xbox Wireless Controller*</span></span>

[<span data-ttu-id="e72e7-168">Xbox コントローラーと PC のペアリング</span><span class="sxs-lookup"><span data-stu-id="e72e7-168">Pairing an Xbox controller with your PC</span></span>](hardware-accessories.md#pairing-bluetooth-accessories)


### <a name="head-gaze-and-xbox-adaptive-controller"></a><span data-ttu-id="e72e7-169">頭の視線入力と Xbox Adaptive Controller</span><span class="sxs-lookup"><span data-stu-id="e72e7-169">Head-gaze and Xbox Adaptive Controller</span></span>
<span data-ttu-id="e72e7-170">多くの場合、モビリティが制限されたゲーマーのニーズを満たすように設計されています。 Xbox Adaptive Controller はデバイス用の統合されたハブであり、混合の現実によりアクセスしやすくなります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-170">Designed primarily to meet the needs of gamers with limited mobility, the Xbox Adaptive Controller is a unified hub for devices that helps make mixed reality more accessible.</span></span>

<span data-ttu-id="e72e7-171">Xbox Adaptive Controller は、"A" ボタンを使用して、2番目の入力としてクリックを実行します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-171">The Xbox Adaptive Controller performs a click actuation as a secondary input by using the 'A' button.</span></span> <span data-ttu-id="e72e7-172">デバイスは、システムの移動と制御に役立つ既定のアクションセットにマップされます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-172">The device is mapped to a default set of actions that help navigate and control the system.</span></span> <span data-ttu-id="e72e7-173">コントローラーをカスタマイズする場合は、Xbox Accesories アプリケーションを使用して Xbox Adaptive コントローラーを構成します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-173">If you want to customize the controller, use the Xbox Accesories application to configure your Xbox Adaptive Controller.</span></span>

<span data-ttu-id="e72e7-174">![Xbox Adaptive Controller](images/xbox-adaptive-controller-devices.jpg)</span><span class="sxs-lookup"><span data-stu-id="e72e7-174">![Xbox Adaptive Controller](images/xbox-adaptive-controller-devices.jpg)</span></span><br>
<span data-ttu-id="e72e7-175">*Xbox Adaptive Controller*</span><span class="sxs-lookup"><span data-stu-id="e72e7-175">*Xbox Adaptive Controller*</span></span>

<span data-ttu-id="e72e7-176">スイッチ、ボタン、マウント、ジョイスティックなどの外部デバイスを接続して、独自のカスタムコントローラーエクスペリエンスを作成します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-176">Connect external devices such as switches, buttons, mounts, and joysticks to create a custom controller experience that is uniquely yours.</span></span> <span data-ttu-id="e72e7-177">ボタン、サムスティック、およびトリガーの入力は、3.5 mm ジャックと USB ポートを介して接続されている補助デバイスで制御されます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-177">Button, thumbstick, and trigger inputs are controlled with assistive devices connected through 3.5mm jacks and USB ports.</span></span>

<span data-ttu-id="e72e7-178">![Xbox Adaptive Controller のポート](images/xbox-adaptive-controller-ports.jpg)</span><span class="sxs-lookup"><span data-stu-id="e72e7-178">![Xbox Adaptive Controller ports](images/xbox-adaptive-controller-ports.jpg)</span></span><br>
<span data-ttu-id="e72e7-179">*Xbox Adaptive Controller のポート*</span><span class="sxs-lookup"><span data-stu-id="e72e7-179">*Xbox Adaptive Controller ports*</span></span>

[<span data-ttu-id="e72e7-180">デバイスをペアリングする手順</span><span class="sxs-lookup"><span data-stu-id="e72e7-180">Instructions to pair the device</span></span>](hardware-accessories.md#pairing-bluetooth-accessories)

<span data-ttu-id="e72e7-181"><a href=https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller>Xbox のサイトで参照できる詳細情報</a></span><span class="sxs-lookup"><span data-stu-id="e72e7-181"><a href=https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller>More info available on the Xbox site</a></span></span>


## <a name="design-guidelines"></a><span data-ttu-id="e72e7-182">設計ガイドライン</span><span class="sxs-lookup"><span data-stu-id="e72e7-182">Design guidelines</span></span>
> [!NOTE]
> <span data-ttu-id="e72e7-183">視線入力の設計に特化したガイダンスは、[近日中に公開](index.md)します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-183">More guidance specific to gaze design [coming soon](index.md).</span></span>

## <a name="head-gaze-targeting"></a><span data-ttu-id="e72e7-184">頭の視線入力のターゲット設定</span><span class="sxs-lookup"><span data-stu-id="e72e7-184">Head-gaze targeting</span></span>
<span data-ttu-id="e72e7-185">すべての操作は、入力モードに関係なく、ユーザーが操作したい要素をターゲットに設定できることに基づいて成り立っています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-185">All interactions are built upon the ability of a user to target the element they want to interact with, regardless of the input modality.</span></span> <span data-ttu-id="e72e7-186">Windows Mixed Reality では、これは、通常はユーザーの視線入力を使用して行われます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-186">In Windows Mixed Reality, this is generally done using the user's gaze.</span></span>
<span data-ttu-id="e72e7-187">ユーザーがエクスペリエンスを正常に操作できるようにするには、システムがユーザーの意図を理解していることと、ユーザーの実際の意図ができるだけ近いものである必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-187">To enable a user to work with an experience successfully, the system's calculated understanding of a user's intent and the user's actual intent must align as closely as possible.</span></span> <span data-ttu-id="e72e7-188">システムがユーザーの意図したアクションを正しく解釈するレベルまで、満足度が高くなりパフォーマンスが向上します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-188">To the degree that the system interprets the user's intended actions correctly, satisfaction increases and performance improves.</span></span>


## <a name="target-sizing-and-feedback"></a><span data-ttu-id="e72e7-189">ターゲットのサイズ設定とフィードバック</span><span class="sxs-lookup"><span data-stu-id="e72e7-189">Target sizing and feedback</span></span>
<span data-ttu-id="e72e7-190">見つめベクターは、適切にターゲットを設定するために繰り返し表示されていますが、多くの場合、ターゲットの総計に最適です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-190">The gaze vector has been shown repeatedly to be usable for fine targeting, but often works best for gross targeting--acquiring somewhat larger targets.</span></span> <span data-ttu-id="e72e7-191">最小目標サイズが 1 ~ 1.5 °の場合、ほとんどのシナリオで成功したユーザー操作が可能になります。ただし、3度のターゲットでは、多くの場合、速度が向上します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-191">Minimum target sizes of 1 to 1.5 degrees allows successful user actions in most scenarios, though targets of 3 degrees often allow for greater speed.</span></span> <span data-ttu-id="e72e7-192">ユーザーがターゲットに設定するサイズは、3D 要素であっても実質的に 2D 領域であり、それが面しているどの投影もターゲット設定可能な領域になるはずです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-192">Note that the size that the user targets is effectively a 2D area even for 3D elements--whichever projection is facing them should be the targetable area.</span></span> <span data-ttu-id="e72e7-193">要素が "アクティブ" である (ユーザーがターゲットとしている) ことを示すいくつかの重要な手掛かりが、非常に便利です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-193">Providing some salient cue that an element is "active" (that the user is targeting it) is extremely helpful.</span></span> <span data-ttu-id="e72e7-194">これには、表示されている "ホバー" 効果、オーディオのハイライトやクリック、要素を含むカーソルの配置のクリアなどの処置が含まれます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-194">This can include treatments like visible "hover" effects, audio highlights or clicks, or clear alignment of a cursor with an element.</span></span>

<span data-ttu-id="e72e7-195">![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="e72e7-195">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="e72e7-196">*2 m の距離での最適なターゲット サイズ*</span><span class="sxs-lookup"><span data-stu-id="e72e7-196">*Optimal target size at 2 meter distance*</span></span>

<span data-ttu-id="e72e7-197">![視線入力のターゲットとなるオブジェクトの強調表示の例](images/gazetargeting-highlighting-640px.jpg)</span><span class="sxs-lookup"><span data-stu-id="e72e7-197">![An example of highlighting a gaze targeted object](images/gazetargeting-highlighting-640px.jpg)</span></span><br>
<span data-ttu-id="e72e7-198">*視線入力のターゲットとなるオブジェクトの強調表示の例*</span><span class="sxs-lookup"><span data-stu-id="e72e7-198">*An example of highlighting a gaze targeted object*</span></span>

## <a name="target-placement"></a><span data-ttu-id="e72e7-199">ターゲットの配置</span><span class="sxs-lookup"><span data-stu-id="e72e7-199">Target placement</span></span>
<span data-ttu-id="e72e7-200">多くの場合、ユーザーは、ビューのフィールドに非常に高い、または低い位置にある UI 要素を見つけることができず、主なフォーカスの周りにある領域に注目することに重点が置かれています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-200">Users often fail to find UI elements that are positioned very high or very low in their field of view, focusing most of their attention on areas around their main focus, which is approximately at eye level.</span></span> <span data-ttu-id="e72e7-201">ほとんどのターゲットは目の高さあたりの適正な帯域に配置すると効果的です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-201">Placing most targets in some reasonable band around eye level can help.</span></span> <span data-ttu-id="e72e7-202">ユーザーは常に比較的小さい視覚野に焦点を合わせる傾向があることを考慮すると (注意の視円錐は約 10 度)、UI 要素を概念的に関連性のある度数にグループ化すれば、ユーザーが視線入力で領域内を移動する場合に、項目から項目への注意の連鎖動作を活用できます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-202">Given the tendency for users to focus on a relatively small visual area at any time (the attentional cone of vision is roughly 10 degrees), grouping UI elements together to the degree that they're related conceptually can leverage attention-chaining behaviors from item to item as a user moves their gaze through an area.</span></span> <span data-ttu-id="e72e7-203">UI を設計する際は、HoloLens とイマーシブ ヘッドセットでは視野内に潜在的な大きなバリエーションがあることに注意してください。</span><span class="sxs-lookup"><span data-stu-id="e72e7-203">When designing UI, keep in mind the potential large variation in field of view between HoloLens and immersive headsets.</span></span>

<span data-ttu-id="e72e7-204">![Galaxy Explorer で視線入力のターゲット設定を容易にするためにグループ化された UI 要素の例](images/gazetargeting-grouping-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="e72e7-204">![An example of grouped UI elements for easier gaze targeting in Galaxy Explorer](images/gazetargeting-grouping-1000px.jpg)</span></span><br>
<span data-ttu-id="e72e7-205">*Galaxy Explorer で視線入力のターゲット設定を容易にするためにグループ化された UI 要素の例*</span><span class="sxs-lookup"><span data-stu-id="e72e7-205">*An example of grouped UI elements for easier gaze targeting in Galaxy Explorer*</span></span>

## <a name="improving-targeting-behaviors"></a><span data-ttu-id="e72e7-206">ターゲット設定動作の向上</span><span class="sxs-lookup"><span data-stu-id="e72e7-206">Improving targeting behaviors</span></span>
<span data-ttu-id="e72e7-207">ユーザーの意図を特定できる (または近似値を近似する) ことができた場合は、適切に対象としているかのように、操作の試行回数をよく受け入れることが非常に便利です。</span><span class="sxs-lookup"><span data-stu-id="e72e7-207">If user intent to target something can be determined (or approximated closely), it can be very helpful to accept near miss attempts at interaction as though they were targeted correctly.</span></span> <span data-ttu-id="e72e7-208">Mixed reality エクスペリエンスに組み込むことができる成功したメソッドのいくつかを次に示します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-208">Here are a handful of successful methods that can be incorporated in mixed reality experiences:</span></span>

### <a name="head-gaze-stabilization-gravity-wells"></a><span data-ttu-id="e72e7-209">頭の視線入力の安定化 ("重力井戸")</span><span class="sxs-lookup"><span data-stu-id="e72e7-209">Head-gaze stabilization ("gravity wells")</span></span>
<span data-ttu-id="e72e7-210">これは、ほとんどまたはすべての時間に有効にする必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-210">This should be turned on most or all of the time.</span></span> <span data-ttu-id="e72e7-211">この手法では、自然なヘッドとネックのジッターを排除します。これは、ユーザーが動作を検索したり話したりすることによって動きが多い場合があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-211">This technique removes the natural head and neck jitters that users might have as well movement due to looking and speaking behaviors.</span></span>

### <a name="closest-link-algorithms"></a><span data-ttu-id="e72e7-212">最も近いリンク アルゴリズム</span><span class="sxs-lookup"><span data-stu-id="e72e7-212">Closest link algorithms</span></span>
<span data-ttu-id="e72e7-213">これらは対話型コンテンツが少ない領域で最大の効果を発揮します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-213">These work best in areas with sparse interactive content.</span></span> <span data-ttu-id="e72e7-214">ユーザーが何を操作しようとしているかを判断できる確率が高い場合は、ある程度のインテントを想定して、対象となる機能を補完することができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-214">If there is a high probability that you can determine what a user was attempting to interact with, you can supplement their targeting abilities by assuming some level of intent.</span></span>

### <a name="backdating-and-postdating-actions"></a><span data-ttu-id="e72e7-215">バックと後処理のアクション</span><span class="sxs-lookup"><span data-stu-id="e72e7-215">Backdating and postdating actions</span></span>
<span data-ttu-id="e72e7-216">このメカニズムは、速度が必要なタスクに役立ちます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-216">This mechanism is useful in tasks requiring speed.</span></span> <span data-ttu-id="e72e7-217">ユーザーが一連のターゲットとアクティブ化を高速に進めるときは、何らかのインテントを想定し、ユーザーが tap の前または少しの間にフォーカスしていたターゲット (ea では50ミリ秒前/後) に対して、ユーザーの操作が不要な操作を実行できるようにすると便利です。テスト)。</span><span class="sxs-lookup"><span data-stu-id="e72e7-217">When a user is moving through a series of targeting and activation maneuvers at speed, it is useful to assume some intent, and allow missed steps to act upon targets that the user had in focus slightly before or slightly after the tap (50 ms before/after was effective in early testing).</span></span>

### <a name="smoothing"></a><span data-ttu-id="e72e7-218">スムージング</span><span class="sxs-lookup"><span data-stu-id="e72e7-218">Smoothing</span></span>
<span data-ttu-id="e72e7-219">このメカニズムは、自然なヘッド移動特性によってわずかなジッターと wobble を減らすことで、パスの移動に役立ちます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-219">This mechanism is useful for pathing movements, reducing the slight jitter and wobble due to natural head movement characteristics.</span></span> <span data-ttu-id="e72e7-220">パスモーションをスムーズにスムージングする場合は、時間の経過と共に、移動のサイズと距離を滑らかにします。</span><span class="sxs-lookup"><span data-stu-id="e72e7-220">When smoothing over pathing motions, smooth by the size and distance of movements rather than over time.</span></span>

### <a name="magnetism"></a><span data-ttu-id="e72e7-221">磁性</span><span class="sxs-lookup"><span data-stu-id="e72e7-221">Magnetism</span></span>
<span data-ttu-id="e72e7-222">このメカニズムは、最も近いリンクアルゴリズムのより一般的なバージョンであると考えることができます。これは、ターゲットに向かってカーソルを描画するか、またはユーザーが対話形式のレイアウトについての知識を使用して対象となる可能性のあるターゲットにアプローチすることで、パフォーマンスを向上させることができるからです。ユーザーの意図をアプローチします。</span><span class="sxs-lookup"><span data-stu-id="e72e7-222">This mechanism can be thought of as a more general version of closest link algorithms--drawing a cursor toward a target or simply increasing hitboxes, whether visibly or not, as users approach likely targets by using some knowledge of the interactive layout to better approach user intent.</span></span> <span data-ttu-id="e72e7-223">これは、小さいターゲットの場合は特に効果を発揮する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-223">This can be particularly powerful for small targets.</span></span>

### <a name="focus-stickiness"></a><span data-ttu-id="e72e7-224">焦点の持続性</span><span class="sxs-lookup"><span data-stu-id="e72e7-224">Focus stickiness</span></span>
<span data-ttu-id="e72e7-225">フォーカスのある近接要素にフォーカスを移すときに、フォーカスの持続性により、現在フォーカスがある要素にバイアスが適用されます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-225">When determining which nearby interactive elements to give focus to, focus stickiness provides a bias to the element that is currently focused.</span></span> <span data-ttu-id="e72e7-226">これにより、自然なノイズを持つ2つの要素間の中間点で浮動フォーカスの切り替えビヘイビアーを減らすことができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-226">This helps reduce erratic focus switching behaviours when floating at a midpoint between two elements with natural noise.</span></span>


## <a name="composite-gestures"></a><span data-ttu-id="e72e7-227">複合ジェスチャ</span><span class="sxs-lookup"><span data-stu-id="e72e7-227">Composite gestures</span></span>

### <a name="air-tap"></a><span data-ttu-id="e72e7-228">エアタップ</span><span class="sxs-lookup"><span data-stu-id="e72e7-228">Air tap</span></span>
<span data-ttu-id="e72e7-229">エアタップジェスチャ (およびその他のジェスチャ) は、特定の tap にのみ反応します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-229">The air tap gesture (as well as the other gestures below) reacts only to a specific tap.</span></span> <span data-ttu-id="e72e7-230">メニューやつかみなど、他のタップを検出するには、前の「2つの主要なコンポーネントのジェスチャ」セクションで説明されている下位レベルの相互作用をアプリケーションで直接使用する必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-230">To detect other taps, such as Menu or Grasp, your application must directly use the lower-level interactions described in the two key component gestures section above.</span></span>

### <a name="tap-and-hold"></a><span data-ttu-id="e72e7-231">タップ アンド ホールド</span><span class="sxs-lookup"><span data-stu-id="e72e7-231">Tap and hold</span></span>
<span data-ttu-id="e72e7-232">ホールドは、単にエアタップの下向きの指の位置を保持することです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-232">Hold is simply maintaining the downward finger position of the air tap.</span></span> <span data-ttu-id="e72e7-233">エアタップとホールディングを組み合わせることにより、さまざまな複雑な "クリックアンドドラッグ" の相互作用が可能になります。たとえば、オブジェクトをアクティブ化する代わりにオブジェクトを選択したり、コンテキストメニューを表示するなどの二次的な相互作用を待機したりすることができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-233">The combination of air tap and hold allows for a variety of more complex "click and drag" interactions when combined with arm movement such as picking up an object instead of activating it or mousedown secondary interactions such as showing a context menu.</span></span>
<span data-ttu-id="e72e7-234">ただし、このジェスチャの設計時には注意が必要です。これは、ユーザーはジェスチャを長く続けると途中で手の姿勢を緩める傾向があるためです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-234">Caution should be used when designing for this gesture however, as users can be prone to relaxing their hand postures during the course of any extended gesture.</span></span>

### <a name="manipulation"></a><span data-ttu-id="e72e7-235">操作</span><span class="sxs-lookup"><span data-stu-id="e72e7-235">Manipulation</span></span>
<span data-ttu-id="e72e7-236">操作ジェスチャを使用すると、ホログラムがユーザーの手の動きに1:1 を反応させる場合に、ホログラムの移動、サイズ変更、または回転を行うことができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-236">Manipulation gestures can be used to move, resize, or rotate a hologram when you want the hologram to react 1:1 to the user's hand movements.</span></span> <span data-ttu-id="e72e7-237">このような 1 対 1 の動きの 1 つの用途は、ユーザーが世界中で絵を描いたりペイントしたりできるようにすることです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-237">One use for such 1:1 movements is to let the user draw or paint in the world.</span></span>
<span data-ttu-id="e72e7-238">操作のジェスチャの最初のターゲット設定は、視線入力またはポインティングによって行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-238">The initial targeting for a manipulation gesture should be done by gaze or pointing.</span></span> <span data-ttu-id="e72e7-239">タップとホールドが開始されると、オブジェクトの操作は手動で処理され、ユーザーが操作中に見えなくなるのを解放します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-239">Once the tap and hold starts, any manipulation of the object is handled by hand movements, freeing the user to look around while they manipulate.</span></span>

### <a name="navigation"></a><span data-ttu-id="e72e7-240">ナビゲーション</span><span class="sxs-lookup"><span data-stu-id="e72e7-240">Navigation</span></span>
<span data-ttu-id="e72e7-241">ナビゲーションのジェスチャは仮想ジョイスティックのように動作し、リング メニューなどの UI ウィジェット内で移動するために使用できます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-241">Navigation gestures operate like a virtual joystick, and can be used to navigate UI widgets, such as radial menus.</span></span> <span data-ttu-id="e72e7-242">タップ アンド ホールドでジェスチャを始めてから、最初に押したところを中心に、正規化された 3D 立方体の中で手を動かします。</span><span class="sxs-lookup"><span data-stu-id="e72e7-242">You tap and hold to start the gesture and then move your hand within a normalized 3D cube, centered around the initial press.</span></span> <span data-ttu-id="e72e7-243">開始点 0 で、-1 から 1 までの値から X、Y、または Z 軸に沿って手を動かすことができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-243">You can move your hand along the X, Y or Z axis from a value of -1 to 1, with 0 being the starting point.</span></span>
<span data-ttu-id="e72e7-244">ナビゲーションを使用すると、マウスの中央ボタンをクリックしてからマウスを上下に移動して 2 次元の UI をスクロールするのと同様に、速度ベースの連続したスクロールやズームのジェスチャを作成できます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-244">Navigation can be used to build velocity-based continuous scrolling or zooming gestures, similar to scrolling a 2D UI by clicking the middle mouse button and then moving the mouse up and down.</span></span>

<span data-ttu-id="e72e7-245">Rails を使用したナビゲーションとは、特定の軸で特定のしきい値に達するまで移動を認識する機能を指します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-245">Navigation with rails refers to the ability of recognizing movements in certain axis until a certain threshold is reached on that axis.</span></span> <span data-ttu-id="e72e7-246">これは、開発者によってアプリケーションで複数の軸での移動が有効になっている場合にのみ役立ちます。たとえば、アプリケーションが X 軸と Y 軸にわたるナビゲーションジェスチャを認識するように構成されていても、X 軸に rails が指定されている場合などです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-246">This is only useful when movement in more than one axis is enabled in an application by the developer, such as if an application is configured to recognize navigation gestures across X, Y axis but also specified X axis with rails.</span></span> <span data-ttu-id="e72e7-247">この場合、システムは x 軸上の架空のレール (ガイド) 内にある限り、X 軸を越えた手の移動を認識します。これは、Y 軸でも手動で移動した場合に発生します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-247">In this case the system will recognize hand movements across X axis as long as they remain within an imaginary rails (guide) on the X axis, if hand movement also occurs on the Y axis.</span></span>

<span data-ttu-id="e72e7-248">2D のアプリ内では、ユーザーは、アプリ内でスクロール、ズーム、またはドラッグするために、垂直方向のナビゲーション ジェスチャを使用できます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-248">Within 2D apps, users can use vertical navigation gestures to scroll, zoom, or drag inside the app.</span></span> <span data-ttu-id="e72e7-249">これは、同じ種類のタッチ ジェスチャをシミュレートするために、アプリに仮想の指でのタッチを挿入します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-249">This injects virtual finger touches to the app to simulate touch gestures of the same type.</span></span> <span data-ttu-id="e72e7-250">ユーザーは、アプリケーションの上部にあるバーのツールを切り替えることによって実行するアクションを選択できます。これを行うには、ボタンを選択するか、[スクロール/ドラッグ/ズーム > ツールを <] をクリックします。</span><span class="sxs-lookup"><span data-stu-id="e72e7-250">Users can select which of these actions take place by toggling between the tools on the bar above the application, either by selecting the button or saying '<Scroll/Drag/Zoom> Tool'.</span></span>

[<span data-ttu-id="e72e7-251">複合ジェスチャに関する詳細情報</span><span class="sxs-lookup"><span data-stu-id="e72e7-251">More info on composite gestures</span></span>](gestures.md#composite-gestures)

## <a name="gesture-recognizers"></a><span data-ttu-id="e72e7-252">ジェスチャ認識エンジン</span><span class="sxs-lookup"><span data-stu-id="e72e7-252">Gesture recognizers</span></span>

<span data-ttu-id="e72e7-253">ジェスチャ認識を使用する利点の1つは、現在ターゲットとなっているホログラムが受け入れるジェスチャに対してのみジェスチャ認識エンジンを構成できることです。</span><span class="sxs-lookup"><span data-stu-id="e72e7-253">One benefit of using gesture recognition is that you can configure a gesture recognizer only for the gestures the currently targeted hologram can accept.</span></span> <span data-ttu-id="e72e7-254">プラットフォームは、サポートされている特定のジェスチャを区別するために、必要に応じてあいまいさを解消します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-254">The platform only does disambiguation as necessary to distinguish those particular supported gestures.</span></span> <span data-ttu-id="e72e7-255">このようにして、エアタップをサポートしているホログラムは、プレスとリリースの間に任意の時間を受け入れることができます。一方、タップとホールドの両方をサポートするホログラムは、ホールド時間のしきい値を超えたときにタップを保留に昇格させることができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-255">In this way, a hologram that just supports air tap can accept any length of time between press and release, while a hologram that supports both tap and hold can promote the tap to a hold after the hold time threshold.</span></span>

## <a name="hand-recognition"></a><span data-ttu-id="e72e7-256">手の認識</span><span class="sxs-lookup"><span data-stu-id="e72e7-256">Hand recognition</span></span>
<span data-ttu-id="e72e7-257">HoloLens は、デバイスで確認できる片手または両手の位置を追跡することで、手のジェスチャを認識します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-257">HoloLens recognizes hand gestures by tracking the position of either or both hands that are visible to the device.</span></span> <span data-ttu-id="e72e7-258">HoloLens は、手が準備完了状態 (手の甲を自分に向けて人差し指を立てる) または押された状態 (手の甲を自分に向けて人差し指を曲げる) のいずれかの状態のときに、手を認識します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-258">HoloLens sees hands when they are in either the ready state (back of the hand facing you with index finger up) or the pressed state (back of the hand facing you with the index finger down).</span></span> <span data-ttu-id="e72e7-259">他の人の手による場合、HoloLens は z を無視します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-259">When hands are in other poses, HoloLens ignore themz.</span></span>
<span data-ttu-id="e72e7-260">HoloLens が検出した各ハンドでは、向きと押されていない状態でその位置にアクセスできます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-260">For each hand that HoloLens detects, you can access its position without orientation and its pressed state.</span></span> <span data-ttu-id="e72e7-261">手がジェスチャ フレームの端に近づくと、方向ベクトルも表示されます。これをユーザーに示すことで、ユーザーは、どのように手を動かせば、HoloLens が認識できる位置に戻せるかを知ることができます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-261">As the hand nears the edge of the gesture frame, you're also provided with a direction vector, which you can show to the user so they know how to move their hand to get it back where HoloLens can see it.</span></span>

## <a name="gesture-frame"></a><span data-ttu-id="e72e7-262">ジェスチャ フレーム</span><span class="sxs-lookup"><span data-stu-id="e72e7-262">Gesture frame</span></span>
<span data-ttu-id="e72e7-263">HoloLens でのジェスチャでは、ジェスチャが検出されたカメラが適切に見える範囲で、ウェストとショルダーの間にジェスチャを使用する必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-263">For gestures on HoloLens, the hand must be within a gesture frame, in a range that the gesture-sensing cameras can see appropriately,  from nose to waist and between the shoulders.</span></span> <span data-ttu-id="e72e7-264">ユーザーは、正常に動作していて快適にするために、この認識の領域でトレーニングを受ける必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-264">Users need to be trained on this area of recognition both for success of action and for their own comfort.</span></span> <span data-ttu-id="e72e7-265">多くのユーザーは、最初に、ジェスチャフレームが HoloLens を通じて表示されている必要があります。また、対話するために、そのアームをすぐに保持しておく必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-265">Many users will initially assume that the gesture frame must be within their view through HoloLens, and hold their arms up uncomfortably in order to interact.</span></span> <span data-ttu-id="e72e7-266">HoloLens Clicker を使用する場合、ジェスチャフレーム内にハンドを配置する必要はありません。</span><span class="sxs-lookup"><span data-stu-id="e72e7-266">When using the HoloLens Clicker, it's not necessary for hands to be within the gesture frame.</span></span>

<span data-ttu-id="e72e7-267">特に連続したジェスチャの場合、ユーザーが holographic オブジェクトを移動するときにジェスチャの途中でハンドを動かしたときに、意図した結果が失われる危険性があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-267">In the case of continuous gestures in particular, there is some risk of users moving their hands outside of the gesture frame while in mid-gesture when moving a holographic object, for example, and losing their intended outcome.</span></span>

<span data-ttu-id="e72e7-268">考慮すべきことが 3 つあります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-268">There are three things that you should consider:</span></span>

- <span data-ttu-id="e72e7-269">ジェスチャフレームの存在とおおよその境界に関するユーザー教育。</span><span class="sxs-lookup"><span data-stu-id="e72e7-269">User education on the gesture frame's existence and approximate boundaries.</span></span> <span data-ttu-id="e72e7-270">これは、HoloLens セットアップ中に学習されます。</span><span class="sxs-lookup"><span data-stu-id="e72e7-270">This is taught during HoloLens setup.</span></span>

- <span data-ttu-id="e72e7-271">アプリケーション内のジェスチャが近づいているか、ジェスチャフレームの境界が失われた場合に、望ましくない結果につながることをユーザーに通知します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-271">Notifying users when their gestures are nearing or breaking the gesture frame boundaries within an application to the degree that a lost gesture leads to undesired outcomes.</span></span> <span data-ttu-id="e72e7-272">調査には、このような通知システムの主要な品質が示されています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-272">Research has shown the key qualities of such a notification system.</span></span> <span data-ttu-id="e72e7-273">HoloLens シェルは、この種類の通知の良い例を提供しています。つまり、中央カーソルで、境界の交差が行われている方向を示しています。</span><span class="sxs-lookup"><span data-stu-id="e72e7-273">The HoloLens shell provides a good example of this type of notification--visual, on the central cursor, indicating the direction in which boundary crossing is taking place.</span></span>

- <span data-ttu-id="e72e7-274">ジェスチャ フレームの境界を越えることによる影響は、最小限に抑える必要があります。</span><span class="sxs-lookup"><span data-stu-id="e72e7-274">Consequences of breaking the gesture frame boundaries should be minimized.</span></span> <span data-ttu-id="e72e7-275">一般に、これは、ジェスチャの結果を、逆順ではなく境界で停止する必要があることを意味します。</span><span class="sxs-lookup"><span data-stu-id="e72e7-275">In general, this means that the outcome of a gesture should be stopped at the boundary, and not reversed.</span></span> <span data-ttu-id="e72e7-276">たとえば、ユーザーがある程度の holographic オブジェクトを部屋に移動する場合、ジェスチャフレームが侵害されたときに移動が停止し、開始位置には返されません。</span><span class="sxs-lookup"><span data-stu-id="e72e7-276">For example, if a user is moving some holographic object across a room, the movement should stop when the gesture frame is breached, and not returned to the starting point.</span></span> <span data-ttu-id="e72e7-277">ユーザーにはいくつかのフラストレーションが生じる可能性がありますが、境界をよりよく理解しておくことが必要であり、すべての目的のアクションを毎回再起動する必要はありません。</span><span class="sxs-lookup"><span data-stu-id="e72e7-277">The user might experience some frustration, but might more quickly understand the boundaries, and not have to restart their full intended actions each time.</span></span>


## <a name="see-also"></a><span data-ttu-id="e72e7-278">関連項目</span><span class="sxs-lookup"><span data-stu-id="e72e7-278">See also</span></span>
* [<span data-ttu-id="e72e7-279">手で直接操作</span><span class="sxs-lookup"><span data-stu-id="e72e7-279">Direct manipulation with hands</span></span>](direct-manipulation.md)
* [<span data-ttu-id="e72e7-280">手を使ったポイントとコミット</span><span class="sxs-lookup"><span data-stu-id="e72e7-280">Point and commit with hands</span></span>](point-and-commit.md)
* [<span data-ttu-id="e72e7-281">本能的な操作</span><span class="sxs-lookup"><span data-stu-id="e72e7-281">Instinctual interactions</span></span>](interaction-fundamentals.md)
* [<span data-ttu-id="e72e7-282">ヘッド視線入力とドウェル</span><span class="sxs-lookup"><span data-stu-id="e72e7-282">Head-gaze and dwell</span></span>](gaze-and-dwell.md)
* [<span data-ttu-id="e72e7-283">音声コマンド</span><span class="sxs-lookup"><span data-stu-id="e72e7-283">Voice commanding</span></span>](voice-design.md)





