---
title: 頭の視線入力とコミット
description: 頭の視線入力とコミットの入力モデルの概要
author: caseymeekhof
ms.author: cmeekhof
ms.date: 03/31/2019
ms.topic: article
ms.localizationpriority: high
keywords: Mixed Reality, 視線入力, 視線入力ターゲット設定, 対話, 設計
ms.openlocfilehash: d9eae3c0cfceba7c2c31425941dfce865f3aa609
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 06/05/2019
ms.locfileid: "66692304"
---
# <a name="head-gaze-and-commit"></a>頭の視線入力とコミット
頭の視線入力とコミットは、前を向いた頭の方向 (頭の向き) にあるオブジェクトをターゲットにしてから、手のジェスチャであるエアタップや音声コマンドの [選択] などのセカンダリ入力でそれを操作する入力モデルです。 これは [遠距離] 入力モデルと考えられており、手の届く範囲を越えたところにあるコンテンツの操作に最適です。

## <a name="device-support"></a>デバイスのサポート

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><strong>入力モデル</strong></td>
        <td><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></td>
        <td><strong>HoloLens 2</strong></td>
        <td><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></td>
    </tr>
     <tr>
        <td>頭の視線入力とコミット</td>
        <td>✔️ 推奨</td>
        <td>✔️ 推奨 (3 番目の選択肢 -<a href="interaction-fundamentals.md">他のオプションを参照</a>)</td>
        <td>➕ 代替オプション</td>
    </tr>
</table>

## <a name="head-gaze"></a>頭の視線入力
Mixed Reality ヘッドセットは、ユーザーの頭の位置と向きを利用して、その頭の方向ベクトルを決定します。 これは、ユーザーの目と目の間から直接、まっすぐ前を指し示すレーザーと考えることができます。 これはユーザーが目を向けている場所の非常に粗い近似値です。 アプリケーションは、この光線を仮想のオブジェクトや現実世界のオブジェクトと交差させ、その場所にカーソルを描画して、現在ターゲットにしているものをユーザーに知らせます。

HoloLens 2 のような一部の Mixed Reality ヘッドセットには、頭の視線入力だけでなく、目の視線入力ベクトルを生成する視線追跡システムが組み込まれています。 これにより、ユーザーが目を向けている場所のきめ細かい測定値が得られます。 視線入力をビルドし、目の視線入力を使用して操作をコミットすることはできますが、これにはまったく異なる設計上の制約が伴います。これについては[視線追跡の記事](eye-tracking.md)で別に説明します。

## <a name="commit"></a>コミット
ユーザーは、オブジェクトまたは UI 要素をターゲットにした後、セカンダリ入力を使用してそれを操作したり [クリック] したりできます。 これは、モデルのコミット ステップと呼ばれています。 以下のコミット方法がサポートされています。

- エアタップ ジェスチャ
- 音声コマンドの [選択]、または対象となる音声コマンドのいずれかを読み上げる
- [HoloLens クリッカー](hardware-accessories.md#hololens-clicker)で単一のボタンを押す
- Xbox コントローラーの 'A' ボタンを押す
- Xbox Adaptive Controller の 'A' ボタンを押す

### <a name="head-gaze-and-air-tap-gesture"></a>頭の視線入力とエアタップ ジェスチャ
エアタップは、手をまっすぐにしてタップするジェスチャです。 エアタップを実行するには、人差し指を準備完了位置まで立ててから親指でピンチし、放すときはもう一度人差し指を立てます。 HoloLens 1 では、エアタップは最も一般的なセカンダリ入力です。

![準備完了位置の指と、タップまたはクリックの動き](images/readyandpress.jpg)<br>

エアタップは HoloLens 2 でも使用でき、元のバージョンから緩和されています。 手をまっすぐにして握っている限り、ほぼすべてのピンチの種類がサポートされるようになりました。 これによりユーザーは、はるかに簡単にジェスチャを学習したり実行したりできます。  この新しいエアタップは同じ API を使って古いものの後を継いでいるため、既存のアプリケーションは HoloLens 2 用に再コンパイルした後で新しい動作を自動的に取得します。

### <a name="head-gaze-and-select-voice-command"></a>頭の視線入力と [選択] 音声コマンド
音声コマンドの実行は、Mixed Reality での主要な操作方法の 1 つです。 これは、システムを制御するための非常に強力な "ハンズフリー" メカニズムを提供します。 さまざまな種類の音声操作モデルがあります。

- 汎用コマンドの [選択]。[クリック] の作動またはコミットをセカンダリ入力として実行できるようにします。
- [閉じる] や [大きくする] のようなオブジェクト コマンド。アクションを実行したりアクションにコミットしたりできるにします。
- ターゲットを必要としない [スタートに移動] などのグローバル コマンド。
- AI 自然言語機能を搭載した Cortana などの、会話のユーザー インターフェイスまたはエンティティ。
- カスタム コマンド

使用可能なコマンドと使用方法の詳細と包括的な一覧については、「[音声コマンド](voice-design.md)」のガイダンスを参照してください。


### <a name="head-gaze-and-hololens-clicker"></a>頭の視線入力と HoloLens クリッカー
HoloLens クリッカーは、HoloLens 専用に構築された最初の周辺機器で、HoloLens 1 Development Edition に含まれています。 HoloLens クリッカーでは、ユーザーは、最小限の手の動きでクリックし、セカンダリ入力としてコミットできます。 HoloLens クリッカーは、Bluetooth 低エネルギー (BTLE) を使用して HoloLens 1 または 2 に接続します。

![HoloLens クリッカー](images/hololens-clicker-500px.jpg)<br>
*HoloLens クリッカー*

デバイスのペアリングの詳細と手順については、[こちら](hardware-accessories.md#pairing-bluetooth-accessories)を参照してください




### <a name="head-gaze-and-xbox-wireless-controller"></a>頭の視線入力と Xbox ワイヤレス コントローラー
Xbox ワイヤレス コントローラーでは、A ボタンを使用して [クリック] の作動をセカンダリ入力として実行できます。 このデバイスは、システムのナビゲーションや制御に役立つ既定のアクションのセットにマップされています。 コントローラーをカスタマイズする場合は、Xbox アクセサリー アプリを使用して Xbox ワイヤレス コントローラーを構成します。

![Xbox ワイヤレス コントローラー](images/xboxcontroller.jpg)<br>
*Xbox ワイヤレス コントローラー*

[Xbox コントローラーと PC のペアリング](hardware-accessories.md#pairing-bluetooth-accessories)


### <a name="head-gaze-and-xbox-adaptive-controller"></a>頭の視線入力と Xbox Adaptive Controller
Xbox Adaptive Controller は、主に可動性に制限のあるゲーマーのニーズに応えるために設計されており、Mixed Reality を使いやすくするのに役立つデバイスが 1 つにまとめられたハブです。

Xbox Adaptive Controller では、A ボタンを使用して [クリック] の作動をセカンダリ入力として実行できます。 このデバイスは、システムのナビゲーションや制御に役立つ既定のアクションのセットにマップされています。 コントローラーをカスタマイズする場合は、Xbox アクセサリー アプリを使用して Xbox Adaptive Controller を構成します。

![Xbox Adaptive Controller](images/xbox-adaptive-controller-devices.jpg)<br>
*Xbox Adaptive Controller*

スイッチ、ボタン、マウント、ジョイスティックなどの外部のデバイスを接続して、ユーザーにとって独自のカスタムのコントローラーのエクスペリエンスを作成します。 ボタン、スティック、およびトリガーの入力は、3.5 mm のジャックと USB ポートを介して接続されている補助デバイスで制御されます。

![Xbox Adaptive Controller のポート](images/xbox-adaptive-controller-ports.jpg)<br>
*Xbox Adaptive Controller のポート*

[デバイスをペアリングする手順](hardware-accessories.md#pairing-bluetooth-accessories)

<a href=https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller>Xbox のサイトで参照できる詳細情報</a>


## <a name="design-guidelines"></a>設計ガイドライン
> [!NOTE]
> 視線入力の設計に特化したガイダンスは、[近日中に公開](index.md)します。

## <a name="head-gaze-targeting"></a>頭の視線入力のターゲット設定
すべての操作は、入力モードに関係なく、ユーザーが操作したい要素をターゲットに設定できることに基づいて成り立っています。 Windows Mixed Reality では、これは、通常はユーザーの視線入力を使用して行われます。
ユーザーがエクスペリエンスを正常に使用できるようにするには、ユーザーの意図に対する計算されたシステムの理解と、ユーザーの実際の意図に、できる限り忠実に従う必要があります。 システムがユーザーの意図したアクションを正しく解釈するレベルまで、満足度が高くなりパフォーマンスが向上します。


## <a name="target-sizing-and-feedback"></a>ターゲットのサイズ設定とフィードバック
視線入力ベクトルは、細かいターゲット設定はできなくても大まかなターゲット設定 (若干大きめのターゲットの取得) には最適であることは、繰り返し示されています。 ターゲットのサイズが最小 1 ～ 1.5 度では、ほとんどのシナリオでユーザーのアクションが成功するはずですが、3 度のターゲットでは、多くの場合、速度がより速いことが考慮されます。 ユーザーがターゲットに設定するサイズは、3D 要素であっても実質的に 2D 領域であり、それが面しているどの投影もターゲット設定可能な領域になるはずです。 要素が "アクティブ" である (ユーザーがそれをターゲットにしている) という目立った合図を示すことは非常に役に立ちます。これには、目に見える "ホバー" 効果、音声のハイライトやクリック、要素へのカーソルの明確な配置などの処理が含まれます。

![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)<br>
*2 m の距離での最適なターゲット サイズ*

![視線入力のターゲットとなるオブジェクトの強調表示の例](images/gazetargeting-highlighting-640px.jpg)<br>
*視線入力のターゲットとなるオブジェクトの強調表示の例*

## <a name="target-placement"></a>ターゲットの配置
ユーザーは、自分の視野内の非常に高い位置や非常に低い位置にある UI 要素を見つけられないことが多く、ユーザーの意識は主に焦点が合っている部分 (通常はほぼ目の高さ) に集中しています。 ほとんどのターゲットは目の高さあたりの適正な帯域に配置すると効果的です。 ユーザーは常に比較的小さい視覚野に焦点を合わせる傾向があることを考慮すると (注意の視円錐は約 10 度)、UI 要素を概念的に関連性のある度数にグループ化すれば、ユーザーが視線入力で領域内を移動する場合に、項目から項目への注意の連鎖動作を活用できます。 UI を設計する際は、HoloLens とイマーシブ ヘッドセットでは視野内に潜在的な大きなバリエーションがあることに注意してください。

![Galaxy Explorer で視線入力のターゲット設定を容易にするためにグループ化された UI 要素の例](images/gazetargeting-grouping-1000px.jpg)<br>
*Galaxy Explorer で視線入力のターゲット設定を容易にするためにグループ化された UI 要素の例*

## <a name="improving-targeting-behaviors"></a>ターゲット設定動作の向上
何かをターゲットにするというユーザーの意図を判断できる場合は (または、ほぼ間違いないと思われる場合は)、正しくターゲットに設定されているかのように、操作で "ニアミス" の試行を受け入れると非常に便利な場合があります。 Mixed Reality エクスペリエンスに組み込むことができる、成功する方法がいくつかあります。

### <a name="head-gaze-stabilization-gravity-wells"></a>頭の視線入力の安定化 ("重力井戸")
これは、ほとんど、または常にオンにしておくべきです。 この手法は、ユーザーに起こることのある頭や首の自然な小刻みな揺れを取り除きます。 また、見たり話したりする動作による動きも取り除きます。

### <a name="closest-link-algorithms"></a>最も近いリンク アルゴリズム
これらは対話型コンテンツが少ない領域で最大の効果を発揮します。 ユーザーが何を操作しようとしていたのかを判断できる可能性が高い場合は、ある程度の意図を想定するだけで、ターゲット設定機能を補完できます。

### <a name="backdatingpostdating-actions"></a>アクションをさかのぼって有効にする/後で有効にする
このメカニズムは、速度が必要なタスクに役立ちます。 ユーザーが一連のターゲット設定/アクティブ化の操作をすばやく進めているときは、何らかの意図を想定して、飛ばされた手順で、ユーザーがタップの少し前または少し後 (初期段階のテストでは前後 50 ミリ秒が効果的でした) に焦点を合わせたターゲットに作用できるようにすると便利です。

### <a name="smoothing"></a>スムージング
このメカニズムはパス設定の動きで役に立ち、自然な頭の動きの特性によるわずかな小刻みな揺れやぐらつきを減らします。 パス設定の動きをスムーズにするときは、時間の経過と共にではなく、動きのサイズ/距離によってスムーズにしてください。

### <a name="magnetism"></a>磁性
このメカニズムは、"最も近いリンク" アルゴリズムのより一般的なバージョンと考えることができます。つまり、ユーザーの意図に近づくために対話型レイアウトに関する知識を活用して、ユーザーがターゲットに近づくにつれてターゲットに向かってカーソルを描画したり、単にヒットボックスを増やしたりします。 これは、小さいターゲットの場合は特に効果を発揮する可能性があります。

### <a name="focus-stickiness"></a>焦点の持続性
近くにあるどの対話型要素に焦点を合わせるかを判断するときは、現在焦点が合っている要素にバイアスを提供します。 これは、自然なノイズを含む 2 つの要素間の中間でさまよっているときの、焦点の不規則な切り替え動作を減らすのに役立ちます。


## <a name="composite-gestures"></a>複合ジェスチャ
アプリは個々のタップ以上のものを認識できます。 タップを組み合わせることで、手の動きでホールドしたりリリースしたりして、複雑な複合ジェスチャを実行できます。 これらの複合または高レベルのジェスチャは、開発者がアクセスできる (エアタップとブルームからの) 低レベルの空間入力データを利用します。

### <a name="air-tap"></a>エアタップ
エアタップ ジェスチャ (および以下のその他のジェスチャ) は、特定のタップのみに反応します。 アプリは、[メニュー] や [握る] などの他のタップを検出するために、上記の 2 つの重要な要素であるジェスチャのセクションで説明されている低レベルの操作を直接使用する必要があります。

### <a name="tap-and-hold"></a>タップ アンド ホールド
ホールドは、単にエアタップの下向きの指の位置を保持することです。 エアタップとホールドを組み合わせると、オブジェクトのアクティブ化の代わりのピックアップや、コンテキスト メニューの表示などの "マウスダウン" の 2 次操作などの腕の動きと組み合わせたときに、より複雑なさまざまな "クリックしてドラッグ" 操作が可能になります。
ただし、このジェスチャの設計時には注意が必要です。これは、ユーザーはジェスチャを長く続けると途中で手の姿勢を緩める傾向があるためです。

### <a name="manipulation"></a>操作
操作のジェスチャを使用すると、ユーザーの手の動きに対して 1 対 1 でホログラムに反応をさせるときに、ホログラムを移動、サイズ変更、または回転させることができます。 このような 1 対 1 の動きの 1 つの用途は、ユーザーが世界中で絵を描いたりペイントしたりできるようにすることです。
操作のジェスチャの最初のターゲット設定は、視線入力またはポインティングによって行う必要があります。 タップ アンド ホールドを始めると、オブジェクトのすべての操作が手の動きによって処理され、ユーザーは、操作している間は自由に見回すことができます。

### <a name="navigation"></a>ナビゲーション
ナビゲーションのジェスチャは仮想ジョイスティックのように動作し、リング メニューなどの UI ウィジェット内で移動するために使用できます。 タップ アンド ホールドでジェスチャを始めてから、最初に押したところを中心に、正規化された 3D 立方体の中で手を動かします。 開始点 0 で、-1 から 1 までの値から X、Y、または Z 軸に沿って手を動かすことができます。
ナビゲーションを使用すると、マウスの中央ボタンをクリックしてからマウスを上下に移動して 2 次元の UI をスクロールするのと同様に、速度ベースの連続したスクロールやズームのジェスチャを作成できます。

レールを使用したナビゲーションは、ある軸で一定のしきい値に達するまで、その軸での移動を認識する機能を指します。 これは、開発者がアプリケーション内で複数の軸での移動を有効にしている場合にのみ役立ちます。たとえば、アプリケーションが X、Y 軸を横切るナビゲーションのジェスチャを認識するように構成されていて、同時に X 軸がレールを使用して指定されている場合です。 この場合、システムは、手の動きが Y 軸上でも発生する合は、それが X 軸上の想像上のレール (ガイド) 内にとどまっている限り、X 軸を横切る手の動きを認識します。

2D のアプリ内では、ユーザーは、アプリ内でスクロール、ズーム、またはドラッグするために、垂直方向のナビゲーション ジェスチャを使用できます。 これは、同じ種類のタッチ ジェスチャをシミュレートするために、アプリに仮想の指でのタッチを挿入します。 ユーザーは、ボタンを選択するか、'ツールを <スクロール/ドラッグ/ズーム>' と読み上げることで、アプリ上部のバーでツールを切り替えて、これらのどのアクションを実行するかを選択できます。

[複合ジェスチャに関する詳細情報](gestures.md#composite-gestures)

## <a name="gesture-recognizers"></a>ジェスチャ認識エンジン

ジェスチャ認識を使用する利点の 1 つは、現在のターゲットであるホログラムが受け入れることができるジェスチャだけのためにジェスチャ認識エンジンを構成できることです。 プラットフォームでは、これらのサポートされている特定のジェスチャを区別するために必要な曖昧性の解消のみを行います。 このように、エアタップのみをサポートするホログラムは、押してから放すまでの任意の長さの時間を受け入れることができ、タップとホールドの両方をサポートするホログラムは、ホールド時間のしきい値の後にタップをホールドに昇格します。

## <a name="hand-recognition"></a>手の認識
HoloLens は、デバイスで確認できる片手または両手の位置を追跡することで、手のジェスチャを認識します。 HoloLens は、手が準備完了状態 (手の甲を自分に向けて人差し指を立てる) または押された状態 (手の甲を自分に向けて人差し指を曲げる) のいずれかの状態のときに、手を認識します。 その他の手のポーズの場合、HoloLens はそれらを無視します。
HoloLens が検出するそれぞれの手について、その位置 (向きはなし) と押された状態にアクセスすることができます。 手がジェスチャ フレームの端に近づくと、方向ベクトルも表示されます。これをユーザーに示すことで、ユーザーは、どのように手を動かせば、HoloLens が認識できる位置に戻せるかを知ることができます。

## <a name="gesture-frame"></a>ジェスチャ フレーム
HoloLens のジェスチャでは、手は、ジェスチャを検知するカメラが適切に認識できる範囲 (非常に大まかに言うと、鼻から腰までの肩と肩の間) 内の、"ジェスチャ フレーム" 内にある必要があります。 ユーザーは、アクションの成功と自分自身の快適さの両方のために、この認識領域についてトレーニングを受ける必要があります (多くのユーザーは、最初は、ジェスチャ フレームは HoloLens を通した視野内にあり、操作するためには不快なほど腕を上げなければならないと思い込んでいます。 HoloLens クリッカーを使用するときは、手がジェスチャ フレーム内にある必要はありません。

特に連続したジェスチャの場合は、ユーザーがジェスチャの途中で (たとえば、ホログラフィック オブジェクトの移動中に) ジェスチャ フレームの外側で手を動かし、意図した結果を見失う危険性があります。

考慮すべきことが 3 つあります。

- ジェスチャ フレームのエクスペリエンスとおおよその境界に関するユーザーへの指導 (これは HoloLens のセットアップ中に指導されます)。

- ジェスチャが、ジェスチャが失われると望ましくない結果が生じる可能性がある程度まで、アプリケーション内のジェスチャ フレームの境界に近づいている、または境界を越えているときの、ユーザーへの通知。 調査により、そのような通知システムの重要な特質が示されています。また、HoloLens シェルで、この種類の通知の良い例が提供されています (視覚的、中央のカーソル上、境界の交差が発生している方向を示す)。

- ジェスチャ フレームの境界を越えることによる影響は、最小限に抑える必要があります。 通常、これは、ジェスチャの結果が境界で停止していても、逆転はしていないことを意味します。 たとえば、ユーザーが部屋を横切るホログラフィック オブジェクトを動かしている場合、ジェスチャ フレームを越えると動きは停止しますが、開始点には戻りません。 その場合、ユーザーはフラストレーションを感じるかもしれませんが、より早く境界を理解して、意図したアクションすべてを毎回再開しなくても済む場合があります。


## <a name="see-also"></a>関連項目
* [手で直接操作](direct-manipulation.md)
* [手を使ったポイントとコミット](point-and-commit.md)
* [本能的な操作](interaction-fundamentals.md)
* [ヘッド視線入力とドウェル](gaze-and-dwell.md)
* [音声コマンド](voice-design.md)





