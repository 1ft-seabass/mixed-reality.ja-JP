---
title: Head 注視とコミット
description: Head 注視し、コミットの入力モデルの概要
author: caseymeekhof
ms.author: cmeekhof
ms.date: 03/31/2019
ms.topic: article
ms.localizationpriority: high
keywords: 複合現実、視線の先を対象とする操作、視線の先をデザインします。
ms.openlocfilehash: a84465de3479bf3da2131b94dd522539cd7de6e9
ms.sourcegitcommit: c20563b8195c0c374a927b96708d958b127ffc8f
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 05/21/2019
ms.locfileid: "65974876"
---
# <a name="head-gaze-and-commit"></a><span data-ttu-id="f876c-104">Head 注視とコミット</span><span class="sxs-lookup"><span data-stu-id="f876c-104">Head-gaze and commit</span></span>
<span data-ttu-id="f876c-105">Head 注視し、コミットが転送ポイント、head、(ヘッド-方向) の方向を持つオブジェクトを対象とする場合、入力モデルとなどを入力し、セカンダリに操作しているエア タップ手のジェスチャと音声コマンドの"Select"です。</span><span class="sxs-lookup"><span data-stu-id="f876c-105">Head-gaze and commit is an input model that involves targeting an object with the direction of your head pointing forward (head-direction), and then acting on it with a secondary input such as the hand gesture Air Tap or the voice command “Select”.</span></span> <span data-ttu-id="f876c-106">間接的な操作、つまり到達腕以外ではコンテンツと対話するための使用に適して使用してモデルを「まで」の入力と見なされます。</span><span class="sxs-lookup"><span data-stu-id="f876c-106">It is considered a "far" input model with indirect manipulation, meaning it is best used for interacting with content that is beyond arms reach.</span></span>

## <a name="device-support"></a><span data-ttu-id="f876c-107">デバイスのサポート</span><span class="sxs-lookup"><span data-stu-id="f876c-107">Device support</span></span>

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><span data-ttu-id="f876c-108"><strong>入力モデル</strong></span><span class="sxs-lookup"><span data-stu-id="f876c-108"><strong>Input model</strong></span></span></td>
        <td><span data-ttu-id="f876c-109"><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></span><span class="sxs-lookup"><span data-stu-id="f876c-109"><a href="hololens-hardware-details.md"><strong>HoloLens (1st gen)</strong></a></span></span></td>
        <td><span data-ttu-id="f876c-110"><strong>HoloLens 2</strong></span><span class="sxs-lookup"><span data-stu-id="f876c-110"><strong>HoloLens 2</strong></span></span></td>
        <td><span data-ttu-id="f876c-111"><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></span><span class="sxs-lookup"><span data-stu-id="f876c-111"><a href="immersive-headset-hardware-details.md"><strong>Immersive headsets</strong></a></span></span></td>
    </tr>
     <tr>
        <td><span data-ttu-id="f876c-112">Head 注視とコミット</span><span class="sxs-lookup"><span data-stu-id="f876c-112">Head-gaze and commit</span></span></td>
        <td><span data-ttu-id="f876c-113">推奨 ✔️</span><span class="sxs-lookup"><span data-stu-id="f876c-113">✔️ Recommended</span></span></td>
        <td><span data-ttu-id="f876c-114">✔️ お勧めします (3 番目の選択 -<a href="interaction-fundamentals.md">他のオプションを参照してください</a>)。</span><span class="sxs-lookup"><span data-stu-id="f876c-114">✔️ Recommended (third choice - <a href="interaction-fundamentals.md">See the other options</a>)</span></span></td>
        <td><span data-ttu-id="f876c-115">➕ 代替オプション</span><span class="sxs-lookup"><span data-stu-id="f876c-115">➕ Alternate option</span></span></td>
    </tr>
</table>

## <a name="head-gaze"></a><span data-ttu-id="f876c-116">Head 視線入力</span><span class="sxs-lookup"><span data-stu-id="f876c-116">Head-gaze</span></span>
<span data-ttu-id="f876c-117">Mixed reality ヘッドセットでは、位置とユーザーの頭の向きを使用して、そのヘッド方向ベクトルを決定します。</span><span class="sxs-lookup"><span data-stu-id="f876c-117">Mixed reality headsets use the position and orientation of the user's head to determine their head direction vector.</span></span> <span data-ttu-id="f876c-118">このユーザーの目の間で直接からまっすぐ指すレーザーと考えることができます。</span><span class="sxs-lookup"><span data-stu-id="f876c-118">You can think of this as a laser that points straight ahead from directly between the user's eyes.</span></span> <span data-ttu-id="f876c-119">これは非常に粗い近似のユーザーが検索されます。</span><span class="sxs-lookup"><span data-stu-id="f876c-119">This is a fairly coarse approximation of where the user is looking.</span></span> <span data-ttu-id="f876c-120">アプリケーションは、仮想または実際のオブジェクトをこの射線が交差して、ユーザーに知らせると、現在対象にするには、その場所にカーソルを描画します。</span><span class="sxs-lookup"><span data-stu-id="f876c-120">Your application can intersect this ray with virtual or real-world objects and draw a cursor at that location to let the user know what they are currently targeting.</span></span>

<span data-ttu-id="f876c-121">ヘッドの視線入力だけでなく、HoloLens 2 のようないくつかの複合現実ヘッドセットには、視線目視線入力ベクトルを生成するシステムにはが含まれます。</span><span class="sxs-lookup"><span data-stu-id="f876c-121">In addition to head gaze, some mixed reality headsets like the HoloLens 2 include eye tracking systems that produce an eye-gaze vector.</span></span> <span data-ttu-id="f876c-122">これは、ユーザーが検索しているにきめ細かい測定値を提供します。</span><span class="sxs-lookup"><span data-stu-id="f876c-122">This provides a fine-grained measurement of where the user is looking.</span></span> <span data-ttu-id="f876c-123">視線の先を作成し、目視線の先を使用して相互作用をコミットすることはできますが、これは、非常にさまざまなデザインの制約で個別に説明する一連の[視線記事](eye-tracking.md)します。</span><span class="sxs-lookup"><span data-stu-id="f876c-123">It is possible to build gaze and commit interactions using eye gaze, but this comes with a very different set of design constraints, which will be covered separately in the [eye tracking article](eye-tracking.md).</span></span>

## <a name="commit"></a><span data-ttu-id="f876c-124">コミット</span><span class="sxs-lookup"><span data-stu-id="f876c-124">Commit</span></span>
<span data-ttu-id="f876c-125">オブジェクトまたは UI 要素を対象とした後は、ユーザーは対話するか、セカンダリ入力を使用するのには、「クリックして」。</span><span class="sxs-lookup"><span data-stu-id="f876c-125">After targeting an object or UI element, the user can interact or "click" on it using a secondary input.</span></span> <span data-ttu-id="f876c-126">これは、モデルのコミットの手順と呼ばれます。</span><span class="sxs-lookup"><span data-stu-id="f876c-126">This is known as the commit step of the model.</span></span> <span data-ttu-id="f876c-127">次のコミット方法がサポートされています。</span><span class="sxs-lookup"><span data-stu-id="f876c-127">The following commit methods are supported:</span></span>

- <span data-ttu-id="f876c-128">エア タップのジェスチャ</span><span class="sxs-lookup"><span data-stu-id="f876c-128">Air Tap gesture</span></span>
- <span data-ttu-id="f876c-129">音声コマンドの"Select"、または対象となる音声コマンドのいずれかを読み上げる</span><span class="sxs-lookup"><span data-stu-id="f876c-129">Speak the voice command "Select" or one of the targeted voice commands</span></span>
- <span data-ttu-id="f876c-130">1 つのボタンを押して、 [HoloLens Clicker](hardware-accessories.md#hololens-clicker)</span><span class="sxs-lookup"><span data-stu-id="f876c-130">Press the single button on a [HoloLens Clicker](hardware-accessories.md#hololens-clicker)</span></span>
- <span data-ttu-id="f876c-131">Xbox ゲームパッドの 'A' ボタンを押します</span><span class="sxs-lookup"><span data-stu-id="f876c-131">Press the 'A' button on an Xbox Gamepad</span></span>
- <span data-ttu-id="f876c-132">Xbox アダプティブ コント ローラーの 'A' ボタンを押します</span><span class="sxs-lookup"><span data-stu-id="f876c-132">Press the 'A' button on an Xbox Adaptive Controller</span></span>

### <a name="head-gaze-and-air-tap-gesture"></a><span data-ttu-id="f876c-133">Head 注視しエア タップ ジェスチャ</span><span class="sxs-lookup"><span data-stu-id="f876c-133">Head-gaze and air tap gesture</span></span>
<span data-ttu-id="f876c-134">エア タップは、垂直に保持して手のアイコンをタップするジェスチャです。</span><span class="sxs-lookup"><span data-stu-id="f876c-134">Air tap is a tapping gesture with the hand held upright.</span></span> <span data-ttu-id="f876c-135">エア タップを実行するには、準備完了の位置に、人差し指を発生させるし、親指ピンチ、バックアップを解放する際、人差し指を発生させます。</span><span class="sxs-lookup"><span data-stu-id="f876c-135">To perform an Air tap, raise your index finger to the ready position, then pinch with your thumb and raise your index finger back up to release.</span></span> <span data-ttu-id="f876c-136">HoloLens 1 では、エア タップは、最も一般的なセカンダリ入力です。</span><span class="sxs-lookup"><span data-stu-id="f876c-136">On HoloLens 1, Air tap is the most common secondary input.</span></span>

![準備が位置し、タップまたはクリック モーション指](images/readyandpress.jpg)<br>

<span data-ttu-id="f876c-138">エア タップも HoloLens 2 では、使用し、元のバージョンで緩和されました。</span><span class="sxs-lookup"><span data-stu-id="f876c-138">Air tap is also available on HoloLens 2, and it has been relaxed from the original version.</span></span> <span data-ttu-id="f876c-139">Pinches のほぼすべての型はサポートされています、手が垂直化も。</span><span class="sxs-lookup"><span data-stu-id="f876c-139">Nearly all types of pinches are now supported, as long as the hand is upright and holding still.</span></span> <span data-ttu-id="f876c-140">これにより、ユーザーについて説明し、ジェスチャを実行するため、はるかに簡単です。</span><span class="sxs-lookup"><span data-stu-id="f876c-140">This makes it much easier for users to learn and perform the gesture.</span></span>  <span data-ttu-id="f876c-141">この新しいエア タップでは、既存のアプリケーションは HoloLens 2 の再コンパイルした後、新しい動作を自動的に取得されますので、同じ API を使って古いものが置き換えられます。</span><span class="sxs-lookup"><span data-stu-id="f876c-141">This new Air tap replaces the old one through the same API, so existing applications will get the new behavior automatically after recompiling for HoloLens 2.</span></span>

### <a name="head-gaze-and-select-voice-command"></a><span data-ttu-id="f876c-142">Head 注視し、"Select"音声指示コマンド</span><span class="sxs-lookup"><span data-stu-id="f876c-142">Head-gaze and "Select" voice command</span></span>
<span data-ttu-id="f876c-143">音声コマンドを実行すると、複合現実での主要な対話方法の 1 つです。</span><span class="sxs-lookup"><span data-stu-id="f876c-143">Voice commanding is one of the primary interaction methods on Mixed Reality.</span></span> <span data-ttu-id="f876c-144">システムを制御する場合は、非常に強力な「自在」メカニズムを提供します。</span><span class="sxs-lookup"><span data-stu-id="f876c-144">It provides a very powerful "Hands Free" mechanism to control the system.</span></span> <span data-ttu-id="f876c-145">音声の相互作用モデルの異なる種類があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-145">There are diferent types of voice interaction models:</span></span>

- <span data-ttu-id="f876c-146">「クリック」作動またはセカンダリの入力としてコミットを実行する"Select"を汎用のコマンドを使用します。</span><span class="sxs-lookup"><span data-stu-id="f876c-146">The generic command "Select" that allows to perform a "click" actuation or commit as a secondary input.</span></span>
- <span data-ttu-id="f876c-147">実行して、セカンダリ入力としてアクションへのコミットを許可する「閉じる」か"より大きな"などのオブジェクト コマンド。</span><span class="sxs-lookup"><span data-stu-id="f876c-147">Object commands like "Close" or "Make it bigger" that allow to perform and commit to an action as a secondary input.</span></span>
- <span data-ttu-id="f876c-148">ターゲットを必要としない「最初に移動」のようなグローバル commnads します。</span><span class="sxs-lookup"><span data-stu-id="f876c-148">Global commnads like "Go to start" that don't require a target.</span></span>
- <span data-ttu-id="f876c-149">メッセージ交換のユーザー インターフェイスまたはエンティティ、AI 自然言語の機能を搭載する Cortana のようにします。</span><span class="sxs-lookup"><span data-stu-id="f876c-149">Conversation user interfaces or entities like Cortana that have an AI Natural Language capability.</span></span>
- <span data-ttu-id="f876c-150">カスタム commnads</span><span class="sxs-lookup"><span data-stu-id="f876c-150">Custom commnads</span></span>

<span data-ttu-id="f876c-151">詳細と使用可能なコマンドと使用方法の comprenhesive 一覧を検索するには、チェック アウト、[音声コマンド実行](voice-design.md)ガイダンス。</span><span class="sxs-lookup"><span data-stu-id="f876c-151">To find more details and a comprenhesive list of available commands and how to use, check out our [voice commanding](voice-design.md) guidance.</span></span>


### <a name="head-gaze-and-hololens-clicker"></a><span data-ttu-id="f876c-152">Head 注視し、HoloLens Clicker</span><span class="sxs-lookup"><span data-stu-id="f876c-152">Head-gaze and HoloLens Clicker</span></span>
<span data-ttu-id="f876c-153">HoloLens Clicker HoloLens 専用に構築された最初の周辺機器は、HoloLens の 1 Development Edition に含まれています。</span><span class="sxs-lookup"><span data-stu-id="f876c-153">The HoloLens Clicker is the first peripheral device built specifically for HoloLens and is included with the HoloLens 1 Development Edition.</span></span> <span data-ttu-id="f876c-154">HoloLens Clicker を最小限の手の動きをクリックし、セカンダリ入力としてコミットできます。</span><span class="sxs-lookup"><span data-stu-id="f876c-154">The HoloLens Clicker allows a user to click with minimal hand motion and commit as a secondary input.</span></span> <span data-ttu-id="f876c-155">HoloLens clicker は、HoloLens 1 または 2 の Bluetooth 低エネルギー (BTLE) を使用してに接続します。</span><span class="sxs-lookup"><span data-stu-id="f876c-155">The HoloLens clicker connects to the HoloLens 1 or 2 using Bluetooth Low Energy (BTLE).</span></span>

![](images/hololens-clicker-500px.jpg)<br>
<span data-ttu-id="f876c-156">HoloLens Clicker</span><span class="sxs-lookup"><span data-stu-id="f876c-156">HoloLens Clicker</span></span>

<span data-ttu-id="f876c-157">詳細情報と、デバイスをペアリングする手順を参照して[ここ](hardware-accessories.md#pairing-bluetooth-accessories)</span><span class="sxs-lookup"><span data-stu-id="f876c-157">More information and instructions to pair the device can be found [here](hardware-accessories.md#pairing-bluetooth-accessories)</span></span>




### <a name="head-gaze-and-xbox-wireless-controller"></a><span data-ttu-id="f876c-158">Head 注視と Xbox のワイヤレス コント ローラー</span><span class="sxs-lookup"><span data-stu-id="f876c-158">Head-gaze and Xbox Wireless Controller</span></span>
<span data-ttu-id="f876c-159">Xbox のワイヤレス コント ローラーは、A ボタンを使用して入力セカンダリとして"click"作動を実行できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-159">The Xbox Wireless Controller allows to perform a "click" actuation as a secondary input by using the A button.</span></span> <span data-ttu-id="f876c-160">デバイスは、移動に役立つ操作および制御する、システムの既定のセットにマップされます。</span><span class="sxs-lookup"><span data-stu-id="f876c-160">The device is mapped to a default set of actions that help navigate and controll the system.</span></span> <span data-ttu-id="f876c-161">コント ローラーをカスタマイズする場合は、アクセサリの Xbox アプリを使用して、Xbox のワイヤレス コント ローラーを構成します。</span><span class="sxs-lookup"><span data-stu-id="f876c-161">If you want to customize the controller, use the Xbox Accesories App to configure your Xbox Wireless Controller.</span></span>

![](images/xboxcontroller.jpg)<br>
<span data-ttu-id="f876c-162">Xbox のワイヤレス コント ローラー</span><span class="sxs-lookup"><span data-stu-id="f876c-162">Xbox Wireless Controller</span></span>

[<span data-ttu-id="f876c-163">お使いの PC と Xbox コント ローラーのペアリング</span><span class="sxs-lookup"><span data-stu-id="f876c-163">Pairing an Xbox controller with your PC</span></span>](hardware-accessories.md#pairing-bluetooth-accessories)


### <a name="head-gaze-and-xbox-adaptive-controller"></a><span data-ttu-id="f876c-164">Head 視線入力、および Xbox のアダプティブ コント ローラー</span><span class="sxs-lookup"><span data-stu-id="f876c-164">Head-gaze and Xbox Adaptive Controller</span></span>
<span data-ttu-id="f876c-165">主に、モビリティを限られたプレイヤーがゲームのニーズを満たすように設計、Xbox アダプティブ コント ローラーは、Mixed Reality をよりアクセスできるようにする統合されたハブのデバイスです。</span><span class="sxs-lookup"><span data-stu-id="f876c-165">Designed primarily to meet the needs of gamers with limited mobility, the Xbox Adaptive Controller is a unified hub for devices that helps make Mixed Reality more accessible.</span></span>

<span data-ttu-id="f876c-166">アダプティブ Xbox コント ローラーは、A ボタンを使用して入力セカンダリとして"click"作動を実行できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-166">The Xbox Adaptive Controller allows to perform a "click" actuation as a secondary input by using the A button.</span></span> <span data-ttu-id="f876c-167">デバイスは、移動に役立つ操作および制御する、システムの既定のセットにマップされます。</span><span class="sxs-lookup"><span data-stu-id="f876c-167">The device is mapped to a default set of actions that help navigate and controll the system.</span></span> <span data-ttu-id="f876c-168">コント ローラーをカスタマイズする場合は、アクセサリの Xbox アプリを使用して、Xbox アダプティブ コント ローラーを構成します。</span><span class="sxs-lookup"><span data-stu-id="f876c-168">If you want to customize the controller, use the Xbox Accesories App to configure your Xbox Adaptive Controller.</span></span>

![](images/xbox-adaptive-controller-devices.jpg)<br>
<span data-ttu-id="f876c-169">Xbox アダプティブ コント ローラー</span><span class="sxs-lookup"><span data-stu-id="f876c-169">Xbox Adaptive Controller</span></span>

<span data-ttu-id="f876c-170">スイッチ、ボタン、マウント、ジョイスティックは自分で一意にするカスタム コント ローラーのエクスペリエンスを作成するなどの外部のデバイスを接続します。</span><span class="sxs-lookup"><span data-stu-id="f876c-170">Connect external devices such as switches, buttons, mounts, and joysticks to create a custom controllers experience that is uniquely yours.</span></span> <span data-ttu-id="f876c-171">3.5 mm ジャック、および USB ポート経由で接続して補助デバイスでは、ボタン、スティックおよびトリガーの入力が制御されます。</span><span class="sxs-lookup"><span data-stu-id="f876c-171">Button, thumbstick and trigger inputs are controlled with assistive devices connected through 3.5mm jacks and USB ports.</span></span>

![](images/xbox-adaptive-controller-ports.jpg)<br>
<span data-ttu-id="f876c-172">Xbox アダプティブ コント ローラーのポート</span><span class="sxs-lookup"><span data-stu-id="f876c-172">Xbox Adaptive Controller ports</span></span>

[<span data-ttu-id="f876c-173">手順については、デバイスをペアリングするには</span><span class="sxs-lookup"><span data-stu-id="f876c-173">Instructions to pair the device</span></span>](hardware-accessories.md#pairing-bluetooth-accessories)

<span data-ttu-id="f876c-174"><a href=https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller>詳細については使用可能な Xbox サイト</a></span><span class="sxs-lookup"><span data-stu-id="f876c-174"><a href=https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller>More info available on the Xbox site</a></span></span>


# <a name="head-gaze-design-guidelines"></a><span data-ttu-id="f876c-175">Head 注視デザイン ガイドライン</span><span class="sxs-lookup"><span data-stu-id="f876c-175">Head-gaze design guidelines</span></span>
> [!NOTE]
> <span data-ttu-id="f876c-176">視線の設計に固有のガイダンスについて[近日](index.md)します。</span><span class="sxs-lookup"><span data-stu-id="f876c-176">More guidance specific to gaze design [coming soon](index.md).</span></span>

## <a name="head-gaze-targeting"></a><span data-ttu-id="f876c-177">ヘッド-視線の先の対象とします。</span><span class="sxs-lookup"><span data-stu-id="f876c-177">Head-gaze targeting</span></span>
<span data-ttu-id="f876c-178">すべての対話は、ユーザーの入力モードに関係なく、対話する要素を対象とする機能に基づいて構築します。</span><span class="sxs-lookup"><span data-stu-id="f876c-178">All interactions are built upon the ability of a user to target the element they want to interact with, regardless of the input modality.</span></span> <span data-ttu-id="f876c-179">Windows Mixed Reality でこれは、一般に、ユーザーの視線の先を使用します。</span><span class="sxs-lookup"><span data-stu-id="f876c-179">In Windows Mixed Reality, this is generally done using the user's gaze.</span></span>
<span data-ttu-id="f876c-180">エクスペリエンスを正常に使用するユーザーを有効にするには、ユーザーの目的、およびユーザーの実際のインテントのシステムの計算については、できる限り忠実に従う必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-180">To enable a user to work with an experience successfully, the system's calculated understanding of a user's intent, and the user's actual intent, must align as closely as possible.</span></span> <span data-ttu-id="f876c-181">程度にシステムがユーザーの意図した操作を解釈する正しく、満足度が増えるとパフォーマンスが向上します。</span><span class="sxs-lookup"><span data-stu-id="f876c-181">To the degree that the system interprets the user's intended actions correctly, satisfaction increases and performance improves.</span></span>


## <a name="target-sizing-and-feedback"></a><span data-ttu-id="f876c-182">ターゲットのサイズ変更とフィードバック</span><span class="sxs-lookup"><span data-stu-id="f876c-182">Target sizing and feedback</span></span>
<span data-ttu-id="f876c-183">視線入力ベクトルは正しく対象とするために使用するのには繰り返し示されていますが、gross ターゲット (取得若干大きくターゲット) に最も適した多くの場合。</span><span class="sxs-lookup"><span data-stu-id="f876c-183">The gaze vector has been shown repeatedly to be usable for fine targeting, but often works best for gross targeting (acquiring somewhat larger targets).</span></span> <span data-ttu-id="f876c-184">1.5 に 1 度の最小のターゲットのサイズは、3 度のターゲットが多くの場合より高速化を使用する場合、ほとんどのシナリオで成功したユーザーの操作を許可する必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-184">Minimum target sizes of 1 to 1.5 degrees should allow successful user actions in most scenarios, though targets of 3 degrees often allow for greater speed.</span></span> <span data-ttu-id="f876c-185">注いる対象ユーザーは 3D 要素の場合でも 2D 領域では効果的にサイズどのプロジェクションが直面する、ターゲット設定可能な領域があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-185">Note that the size that the user targets is effectively a 2D area even for 3D elements--whichever projection is facing them should be the targetable area.</span></span> <span data-ttu-id="f876c-186">要素が「アクティブ」(こと、ユーザーが対象にすること) があるいくつかの注目すべきキューが非常に役立つ - これを含めることができますを提供する処理などの表示「ホバー」効果やオーディオのハイライト、 をクリックまたは要素を使用してカーソルの配置をオフにします。</span><span class="sxs-lookup"><span data-stu-id="f876c-186">Providing some salient cue that an element is "active" (that the user is targeting it) is extremely helpful - this can include treatments like visible "hover" effects, audio highlights or clicks, or clear alignment of a cursor with an element.</span></span>

<span data-ttu-id="f876c-187">![2 つのメーター距離にある最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="f876c-187">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="f876c-188">*2 つのメーター距離にある最適なターゲット サイズ*</span><span class="sxs-lookup"><span data-stu-id="f876c-188">*Optimal target size at 2 meter distance*</span></span>

<span data-ttu-id="f876c-189">![視線の先の対象となるオブジェクトを強調表示の例](images/gazetargeting-highlighting-640px.jpg)</span><span class="sxs-lookup"><span data-stu-id="f876c-189">![An example of highlighting a gaze targeted object](images/gazetargeting-highlighting-640px.jpg)</span></span><br>
<span data-ttu-id="f876c-190">*視線の先の対象となるオブジェクトを強調表示の例*</span><span class="sxs-lookup"><span data-stu-id="f876c-190">*An example of highlighting a gaze targeted object*</span></span>

## <a name="target-placement"></a><span data-ttu-id="f876c-191">ターゲットの配置</span><span class="sxs-lookup"><span data-stu-id="f876c-191">Target placement</span></span>
<span data-ttu-id="f876c-192">ユーザーは、多くの場合、(通常は約監視レベル) にフォーカスがメインの周囲の領域で、注意のほとんどに重点を置いて、視野で非常に高または非常に低配置されている UI 要素を検索する失敗します。</span><span class="sxs-lookup"><span data-stu-id="f876c-192">Users will often fail to find UI elements that are positioned very high or very low in their field of view, focusing most of their attention on areas around their main focus (usually roughly eye level).</span></span> <span data-ttu-id="f876c-193">目のレベルによって適切なバンドでほとんどのターゲットを配置することができます。</span><span class="sxs-lookup"><span data-stu-id="f876c-193">Placing most targets in some reasonable band around eye level can help.</span></span> <span data-ttu-id="f876c-194">ユーザーの傾向を与え、比較的小さな視覚的な領域 (ビジョンの attentional コーンは 10 度では約) いつでも、概念的に関連している度合いをまとめて UI 要素をグループに重点を活用できますから注意順行連鎖の動作項目のユーザーとして領域を通じて、視線の先に移動します。</span><span class="sxs-lookup"><span data-stu-id="f876c-194">Given the tendency for users to focus on a relatively small visual area at any time (the attentional cone of vision is roughly 10 degrees), grouping UI elements together to the degree that they're related conceptually can leverage attention-chaining behaviors from item to item as a user moves their gaze through an area.</span></span> <span data-ttu-id="f876c-195">UI を設計するときは、HoloLens、イマーシブ ヘッドセットとビューのフィールドに潜在的な大規模なバリエーションに注意してください。</span><span class="sxs-lookup"><span data-stu-id="f876c-195">When designing UI, keep in mind the potential large variation in field of view between HoloLens and immersive headsets.</span></span>

<span data-ttu-id="f876c-196">![Galaxy エクスプ ローラーで対象とする簡単視線の先のグループ化された UI 要素の例](images/gazetargeting-grouping-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="f876c-196">![An example of grouped UI elements for easier gaze targeting in Galaxy Explorer](images/gazetargeting-grouping-1000px.jpg)</span></span><br>
<span data-ttu-id="f876c-197">*Galaxy エクスプ ローラーで対象とする簡単視線の先のグループ化された UI 要素の例*</span><span class="sxs-lookup"><span data-stu-id="f876c-197">*An example of grouped UI elements for easier gaze targeting in Galaxy Explorer*</span></span>

## <a name="improving-targeting-behaviors"></a><span data-ttu-id="f876c-198">ターゲットの動作の向上</span><span class="sxs-lookup"><span data-stu-id="f876c-198">Improving targeting behaviors</span></span>
<span data-ttu-id="f876c-199">何かを対象とするユーザーの意図を確認 (または密接に近似)、「ニアミス」が正しくを対象としていた場合と同様の対話に試行を受け入れるように非常に便利ですができます。</span><span class="sxs-lookup"><span data-stu-id="f876c-199">If user intent to target something can be determined (or approximated closely), it can be very helpful to accept "near miss" attempts at interaction as though they were targeted correctly.</span></span> <span data-ttu-id="f876c-200">複合現実エクスペリエンスに組み込むことが成功したメソッドのいくつかあります。</span><span class="sxs-lookup"><span data-stu-id="f876c-200">There are a handful of successful methods that can be incorporated in mixed reality experiences:</span></span>

### <a name="head-gaze-stabilization-gravity-wells"></a><span data-ttu-id="f876c-201">Head 注視安定化 ("重力 wells")</span><span class="sxs-lookup"><span data-stu-id="f876c-201">Head-gaze stabilization ("gravity wells")</span></span>
<span data-ttu-id="f876c-202">これが有効にする時間のほとんどまたはすべて。</span><span class="sxs-lookup"><span data-stu-id="f876c-202">This should be turned on most/all of the time.</span></span> <span data-ttu-id="f876c-203">この手法は、自然な head/ネック ジッター可能性のあるユーザーを削除します。</span><span class="sxs-lookup"><span data-stu-id="f876c-203">This technique removes the natural head/neck jitters that users may have.</span></span> <span data-ttu-id="f876c-204">検索読み上げの動作が原因も移動します。</span><span class="sxs-lookup"><span data-stu-id="f876c-204">Also movement due to looking/speaking behaviors.</span></span>

### <a name="closest-link-algorithms"></a><span data-ttu-id="f876c-205">最も近いリンク アルゴリズム</span><span class="sxs-lookup"><span data-stu-id="f876c-205">Closest link algorithms</span></span>
<span data-ttu-id="f876c-206">これらは、スパースの対話型コンテンツ領域に最適な機能します。</span><span class="sxs-lookup"><span data-stu-id="f876c-206">These work best in areas with sparse interactive content.</span></span> <span data-ttu-id="f876c-207">対話する試行がユーザーを決定することができます可能性が高い場合は、単に一定レベルのインテントを想定して、ターゲットの能力を補うことができます。</span><span class="sxs-lookup"><span data-stu-id="f876c-207">If there is a high probability that you can determine what a user was attempting to interact with, you can supplement their targeting abilities by simply assuming some level of intent.</span></span>

### <a name="backdatingpostdating-actions"></a><span data-ttu-id="f876c-208">Backdating 遅延アクション</span><span class="sxs-lookup"><span data-stu-id="f876c-208">Backdating/postdating actions</span></span>
<span data-ttu-id="f876c-209">このメカニズムは、速度が必要なタスクに役立ちます。</span><span class="sxs-lookup"><span data-stu-id="f876c-209">This mechanism is useful in tasks requiring speed.</span></span> <span data-ttu-id="f876c-210">ユーザーは、一連の対象とする、アクティブ化の最適経路の速度で移動する場合に役に立ちますいくつかの目的を想定し、ユーザーがいたフォーカスで若干の前後に若干 (50 ミリ秒の前に、/後が有効であったでタップのターゲットに対して操作を実行する手順を実行されなかったを許可します。初期のテスト)。</span><span class="sxs-lookup"><span data-stu-id="f876c-210">When a user is moving through a series of targeting/activation maneuvers at speed, it can be useful to assume some intent and allow missed steps to act upon targets which the user had in focus slightly before or slightly after the tap (50ms before/after was effective in early testing).</span></span>

### <a name="smoothing"></a><span data-ttu-id="f876c-211">スムージング</span><span class="sxs-lookup"><span data-stu-id="f876c-211">Smoothing</span></span>
<span data-ttu-id="f876c-212">このメカニズムは、パスの移動、ヘッドの移動を自然な特性のため若干のジッター/補助の削減に役立ちます。</span><span class="sxs-lookup"><span data-stu-id="f876c-212">This mechanism is useful for pathing movements, reducing the slight jitter/wobble due to natural head movement characteristics.</span></span> <span data-ttu-id="f876c-213">パスのモーション、時間の経過と共にではなく、動きの距離のサイズでスムーズに円滑化するとき</span><span class="sxs-lookup"><span data-stu-id="f876c-213">When smoothing over pathing motions, smooth by size/distance of movements rather than over time</span></span>

### <a name="magnetism"></a><span data-ttu-id="f876c-214">磁力</span><span class="sxs-lookup"><span data-stu-id="f876c-214">Magnetism</span></span>
<span data-ttu-id="f876c-215">このメカニズムはターゲットに向かって、カーソルを描画するか、単に (視覚的またはない) かどうかに hitboxes を増やす「リンクでは最も近い」アルゴリズムの一般的なバージョンとして考えることができます可能性の高い目標を実現するユーザーに、使用する対話型のレイアウトの知識優れたアプローチ ユーザーのものです。</span><span class="sxs-lookup"><span data-stu-id="f876c-215">This mechanism can be thought of as a more general version of "Closest link" algorithms - drawing a cursor toward a target, or simply increasing hitboxes (whether visibly or not) as users approach likely targets, using some knowledge of the interactive layout to better approach user intent.</span></span> <span data-ttu-id="f876c-216">これは、小規模のターゲットの非常に強力なことができます。</span><span class="sxs-lookup"><span data-stu-id="f876c-216">This can be particularly powerful for small targets.</span></span>

### <a name="focus-stickiness"></a><span data-ttu-id="f876c-217">フォーカスの持続性</span><span class="sxs-lookup"><span data-stu-id="f876c-217">Focus stickiness</span></span>
<span data-ttu-id="f876c-218">フォーカスを移す対話型の要素の近くに判断する、現在フォーカスが要素に、バイアスを提供します。</span><span class="sxs-lookup"><span data-stu-id="f876c-218">When determining which nearby interactive elements to give focus to, provide a bias to the element that is currently focused.</span></span> <span data-ttu-id="f876c-219">これは自然なノイズを含む 2 つの要素間の中間に浮動小数点の動作の切り替えが不規則のフォーカスの削減に役立ちます。</span><span class="sxs-lookup"><span data-stu-id="f876c-219">This will help reduce erratic focus switching behaviours when floating at a midpoint between two elements with natural noise.</span></span>


## <a name="composite-gestures"></a><span data-ttu-id="f876c-220">複合のジェスチャ</span><span class="sxs-lookup"><span data-stu-id="f876c-220">Composite gestures</span></span>
<span data-ttu-id="f876c-221">アプリでは、複数の個々 のタップを認識できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-221">Apps can recognize more than just individual taps.</span></span> <span data-ttu-id="f876c-222">結合をタップして保持して、リリース、手の動きより複雑な複合ジェスチャを実行できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-222">By combining tap, hold and release with the movement of the hand, more complex composite gestures can be performed.</span></span> <span data-ttu-id="f876c-223">これらの複合または高レベルのジェスチャは、低レベル空間からの入力データ (エア タップと Bloom) 開発者がアクセスできるに作成します。</span><span class="sxs-lookup"><span data-stu-id="f876c-223">These composite or high-level gestures build on the low-level spatial input data (from Air tap and Bloom) that developers have access to.</span></span>

### <a name="air-tap"></a><span data-ttu-id="f876c-224">エア タップ</span><span class="sxs-lookup"><span data-stu-id="f876c-224">Air tap</span></span>
<span data-ttu-id="f876c-225">エア タップのジェスチャ (およびその他のジェスチャは以下) は、特定のタップにのみ対応します。</span><span class="sxs-lookup"><span data-stu-id="f876c-225">The Air tap gesture (as well as the other gestures below) reacts only to a specific tap.</span></span> <span data-ttu-id="f876c-226">メニューや把握などの他のタップを検出するために、アプリは、上記 2 つの重要なコンポーネントのジェスチャのセクションで説明されている下位レベルの相互作用を直接使用する必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-226">To detect other taps, such as Menu or Grasp, your app must directly use the lower-level interactions described in two key component gestures section above.</span></span>

### <a name="tap-and-hold"></a><span data-ttu-id="f876c-227">タップ アンド ホールド</span><span class="sxs-lookup"><span data-stu-id="f876c-227">Tap and hold</span></span>
<span data-ttu-id="f876c-228">保留中のエア タップの下向きの指の位置だけ維持です。</span><span class="sxs-lookup"><span data-stu-id="f876c-228">Hold is simply maintaining the downward finger position of the air tap.</span></span> <span data-ttu-id="f876c-229">エア タップ アンド ホールドの組み合わせは、さまざまなより複雑な「クリックしてドラッグ」の対話など、アクティブ化することではなくオブジェクトのピックアップ arm 移動と組み合わせたときに、またはコンテキスト メニューを表示するなどの"mousedown"セカンダリ対話できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-229">The combination of air tap and hold allows for a variety of more complex "click and drag" interactions when combined with arm movement such as picking up an object instead of activating it or "mousedown" secondary interactions such as showing a context menu.</span></span>
<span data-ttu-id="f876c-230">任意拡張のジェスチャの実行中に、手の形の姿勢を緩和しやすいユーザーとしてただし、このジェスチャの設計にできる場合に、注意を使用してください。</span><span class="sxs-lookup"><span data-stu-id="f876c-230">Caution should be used when designing for this gesture however, as users can be prone to relaxing their hand postures during the course of any extended gesture.</span></span>

### <a name="manipulation"></a><span data-ttu-id="f876c-231">操作</span><span class="sxs-lookup"><span data-stu-id="f876c-231">Manipulation</span></span>
<span data-ttu-id="f876c-232">移動、サイズを変更する場合、1:1 ユーザーの手の動きに反応するホログラム ホログラムを回転操作のジェスチャを使用できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-232">Manipulation gestures can be used to move, resize or rotate a hologram when you want the hologram to react 1:1 to the user's hand movements.</span></span> <span data-ttu-id="f876c-233">このような 1 対 1 の動きの 1 つの用途は、描画またはペイントの世界では、ユーザーにです。</span><span class="sxs-lookup"><span data-stu-id="f876c-233">One use for such 1:1 movements is to let the user draw or paint in the world.</span></span>
<span data-ttu-id="f876c-234">操作のジェスチャの初期ターゲットは、視線入力または参照によって行う必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-234">The initial targeting for a manipulation gesture should be done by gaze or pointing.</span></span> <span data-ttu-id="f876c-235">タップ アンド ホールドの開始、オブジェクトのすべての操作が、処理を手動で移動すると、操作中にユーザーを解放します。</span><span class="sxs-lookup"><span data-stu-id="f876c-235">Once the tap and hold starts, any manipulation of the object is then handled by hand movements, freeing the user to look around while they manipulate.</span></span>

### <a name="navigation"></a><span data-ttu-id="f876c-236">ナビゲーション</span><span class="sxs-lookup"><span data-stu-id="f876c-236">Navigation</span></span>
<span data-ttu-id="f876c-237">ナビゲーションのジェスチャでは、仮想ジョイスティックのように動作し、放射状メニューなどの UI ウィジェットを移動に使用できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-237">Navigation gestures operate like a virtual joystick, and can be used to navigate UI widgets, such as radial menus.</span></span> <span data-ttu-id="f876c-238">タップし、ジェスチャが開始され、手を中心に最初のキーを押して、正規化された 3D キューブ内に保持します。</span><span class="sxs-lookup"><span data-stu-id="f876c-238">You tap and hold to start the gesture and then move your hand within a normalized 3D cube, centered around the initial press.</span></span> <span data-ttu-id="f876c-239">0 の開始点の 1 に、値-1 から X、Y、Z 軸に沿った手を移動できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-239">You can move your hand along the X, Y or Z axis from a value of -1 to 1, with 0 being the starting point.</span></span>
<span data-ttu-id="f876c-240">ナビゲーションは、velocity ベースの継続的なスクロールまたはズーム ジェスチャ、マウスの中央ボタンをクリックし、マウスを上下に移動して 2D の UI をスクロールすると同様の作成に使用できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-240">Navigation can be used to build velocity-based continuous scrolling or zooming gestures, similar to scrolling a 2D UI by clicking the middle mouse button and then moving the mouse up and down.</span></span>

<span data-ttu-id="f876c-241">レールを使用したナビゲーションは、その軸で特定のしきい値に達するまでは、特定の軸での動きを認識する機能を指します。</span><span class="sxs-lookup"><span data-stu-id="f876c-241">Navigation with rails refers to the ability of recognizing movements in certain axis until certain threshold is reached on that axis.</span></span> <span data-ttu-id="f876c-242">1 つ以上の軸での移動が有効な場合、アプリケーションで、開発者など、便利ですが、これはのみ、アプリケーションが X、Y 軸の間でナビゲーション ジェスチャを認識するように構成が指定されても場合 rails 軸 X。</span><span class="sxs-lookup"><span data-stu-id="f876c-242">This is only useful, when movement in more than one axis is enabled in an application by the developer, e.g. if an application is configured to recognize navigation gestures across X, Y axis but also specified X axis with rails.</span></span> <span data-ttu-id="f876c-243">ここでシステムが認識手の動きで X 軸の X 軸の rails (ガイド) が虚数部内にある限り、手の形の移動には、Y 軸もが発生した場合。</span><span class="sxs-lookup"><span data-stu-id="f876c-243">In this case system will recognize hand movements across X axis as long as they remain within an imaginary rails (guide) on X axis, if hand movement also occurs Y axis.</span></span>

<span data-ttu-id="f876c-244">2D のアプリ内でユーザーは、スクロール、ズーム、またはアプリ内でドラッグ縦型ナビゲーション ジェスチャを使用できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-244">Within 2D apps, users can use vertical navigation gestures to scroll, zoom, or drag inside the app.</span></span> <span data-ttu-id="f876c-245">これは、同じ種類のタッチ ジェスチャをシミュレートするためにアプリを仮想指タッチを挿入します。</span><span class="sxs-lookup"><span data-stu-id="f876c-245">This injects virtual finger touches to the app to simulate touch gestures of the same type.</span></span> <span data-ttu-id="f876c-246">ユーザーは、ボタンを選択するか、'< スクロール/ドラッグ/ズーム > Tool' を言ってのいずれかのアプリ上にあるバー上のツール間で切り替えることで行われますがこれらのアクションを選択できます。</span><span class="sxs-lookup"><span data-stu-id="f876c-246">Users can select which of these actions take place by toggling between the tools on the bar above the app, either by selecting the button or saying '<Scroll/Drag/Zoom> Tool'.</span></span>

[<span data-ttu-id="f876c-247">詳細については、複合のジェスチャ</span><span class="sxs-lookup"><span data-stu-id="f876c-247">More info on composite gestures</span></span>](gestures.md#composite-gestures)

## <a name="gesture-recognizers"></a><span data-ttu-id="f876c-248">ジェスチャ レコグナイザー</span><span class="sxs-lookup"><span data-stu-id="f876c-248">Gesture recognizers</span></span>

<span data-ttu-id="f876c-249">ジェスチャ認識を使用して 1 つの利点は、現在のターゲット ホログラムが受け入れることができるジェスチャのジェスチャ認識エンジンを構成することができます。</span><span class="sxs-lookup"><span data-stu-id="f876c-249">One benefit of using gesture recognition is that you can configure a gesture recognizer just for the gestures the currently targeted hologram can accept.</span></span> <span data-ttu-id="f876c-250">プラットフォームでは、これらの特定のサポートされているジェスチャを区別するために必要な曖昧のみを行います。</span><span class="sxs-lookup"><span data-stu-id="f876c-250">The platform will do only the disambiguation necessary to distinguish those particular supported gestures.</span></span> <span data-ttu-id="f876c-251">そうすることだけエア タップをサポートするホログラムは任意の長さのキーを押してからリリースまでの時間を受け入れることができます、ホログラム中をサポートしています をタップし、の両方を保持できます後を昇格させる保留をタップして、ホールド時間のしきい値。</span><span class="sxs-lookup"><span data-stu-id="f876c-251">That way, a hologram that just supports air tap can accept any length of time between press and release, while a hologram that supports both tap and hold can promote the tap to a hold after the hold time threshold.</span></span>

## <a name="hand-recognition"></a><span data-ttu-id="f876c-252">手の形の認識</span><span class="sxs-lookup"><span data-stu-id="f876c-252">Hand recognition</span></span>
<span data-ttu-id="f876c-253">HoloLens では、デバイスに表示されているいずれかまたは両方のハンドの位置を追跡して手のジェスチャを認識します。</span><span class="sxs-lookup"><span data-stu-id="f876c-253">HoloLens recognizes hand gestures by tracking the position of either or both hands that are visible to the device.</span></span> <span data-ttu-id="f876c-254">HoloLens では、(背面人差し指をに向けてして手のアイコン) の準備完了状態または押された状態 (背面インデックス指でに向けてして手のアイコン) のいずれかにいるときに手が表示されます。</span><span class="sxs-lookup"><span data-stu-id="f876c-254">HoloLens sees hands when they are in either the ready state (back of the hand facing you with index finger up) or the pressed state (back of the hand facing you with the index finger down).</span></span> <span data-ttu-id="f876c-255">手が他が発生する場合、それら、HoloLens は無視されます。</span><span class="sxs-lookup"><span data-stu-id="f876c-255">When hands are in other poses, the HoloLens will ignore them.</span></span>
<span data-ttu-id="f876c-256">各ハンドの HoloLens を検出すると、(向き) なしの位置と、押された状態にアクセスすることができます。</span><span class="sxs-lookup"><span data-stu-id="f876c-256">For each hand that HoloLens detects, you can access its position (without orientation) and its pressed state.</span></span> <span data-ttu-id="f876c-257">して手のアイコンは、ジェスチャのフレームの端に近いが再び HoloLens が表示できる場所を取得するには、その手を移動する方法を把握できるように、ユーザーに表示できる方向ベクトルも与えられます。</span><span class="sxs-lookup"><span data-stu-id="f876c-257">As the hand nears the edge of the gesture frame, you're also provided with a direction vector, which you can show to the user so they know how to move their hand to get it back where HoloLens can see it.</span></span>

## <a name="gesture-frame"></a><span data-ttu-id="f876c-258">ジェスチャ フレーム</span><span class="sxs-lookup"><span data-stu-id="f876c-258">Gesture frame</span></span>
<span data-ttu-id="f876c-259">HoloLens のジェスチャは、フレーム内で"ジェスチャ"、(非常に大まかにウェストと肩の間での鼻) からジェスチャ検知カメラが適切に参照できる範囲内でして手のアイコン必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-259">For gestures on HoloLens, the hand must be within a “gesture frame”, in a range that the gesture-sensing cameras can see appropriately (very roughly from nose to waist, and between the shoulders).</span></span> <span data-ttu-id="f876c-260">ユーザーは、認識操作の成功と独自の快適性の両方が (多くのユーザーが最初と仮定ジェスチャ フレームが HoloLens、これにより、ビュー内であるし、対話するには多く自分の腕を保持する必要があります) のこの領域でトレーニングを受ける必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-260">Users need to be trained on this area of recognition both for success of action and for their own comfort (many users will initially assume that the gesture frame must be within their view through HoloLens, and hold their arms up uncomfortably in order to interact).</span></span> <span data-ttu-id="f876c-261">HoloLens Clicker を使用する場合は、手をジェスチャ フレーム内である必要はありません。</span><span class="sxs-lookup"><span data-stu-id="f876c-261">When using the HoloLens Clicker, your hands do not need to be within the gesture frame.</span></span>

<span data-ttu-id="f876c-262">継続的なジェスチャの場合具体的がユーザーの中には、(中に何らかの holographic オブジェクトを移動するなど) の中間のジェスチャ、ジェスチャのフレームの外部で手に移動し、目的の結果を失うことのリスクです。</span><span class="sxs-lookup"><span data-stu-id="f876c-262">In the case of continuous gestures in particular, there is some risk of users moving their hands outside of the gesture frame while in mid-gesture (while moving some holographic object, for example), and losing their intended outcome.</span></span>

<span data-ttu-id="f876c-263">考慮すべき 3 つのことがあります。</span><span class="sxs-lookup"><span data-stu-id="f876c-263">There are three things that you should consider:</span></span>

- <span data-ttu-id="f876c-264">ジェスチャのフレームの存在とおおよその境界 (これは、HoloLens のセットアップ中については) でユーザー教育します。</span><span class="sxs-lookup"><span data-stu-id="f876c-264">User education on the gesture frame's existence and approximate boundaries (this is taught during HoloLens setup).</span></span>

- <span data-ttu-id="f876c-265">ときに、ジェスチャが近づいて/重大な程度失わジェスチャが望ましくない結果に至るまで、アプリケーション内でジェスチャ フレーム境界のユーザーに通知します。</span><span class="sxs-lookup"><span data-stu-id="f876c-265">Notifying users when their gestures are nearing/breaking the gesture frame boundaries within an application, to the degree that a lost gesture will lead to undesired outcomes.</span></span> <span data-ttu-id="f876c-266">調査によれば、このような通知システムのキーの品質を評価し、HoloLens のシェルがこの (ビジュアルでは、どの境界の交差が行われて方向を示す、中央のカーソルで) 通知の種類の良い例を提供します。</span><span class="sxs-lookup"><span data-stu-id="f876c-266">Research has shown the key qualities of such a notification system, and the HoloLens shell provides a good example of this type of notification (visual, on the central cursor, indicating the direction in which boundary crossing is taking place).</span></span>

- <span data-ttu-id="f876c-267">ジェスチャのフレームの境界線の重大な影響を最小限に抑える必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-267">Consequences of breaking the gesture frame boundaries should be minimized.</span></span> <span data-ttu-id="f876c-268">一般はジェスチャの結果が、境界で停止していることが逆になっていないを意味します。</span><span class="sxs-lookup"><span data-stu-id="f876c-268">In general, this means that the outcome of a gesture should be stopped at the boundary, but not reversed.</span></span> <span data-ttu-id="f876c-269">たとえば場合は、ユーザーが、部屋の holographic によってオブジェクトを移動、移動はジェスチャ フレームは、違反が発生したが、開始点は返されませんを停止する必要があります。</span><span class="sxs-lookup"><span data-stu-id="f876c-269">For example, if a user is moving some holographic object across a room, movement should stop when the gesture frame is breached, but not be returned to the starting point.</span></span> <span data-ttu-id="f876c-270">ユーザーは、可能性がありますし、いくつかのストレスが発生するが可能性があります、境界がより迅速に理解していなくてたびに完全な意図した操作を再起動します。</span><span class="sxs-lookup"><span data-stu-id="f876c-270">The user may experience some frustration then, but may more quickly understand the boundaries, and not have to restart their full intended actions each time.</span></span>


## <a name="see-also"></a><span data-ttu-id="f876c-271">関連項目</span><span class="sxs-lookup"><span data-stu-id="f876c-271">See also</span></span>
* [<span data-ttu-id="f876c-272">手で直接操作</span><span class="sxs-lookup"><span data-stu-id="f876c-272">Direct manipulation with hands</span></span>](direct-manipulation.md)
* [<span data-ttu-id="f876c-273">手を使ったポイントとコミット</span><span class="sxs-lookup"><span data-stu-id="f876c-273">Point and commit with hands</span></span>](point-and-commit.md)
* [<span data-ttu-id="f876c-274">本能的な操作</span><span class="sxs-lookup"><span data-stu-id="f876c-274">Instinctual interactions</span></span>](interaction-fundamentals.md)
* [<span data-ttu-id="f876c-275">ヘッド視線入力とドウェル</span><span class="sxs-lookup"><span data-stu-id="f876c-275">Head-gaze and dwell</span></span>](gaze-and-dwell.md)
* [<span data-ttu-id="f876c-276">音声コマンド</span><span class="sxs-lookup"><span data-stu-id="f876c-276">Voice commanding</span></span>](voice-design.md)





