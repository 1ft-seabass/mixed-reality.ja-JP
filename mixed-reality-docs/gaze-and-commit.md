---
title: 頭の視線入力とコミット
description: 頭の視線入力とコミットの入力モデルの概要
author: caseymeekhof
ms.author: cmeekhof
ms.date: 03/31/2019
ms.topic: article
keywords: Mixed Reality, 視線入力, 視線入力ターゲット設定, 対話, 設計
ms.openlocfilehash: aeca5ceacf5ae350aa06cb58cc68162f885f6d78
ms.sourcegitcommit: b0b1b8e1182cce93929d409706cdaa99ff24fdee
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 07/23/2019
ms.locfileid: "68387682"
---
# <a name="head-gaze-and-commit"></a>頭の視線入力とコミット
ヘッドを見つめてコミットすると、入力モデルになります。このモデルでは、前方 (ヘッド方向) の方向にあるオブジェクトをターゲットにし、ハンドジェスチャのエアタップや音声コマンドの選択などの2番目の入力で操作します。 これは間接的な操作を伴う外部入力モデルと見なされます。つまり、腕を超えるコンテンツとの対話に最適です。

## <a name="device-support"></a>デバイスのサポート

<table>
    <colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    </colgroup>
    <tr>
        <td><strong>入力モデル</strong></td>
        <td><a href="hololens-hardware-details.md"><strong>HoloLens (第 1 世代)</strong></a></td>
        <td><strong>HoloLens 2</strong></td>
        <td><a href="immersive-headset-hardware-details.md"><strong>イマーシブ ヘッドセット</strong></a></td>
    </tr>
     <tr>
        <td>頭の視線入力とコミット</td>
        <td>✔️ 推奨</td>
        <td>✔️ 推奨 (3 番目の選択肢 -<a href="interaction-fundamentals.md">他のオプションを参照</a>)</td>
        <td>➕ 代替オプション</td>
    </tr>
</table>

## <a name="head-gaze"></a>頭の視線入力
Mixed Reality ヘッドセットは、ユーザーの頭の位置と向きを利用して、その頭の方向ベクトルを決定します。 これは、ユーザーの目と目の間から直接、まっすぐ前を指し示すレーザーと考えることができます。 これはユーザーが目を向けている場所の非常に粗い近似値です。 アプリケーションでは、この射線と仮想または実世界のオブジェクトを交差させ、その位置にカーソルを描画して、現在対象としているものをユーザーに知らせることができます。

「HoloLens 2」のように、一部の mixed reality ヘッドセットには、目を見つめたベクトルを作り出す視線追跡システムが含まれています。 これにより、ユーザーが目を向けている場所のきめ細かい測定値が得られます。 視線を使用して、宝石とコミットの相互作用を構築することができます。 しかし、これには設計上の制約がまったく異なるものがあります。これについては、この[記事](eye-tracking.md)で個別に説明します。

## <a name="commit"></a>確定
オブジェクトまたは UI 要素をターゲットにした後、ユーザーは2次入力を使用して操作またはクリックできます。 これは、モデルのコミット ステップと呼ばれています。 以下のコミット方法がサポートされています。

- エアタップジェスチャ
- 音声コマンド、選択、または対象となる音声コマンドの1つを読み上げます。
- [HoloLens Clicker](hardware-accessories.md#hololens-clicker)で1つのボタンを押す
- Xbox ゲームパッドで [A] ボタンを押す
- Xbox adaptive コントローラーで [A] ボタンを押す

### <a name="head-gaze-and-air-tap-gesture"></a>頭の視線入力とエアタップ ジェスチャ
エアタップは、手をまっすぐにしてタップするジェスチャです。 エアタップを実行するには、インデックスの指を準備完了の位置まで上げて、親指でピンチし、インデックスを作成して、リリースまでさかのぼってください。 HoloLens (第1世代) では、無線タップが最も一般的な2番目の入力です。

![準備完了位置の指と、タップまたはクリックの動き](images/readyandpress.jpg)<br>

無線タップは、HoloLens 2 でも使用できます。 これは、元のバージョンからは緩和されています。 ほとんどすべての種類の pinches は、手が垂直で維持されている限り、サポートされるようになりました。 これによりユーザーは、はるかに簡単にジェスチャを学習したり実行したりできます。 この新しいエアタップでは、古いものが同じ API を使用して置き換えられるため、HoloLens 2 の再コンパイル後に既存のアプリケーションの新しい動作が自動的に行われます。

### <a name="head-gaze-and-select-voice-command"></a>頭の視線入力と [選択] 音声コマンド
音声コマンド処理は、mixed reality の主要な相互作用メソッドの1つです。 システムを制御するための非常に強力な機能を提供します。 さまざまな種類の音声操作モデルがあります。

- クリック処理を実行する、または2次入力としてコミットする汎用コマンド選択。
- Close などのオブジェクトコマンドや、拡大したオブジェクトコマンドは、アクションをセカンダリ入力として実行してコミットします。
- [開始] などのグローバルな commnads は、ターゲットを必要としません。
- 対話ユーザーインターフェイスまたは Cortana のようなエンティティには、AI 自然言語機能があります。
- カスタム コマンド

使用可能なコマンドとその使用方法の詳細については[、「comprenhesive](voice-design.md) 」を参照してください。


### <a name="head-gaze-and-hololens-clicker"></a>頭の視線入力と HoloLens クリッカー
HoloLens Clicker は、HoloLens 専用に構築された最初の周辺機器です。 これは HoloLens (第1世代) Development Edition に含まれています。 HoloLens Clicker を使用すると、ユーザーは最小限の手でクリックし、2番目の入力としてコミットできます。 HoloLens Clicker は、Bluetooth 低エネルギー (BTLE) を使用して HoloLens (第1世代) または HoloLens 2 に接続します。

![HoloLens クリッカー](images/hololens-clicker-500px.jpg)<br>
*HoloLens クリッカー*

デバイスのペアリングの詳細と手順については、[こちら](hardware-accessories.md#pairing-bluetooth-accessories)を参照してください




### <a name="head-gaze-and-xbox-wireless-controller"></a>頭の視線入力と Xbox ワイヤレス コントローラー
Xbox ワイヤレスコントローラーは、"A" ボタンを使用して、2番目の入力としてクリックを実行します。 このデバイスは、システムのナビゲーションや制御に役立つ既定のアクションのセットにマップされています。 コントローラーをカスタマイズする場合は、Xbox Accesories アプリケーションを使用して Xbox ワイヤレスコントローラーを構成します。

![Xbox ワイヤレス コントローラー](images/xboxcontroller.jpg)<br>
*Xbox ワイヤレス コントローラー*

[Xbox コントローラーと PC のペアリング](hardware-accessories.md#pairing-bluetooth-accessories)


### <a name="head-gaze-and-xbox-adaptive-controller"></a>頭の視線入力と Xbox Adaptive Controller
多くの場合、モビリティが制限されたゲーマーのニーズを満たすように設計されています。 Xbox Adaptive Controller はデバイス用の統合されたハブであり、混合の現実によりアクセスしやすくなります。

Xbox Adaptive Controller は、"A" ボタンを使用して、2番目の入力としてクリックを実行します。 デバイスは、システムの移動と制御に役立つ既定のアクションセットにマップされます。 コントローラーをカスタマイズする場合は、Xbox Accesories アプリケーションを使用して Xbox Adaptive コントローラーを構成します。

![Xbox Adaptive Controller](images/xbox-adaptive-controller-devices.jpg)<br>
*Xbox Adaptive Controller*

スイッチ、ボタン、マウント、ジョイスティックなどの外部デバイスを接続して、独自のカスタムコントローラーエクスペリエンスを作成します。 ボタン、サムスティック、およびトリガーの入力は、3.5 mm ジャックと USB ポートを介して接続されている補助デバイスで制御されます。

![Xbox Adaptive Controller のポート](images/xbox-adaptive-controller-ports.jpg)<br>
*Xbox Adaptive Controller のポート*

[デバイスをペアリングする手順](hardware-accessories.md#pairing-bluetooth-accessories)

<a href=https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller>Xbox のサイトで参照できる詳細情報</a>


## <a name="design-guidelines"></a>設計ガイドライン
> [!NOTE]
> 視線入力の設計に特化したガイダンスは、[近日中に公開](index.md)します。

## <a name="head-gaze-targeting"></a>頭の視線入力のターゲット設定
すべての操作は、入力モードに関係なく、ユーザーが操作したい要素をターゲットに設定できることに基づいて成り立っています。 Windows Mixed Reality では、これは、通常はユーザーの視線入力を使用して行われます。
ユーザーがエクスペリエンスを正常に操作できるようにするには、システムがユーザーの意図を理解していることと、ユーザーの実際の意図ができるだけ近いものである必要があります。 システムがユーザーの意図したアクションを正しく解釈するレベルまで、満足度が高くなりパフォーマンスが向上します。


## <a name="target-sizing-and-feedback"></a>ターゲットのサイズ設定とフィードバック
見つめベクターは、適切にターゲットを設定するために繰り返し表示されていますが、多くの場合、ターゲットの総計に最適です。 最小目標サイズが 1 ~ 1.5 °の場合、ほとんどのシナリオで成功したユーザー操作が可能になります。ただし、3度のターゲットでは、多くの場合、速度が向上します。 ユーザーがターゲットに設定するサイズは、3D 要素であっても実質的に 2D 領域であり、それが面しているどの投影もターゲット設定可能な領域になるはずです。 要素が "アクティブ" である (ユーザーがターゲットとしている) ことを示すいくつかの重要な手掛かりが、非常に便利です。 これには、表示されている "ホバー" 効果、オーディオのハイライトやクリック、要素を含むカーソルの配置のクリアなどの処置が含まれます。

![2 m の距離での最適なターゲット サイズ](images/gazetargeting-size-1000px.jpg)<br>
*2 m の距離での最適なターゲット サイズ*

![視線入力のターゲットとなるオブジェクトの強調表示の例](images/gazetargeting-highlighting-640px.jpg)<br>
*視線入力のターゲットとなるオブジェクトの強調表示の例*

## <a name="target-placement"></a>ターゲットの配置
多くの場合、ユーザーは、ビューのフィールドに非常に高い、または低い位置にある UI 要素を見つけることができず、主なフォーカスの周りにある領域に注目することに重点が置かれています。 ほとんどのターゲットは目の高さあたりの適正な帯域に配置すると効果的です。 ユーザーは常に比較的小さい視覚野に焦点を合わせる傾向があることを考慮すると (注意の視円錐は約 10 度)、UI 要素を概念的に関連性のある度数にグループ化すれば、ユーザーが視線入力で領域内を移動する場合に、項目から項目への注意の連鎖動作を活用できます。 UI を設計する際は、HoloLens とイマーシブ ヘッドセットでは視野内に潜在的な大きなバリエーションがあることに注意してください。

![Galaxy Explorer で視線入力のターゲット設定を容易にするためにグループ化された UI 要素の例](images/gazetargeting-grouping-1000px.jpg)<br>
*Galaxy Explorer で視線入力のターゲット設定を容易にするためにグループ化された UI 要素の例*

## <a name="improving-targeting-behaviors"></a>ターゲット設定動作の向上
ユーザーの意図を特定できる (または近似値を近似する) ことができた場合は、適切に対象としているかのように、操作の試行回数をよく受け入れることが非常に便利です。 Mixed reality エクスペリエンスに組み込むことができる成功したメソッドのいくつかを次に示します。

### <a name="head-gaze-stabilization-gravity-wells"></a>頭の視線入力の安定化 ("重力井戸")
これは、ほとんどまたはすべての時間に有効にする必要があります。 この手法では、自然なヘッドとネックのジッターを排除します。これは、ユーザーが動作を検索したり話したりすることによって動きが多い場合があります。

### <a name="closest-link-algorithms"></a>最も近いリンク アルゴリズム
これらは対話型コンテンツが少ない領域で最大の効果を発揮します。 ユーザーが何を操作しようとしているかを判断できる確率が高い場合は、ある程度のインテントを想定して、対象となる機能を補完することができます。

### <a name="backdating-and-postdating-actions"></a>バックと後処理のアクション
このメカニズムは、速度が必要なタスクに役立ちます。 ユーザーが一連のターゲットとアクティブ化を高速に進めるときは、何らかのインテントを想定し、ユーザーが tap の前または少しの間にフォーカスしていたターゲット (ea では50ミリ秒前/後) に対して、ユーザーの操作が不要な操作を実行できるようにすると便利です。テスト)。

### <a name="smoothing"></a>スムージング
このメカニズムは、自然なヘッド移動特性によってわずかなジッターと wobble を減らすことで、パスの移動に役立ちます。 パスモーションをスムーズにスムージングする場合は、時間の経過と共に、移動のサイズと距離を滑らかにします。

### <a name="magnetism"></a>磁性
このメカニズムは、最も近いリンクアルゴリズムのより一般的なバージョンであると考えることができます。これは、ターゲットに向かってカーソルを描画するか、またはユーザーが対話形式のレイアウトについての知識を使用して対象となる可能性のあるターゲットにアプローチすることで、パフォーマンスを向上させることができるからです。ユーザーの意図をアプローチします。 これは、小さいターゲットの場合は特に効果を発揮する可能性があります。

### <a name="focus-stickiness"></a>焦点の持続性
フォーカスのある近接要素にフォーカスを移すときに、フォーカスの持続性により、現在フォーカスがある要素にバイアスが適用されます。 これにより、自然なノイズを持つ2つの要素間の中間点で浮動フォーカスの切り替えビヘイビアーを減らすことができます。


## <a name="composite-gestures"></a>複合ジェスチャ

### <a name="air-tap"></a>エアタップ
エアタップジェスチャ (およびその他のジェスチャ) は、特定の tap にのみ反応します。 メニューやつかみなど、他のタップを検出するには、前の「2つの主要なコンポーネントのジェスチャ」セクションで説明されている下位レベルの相互作用をアプリケーションで直接使用する必要があります。

### <a name="tap-and-hold"></a>タップ アンド ホールド
ホールドは、単にエアタップの下向きの指の位置を保持することです。 エアタップとホールディングを組み合わせることにより、さまざまな複雑な "クリックアンドドラッグ" の相互作用が可能になります。たとえば、オブジェクトをアクティブ化する代わりにオブジェクトを選択したり、コンテキストメニューを表示するなどの二次的な相互作用を待機したりすることができます。
ただし、このジェスチャの設計時には注意が必要です。これは、ユーザーはジェスチャを長く続けると途中で手の姿勢を緩める傾向があるためです。

### <a name="manipulation"></a>操作
操作ジェスチャを使用すると、ホログラムがユーザーの手の動きに1:1 を反応させる場合に、ホログラムの移動、サイズ変更、または回転を行うことができます。 このような 1 対 1 の動きの 1 つの用途は、ユーザーが世界中で絵を描いたりペイントしたりできるようにすることです。
操作のジェスチャの最初のターゲット設定は、視線入力またはポインティングによって行う必要があります。 タップとホールドが開始されると、オブジェクトの操作は手動で処理され、ユーザーが操作中に見えなくなるのを解放します。

### <a name="navigation"></a>ナビゲーション
ナビゲーションのジェスチャは仮想ジョイスティックのように動作し、リング メニューなどの UI ウィジェット内で移動するために使用できます。 タップ アンド ホールドでジェスチャを始めてから、最初に押したところを中心に、正規化された 3D 立方体の中で手を動かします。 開始点 0 で、-1 から 1 までの値から X、Y、または Z 軸に沿って手を動かすことができます。
ナビゲーションを使用すると、マウスの中央ボタンをクリックしてからマウスを上下に移動して 2 次元の UI をスクロールするのと同様に、速度ベースの連続したスクロールやズームのジェスチャを作成できます。

Rails を使用したナビゲーションとは、特定の軸で特定のしきい値に達するまで移動を認識する機能を指します。 これは、開発者によってアプリケーションで複数の軸での移動が有効になっている場合にのみ役立ちます。たとえば、アプリケーションが X 軸と Y 軸にわたるナビゲーションジェスチャを認識するように構成されていても、X 軸に rails が指定されている場合などです。 この場合、システムは x 軸上の架空のレール (ガイド) 内にある限り、X 軸を越えた手の移動を認識します。これは、Y 軸でも手動で移動した場合に発生します。

2D のアプリ内では、ユーザーは、アプリ内でスクロール、ズーム、またはドラッグするために、垂直方向のナビゲーション ジェスチャを使用できます。 これは、同じ種類のタッチ ジェスチャをシミュレートするために、アプリに仮想の指でのタッチを挿入します。 ユーザーは、アプリケーションの上部にあるバーのツールを切り替えることによって実行するアクションを選択できます。これを行うには、ボタンを選択するか、[スクロール/ドラッグ/ズーム > ツールを <] をクリックします。

[複合ジェスチャに関する詳細情報](gestures.md#composite-gestures)

## <a name="gesture-recognizers"></a>ジェスチャ認識エンジン

ジェスチャ認識を使用する利点の1つは、現在ターゲットとなっているホログラムが受け入れるジェスチャに対してのみジェスチャ認識エンジンを構成できることです。 プラットフォームは、サポートされている特定のジェスチャを区別するために、必要に応じてあいまいさを解消します。 このようにして、エアタップをサポートしているホログラムは、プレスとリリースの間に任意の時間を受け入れることができます。一方、タップとホールドの両方をサポートするホログラムは、ホールド時間のしきい値を超えたときにタップを保留に昇格させることができます。

## <a name="hand-recognition"></a>手の認識
HoloLens は、デバイスで確認できる片手または両手の位置を追跡することで、手のジェスチャを認識します。 HoloLens は、手が準備完了状態 (手の甲を自分に向けて人差し指を立てる) または押された状態 (手の甲を自分に向けて人差し指を曲げる) のいずれかの状態のときに、手を認識します。 他の人の手による場合、HoloLens は z を無視します。
HoloLens が検出した各ハンドでは、向きと押されていない状態でその位置にアクセスできます。 手がジェスチャ フレームの端に近づくと、方向ベクトルも表示されます。これをユーザーに示すことで、ユーザーは、どのように手を動かせば、HoloLens が認識できる位置に戻せるかを知ることができます。

## <a name="gesture-frame"></a>ジェスチャ フレーム
HoloLens でのジェスチャでは、ジェスチャが検出されたカメラが適切に見える範囲で、ウェストとショルダーの間にジェスチャを使用する必要があります。 ユーザーは、正常に動作していて快適にするために、この認識の領域でトレーニングを受ける必要があります。 多くのユーザーは、最初に、ジェスチャフレームが HoloLens を通じて表示されている必要があります。また、対話するために、そのアームをすぐに保持しておく必要があります。 HoloLens Clicker を使用する場合、ジェスチャフレーム内にハンドを配置する必要はありません。

特に連続したジェスチャの場合、ユーザーが holographic オブジェクトを移動するときにジェスチャの途中でハンドを動かしたときに、意図した結果が失われる危険性があります。

考慮すべきことが 3 つあります。

- ジェスチャフレームの存在とおおよその境界に関するユーザー教育。 これは、HoloLens セットアップ中に学習されます。

- アプリケーション内のジェスチャが近づいているか、ジェスチャフレームの境界が失われた場合に、望ましくない結果につながることをユーザーに通知します。 調査には、このような通知システムの主要な品質が示されています。 HoloLens シェルは、この種類の通知の良い例を提供しています。つまり、中央カーソルで、境界の交差が行われている方向を示しています。

- ジェスチャ フレームの境界を越えることによる影響は、最小限に抑える必要があります。 一般に、これは、ジェスチャの結果を、逆順ではなく境界で停止する必要があることを意味します。 たとえば、ユーザーがある程度の holographic オブジェクトを部屋に移動する場合、ジェスチャフレームが侵害されたときに移動が停止し、開始位置には返されません。 ユーザーにはいくつかのフラストレーションが生じる可能性がありますが、境界をよりよく理解しておくことが必要であり、すべての目的のアクションを毎回再起動する必要はありません。


## <a name="see-also"></a>関連項目
* [手で直接操作](direct-manipulation.md)
* [手を使ったポイントとコミット](point-and-commit.md)
* [本能的な操作](interaction-fundamentals.md)
* [ヘッド視線入力とドウェル](gaze-and-dwell.md)
* [音声コマンド](voice-design.md)





