---
title: 手で直接操作
description: 直接操作の入力モデルの概要
author: caseymeekhof
ms.author: cmeekhof
ms.date: 04/02/2019
ms.topic: article
ms.localizationpriority: high
keywords: 実際には、視線の先、視線の先を対象との相互作用、混在、デザイン、ほぼ手 HoloLens
ms.openlocfilehash: 412d77a1d7446f82ddf43f051fdb149cb1fd559c
ms.sourcegitcommit: d565a69a9320e736304372b3f010af1a4d286a62
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 05/20/2019
ms.locfileid: "65940766"
---
# <a name="direct-manipulation-with-hands"></a><span data-ttu-id="45129-104">手で直接操作</span><span class="sxs-lookup"><span data-stu-id="45129-104">Direct manipulation with hands</span></span>
<span data-ttu-id="45129-105">直接操作は、手を直接ホログラム手を加えることを含む入力モデルです。</span><span class="sxs-lookup"><span data-stu-id="45129-105">Direct manipulation is an input model that involves touching holograms directly with your hands.</span></span> <span data-ttu-id="45129-106">直接操作の目的は、オブジェクトが現実の世界と同じように動作します。</span><span class="sxs-lookup"><span data-stu-id="45129-106">The goal with direct manipulation is that objects behave just as they do in the real world.</span></span> <span data-ttu-id="45129-107">ボタンを押すだけでアクティブ化できるオブジェクトは、それを取得すること選択できる、2D のコンテンツが仮想のタッチ スクリーンのように動作します。</span><span class="sxs-lookup"><span data-stu-id="45129-107">Buttons can be activated simply by pressing them, objects can be picked up by grabbing them, and 2D content behaves like a virtual touchscreen.</span></span>  <span data-ttu-id="45129-108">このため、直接操作については、ユーザーが簡単とも楽しいですね。</span><span class="sxs-lookup"><span data-stu-id="45129-108">Because of this, direct manipulation is easy for users to learn, and it's fun too.</span></span>  <span data-ttu-id="45129-109">到達武器取引規則内では、コンテンツと対話するための使用に適してつまり、「近く」入力モデルと見なされます。</span><span class="sxs-lookup"><span data-stu-id="45129-109">It is considered a "near" input model, meaning it is best used for interacting with content that is within arms reach.</span></span>

<span data-ttu-id="45129-110">直接操作は、アフォー ダンスに基づく、つまりユーザー フレンドリです。</span><span class="sxs-lookup"><span data-stu-id="45129-110">Direct manipulation is affordance-based, meaning it's user friendly.</span></span> <span data-ttu-id="45129-111">ユーザーに教えるにシンボリック ジェスチャはありません。</span><span class="sxs-lookup"><span data-stu-id="45129-111">There are no symbolic gestures to teach users.</span></span> <span data-ttu-id="45129-112">すべての対話は、タッチまたは取得できるビジュアル要素の周囲に構築されます。</span><span class="sxs-lookup"><span data-stu-id="45129-112">All interactions are built around a visual element that you can touch or grab.</span></span>

## <a name="device-support"></a><span data-ttu-id="45129-113">デバイスのサポート</span><span class="sxs-lookup"><span data-stu-id="45129-113">Device support</span></span>


| <span data-ttu-id="45129-114">入力モデル</span><span class="sxs-lookup"><span data-stu-id="45129-114">Input Model</span></span> | [<span data-ttu-id="45129-115">HoloLens (第 1 世代)</span><span class="sxs-lookup"><span data-stu-id="45129-115">HoloLens (1st Gen)</span></span>](https://review.docs.microsoft.com/en-us/windows/mixed-reality/hololens-hardware-details?branch=master) | <span data-ttu-id="45129-116">HoloLens 2</span><span class="sxs-lookup"><span data-stu-id="45129-116">HoloLens 2</span></span> |[<span data-ttu-id="45129-117">イマーシブ ヘッドセット</span><span class="sxs-lookup"><span data-stu-id="45129-117">Immersive Headsets</span></span>](https://review.docs.microsoft.com/en-us/windows/mixed-reality/immersive-headset-hardware-details?branch=master)|
|:-------- | :-------| :--------| :------------|
| <span data-ttu-id="45129-118">直接操作</span><span class="sxs-lookup"><span data-stu-id="45129-118">Direct manipulation</span></span> | <span data-ttu-id="45129-119">❌ がサポートされていません</span><span class="sxs-lookup"><span data-stu-id="45129-119">❌ Not supported</span></span> | <span data-ttu-id="45129-120">推奨 ✔️</span><span class="sxs-lookup"><span data-stu-id="45129-120">✔️ Recommended</span></span> | <span data-ttu-id="45129-121">➕ 代替[をポイントし、コミット](https://review.docs.microsoft.com/en-us/windows/mixed-reality/point-and-commit?branch=master)をお勧めします。</span><span class="sxs-lookup"><span data-stu-id="45129-121">➕ An alternative [point and commit](https://review.docs.microsoft.com/en-us/windows/mixed-reality/point-and-commit?branch=master) is recommended.</span></span>

<span data-ttu-id="45129-122">直接操作では、HoloLens 2 では、プライマリ入力モデルを新しい関節手動追跡システムを使用します。</span><span class="sxs-lookup"><span data-stu-id="45129-122">Direct manipulation is a primary input model on HoloLens 2, and utilizes the new articulated hand-tracking system.</span></span> <span data-ttu-id="45129-123">入力モデルでは、アニメーション コント ローラーを使用してイマーシブ ヘッドセットに収録されてもが、オブジェクトの操作の外部での相互作用の主要な手段としては推奨されません。</span><span class="sxs-lookup"><span data-stu-id="45129-123">The input model is also available on immersive headsets through the use of motion controllers, but is not recommended as a primary means of interaction outside of object manipulation.</span></span>  <span data-ttu-id="45129-124">ダイレクト manipluation を HoloLens でご利用いただけません (第 1 世代)。</span><span class="sxs-lookup"><span data-stu-id="45129-124">Direct manipluation is not available on HoloLens (1st gen).</span></span>


## <a name="collidable-fingertip"></a><span data-ttu-id="45129-125">Collidable 指先</span><span class="sxs-lookup"><span data-stu-id="45129-125">Collidable fingertip</span></span>

<span data-ttu-id="45129-126">HoloLens 2 では、ユーザーの実際の手が認識され、左側と右側のスケルトン モデルとして解釈されます。</span><span class="sxs-lookup"><span data-stu-id="45129-126">On HoloLens 2, the user's real hands are recognized and interpreted as left and right hand skeletal models.</span></span> <span data-ttu-id="45129-127">手を直接ホログラム手を加えることのアイデアを実装するために理想的には、5 コライダーを接続できます各ハンドのスケルトン モデルのすぐに利用できる 5。</span><span class="sxs-lookup"><span data-stu-id="45129-127">To implement the idea of touching holograms directly with hands, ideally, 5 colliders could be attached to 5 fingertips of each hand skeletal model.</span></span> <span data-ttu-id="45129-128">ただし、実際には、触るフィードバックがないことによる 10 collidable すぐ予期しない競合、発生ホログラム。</span><span class="sxs-lookup"><span data-stu-id="45129-128">However, practically, due to the lack of tactile feedback, 10 collidable fingertips caused unexpected and unpredictable collisions with holograms.</span></span> 

<span data-ttu-id="45129-129">そのため、それぞれインデックスの指でのみコライダーをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="45129-129">Hence, we suggest to only put a collider on each index finger.</span></span> <span data-ttu-id="45129-130">Collidable インデックス ヒントは、次の図に示すようにまだ 1 本の指のキーを押して、1 本指でタップ、2 本の指のキーを押して、5 本の指のキーを押してなどの他の指に関連する多様なタッチ ジェスチャのアクティブなタッチ ポイントとして使用できます。</span><span class="sxs-lookup"><span data-stu-id="45129-130">The collidable index fingertips can still serve as active touch points for diverse touch gestures involving other fingers, such as 1-finger press, 1-finger tap, 2-finger press and 5-finger press, as shown in the image below.</span></span>

![Collidable 指先イメージ](images/Collidable-Fingertip-720px.jpg)

### <a name="sphere-collider"></a><span data-ttu-id="45129-132">球のコライダー</span><span class="sxs-lookup"><span data-stu-id="45129-132">Sphere collider</span></span>

<span data-ttu-id="45129-133">ランダムな汎用図形を使用せずに球コライダーを使用して、ほぼ対象とする場合より優れた的な手掛かりを提供することを視覚的に表示するためにお勧めします。</span><span class="sxs-lookup"><span data-stu-id="45129-133">Instead of using a random generic shape, we suggest to use a sphere collider and to visually render it to provide better cues for near targeting.</span></span> <span data-ttu-id="45129-134">球の直径は、タッチの精度を向上させるインデックス本の指の太さと一致する必要があります。</span><span class="sxs-lookup"><span data-stu-id="45129-134">The sphere's diameter should match the thickness of the index finger to increase touch accuracy.</span></span> <span data-ttu-id="45129-135">手 API を呼び出すことによって、変数の本の指の太さを取得する簡単になります。</span><span class="sxs-lookup"><span data-stu-id="45129-135">It will be easy to retrieve the variable of finger thickness by calling the hand API.</span></span>

### <a name="fingertip-cursor"></a><span data-ttu-id="45129-136">指先カーソル</span><span class="sxs-lookup"><span data-stu-id="45129-136">Fingertip cursor</span></span>

<span data-ttu-id="45129-137">インデックスの指で collidable 球体をレンダリングするだけでなく対話形式での近くに対象とするエクスペリエンスの向上を実現するために、指先カーソル高度なソリューションを作成しました。</span><span class="sxs-lookup"><span data-stu-id="45129-137">In addition to rendering a collidable sphere on the index fingertip, we've created an advanced solution, fingertip cursor, to achieve better near-targeting experience interactively.</span></span> <span data-ttu-id="45129-138">インデックスの指で接続されているドーナツ型のカーソルになります。</span><span class="sxs-lookup"><span data-stu-id="45129-138">It is a donut-shaped cursor attached on the index fingertip.</span></span> <span data-ttu-id="45129-139">近接、に従って動的に反応する方向と以下のようにサイズの観点から、ターゲット。</span><span class="sxs-lookup"><span data-stu-id="45129-139">According to proximity, it dynamically reacts to a target in terms of orientation and size as detailed below:</span></span>

* <span data-ttu-id="45129-140">人差し指ホログラムに向かって移動すると、カーソルはホログラムの画面に並列では常と、徐々 にそれに応じてそのサイズを縮小します。</span><span class="sxs-lookup"><span data-stu-id="45129-140">When an index finger moves toward a hologram, the cursor is always parallel to the hologram's surface  and gradually shrinks its size accordingly.</span></span>
* <span data-ttu-id="45129-141">指表面に触れてとすぐに、カーソルがドットに縮小し、タッチ イベントを出力します。</span><span class="sxs-lookup"><span data-stu-id="45129-141">As soon as the finger touches the surface, the cursor shrinks into a dot and emits a touch event.</span></span>

<span data-ttu-id="45129-142">対話型のフィードバックでは、ユーザーは web コンテンツへのハイパーリンクをトリガーする、次のように、ボタンを押すなどのタスクを対象とするほぼ高精度を実現できます。</span><span class="sxs-lookup"><span data-stu-id="45129-142">With the interactive feedback, users can achieve high precision near targeting tasks, such as triggering a hyperlink on web content or pressing a button, as shown, below.</span></span> 

![指先カーソル イメージ](images/Fingertip-Cursor-720px.jpg)

## <a name="bounding-box-with-proximity-shader"></a><span data-ttu-id="45129-144">近接シェーダーで境界ボックス</span><span class="sxs-lookup"><span data-stu-id="45129-144">Bounding box with proximity shader</span></span>

<span data-ttu-id="45129-145">ホログラム自体には、触るフィードバックがないことを補正するために、ビジュアルおよびオーディオの両方のフィードバックを提供する機能も必要です。</span><span class="sxs-lookup"><span data-stu-id="45129-145">The hologram itself also requires the ability to provide both visual and audio feedback to compensate the lack of tactile feedback.</span></span> <span data-ttu-id="45129-146">そのため、近接シェーダーで境界ボックスの概念を生成します。</span><span class="sxs-lookup"><span data-stu-id="45129-146">For that, we generate the concept of a bounding box with proximity shader.</span></span> <span data-ttu-id="45129-147">境界ボックスは、3 D オブジェクトを囲む最小帯域幅消費型領域です。</span><span class="sxs-lookup"><span data-stu-id="45129-147">A bounding box is a minimum volumetric area that encloses a 3D object.</span></span> <span data-ttu-id="45129-148">境界ボックスには、近接シェーダーと呼ばれる、対話型の表示メカニズムがあります。</span><span class="sxs-lookup"><span data-stu-id="45129-148">The bounding box has an interactive rendering mechanism called proximity shader.</span></span> <span data-ttu-id="45129-149">近接シェーダーに動作します。</span><span class="sxs-lookup"><span data-stu-id="45129-149">The proximity shader behaves:</span></span>

* <span data-ttu-id="45129-150">人差し指が範囲内と、境界ボックスの画面で指のスポット ライトがキャストされます。</span><span class="sxs-lookup"><span data-stu-id="45129-150">When the index finger is within a range, a fingertip spotlight is cast on the surface of bounding box.</span></span>
* <span data-ttu-id="45129-151">画面に指が近づいたら、適宜にスポット ライトがでまとめます。</span><span class="sxs-lookup"><span data-stu-id="45129-151">When the fingertip gets closer to the surface, the spotlight condenses accordingly.</span></span>
* <span data-ttu-id="45129-152">指、画面をタッチとすぐに、境界ボックス全体は色を変更またはタッチの状態を反映するように視覚効果を生成します。</span><span class="sxs-lookup"><span data-stu-id="45129-152">As soon as the fingertip touch the surface, the whole bounding box changes the color or generate visual effect to reflect the touch state.</span></span>
* <span data-ttu-id="45129-153">その一方で、サウンド効果は、タッチを視覚的フィードバックを強化するためにアクティブにできます。</span><span class="sxs-lookup"><span data-stu-id="45129-153">Meanwhile, a sound effect can be activated to enhance the visual touch feedback.</span></span>

![近接シェーダーのイメージの境界ボックス](images/Bounding-Box-With-Proximity-Shader-720px.jpg)

## <a name="pressable-button"></a><span data-ttu-id="45129-155">Pressable ボタン</span><span class="sxs-lookup"><span data-stu-id="45129-155">Pressable button</span></span>

<span data-ttu-id="45129-156">Collidable 指でのユーザーが、非常に基本的な holographic UI コンポーネントは、pressable ボタンと対話する準備ができてようになりました。</span><span class="sxs-lookup"><span data-stu-id="45129-156">With a collidable fingertip, users are now ready to interact with the very fundamental holographic UI component, pressable button.</span></span> <span data-ttu-id="45129-157">Pressable ボタンは、直接本の指のキーを押してに合わせた holographic ボタンです。</span><span class="sxs-lookup"><span data-stu-id="45129-157">A pressable button is a holographic button tailored for direct finger press.</span></span> <span data-ttu-id="45129-158">ここでも、触るフィードバックがないことによる pressable ボタンに触るフィードバックに関連する問題に対処するいくつかのメカニズムを提供します。</span><span class="sxs-lookup"><span data-stu-id="45129-158">Again, due to the lack of tactile feedback, a pressable button equips a couple mechanisms to tackle tactile feedback-related issues.</span></span>

* <span data-ttu-id="45129-159">最初のメカニズムは、前のセクションに記載された、近接シェーダーで境界ボックスです。</span><span class="sxs-lookup"><span data-stu-id="45129-159">The first mechanism is a bounding box with proximity shader, detailed in the previous section.</span></span> <span data-ttu-id="45129-160">深く理解する方法をユーザーの近接を提供する機能、ボタンを含む連絡先。</span><span class="sxs-lookup"><span data-stu-id="45129-160">It serves to provide better sense of proximity for users to approach and make contact with a button.</span></span>
* <span data-ttu-id="45129-161">2 つ目は、低下します。</span><span class="sxs-lookup"><span data-stu-id="45129-161">The second one is depression.</span></span> <span data-ttu-id="45129-162">指先をボタンに接続した後、キーを押して、感を作成します。</span><span class="sxs-lookup"><span data-stu-id="45129-162">It creates sense of press, after a fingertip contacts the button.</span></span> <span data-ttu-id="45129-163">このメカニズムは、ボタンは、深さ軸に沿った指と緊密に移動します。</span><span class="sxs-lookup"><span data-stu-id="45129-163">The mechanism is that the button tightly moves with the fingertip along the depth axis.</span></span> <span data-ttu-id="45129-164">ボタンは、(キーを押します) で指定された深さに達すると、渡された後に (リリース) の深さを離れるときにトリガーできます。</span><span class="sxs-lookup"><span data-stu-id="45129-164">The button can be triggered when it reaches a designated depth (on press) or leaves the depth (on release) after passing through it.</span></span>
* <span data-ttu-id="45129-165">ボタンがトリガーされたときに、フィードバックを強化するために、サウンド効果を追加する必要があります。</span><span class="sxs-lookup"><span data-stu-id="45129-165">The sound effect should be added to enhance feedback, when the button is triggered.</span></span>

![Pressable ボタンの画像](images/Pressable-Button-720px.jpg)

## <a name="2d-slate-interaction"></a><span data-ttu-id="45129-167">2D のスレートの相互作用</span><span class="sxs-lookup"><span data-stu-id="45129-167">2D slate interaction</span></span>

<span data-ttu-id="45129-168">2D スレートは、web ブラウザーなどの 2D アプリのコンテンツをホストしている holographic コンテナーです。</span><span class="sxs-lookup"><span data-stu-id="45129-168">A 2D slate is a holographic container hosting 2D app contents, such as web browser.</span></span> <span data-ttu-id="45129-169">物理タッチ画面との対話のメンタル モデルを利用することを直接操作を使用して 2D スレートと対話するための設計概念です。</span><span class="sxs-lookup"><span data-stu-id="45129-169">The design concept for interacting with a 2D slate via direct manipulation is to leverage the mental model of interacting with a physical touch screen.</span></span>

<span data-ttu-id="45129-170">スレートの連絡先と操作。</span><span class="sxs-lookup"><span data-stu-id="45129-170">To interact with the slate contact:</span></span>

* <span data-ttu-id="45129-171">インデックス本の指を使用して、ハイパーリンクやボタンを押します。</span><span class="sxs-lookup"><span data-stu-id="45129-171">Use an index finger to press a hyperlink or a button.</span></span>
* <span data-ttu-id="45129-172">スレートのコンテンツをスクロール アップ/ダウン、人差し指を使用します。</span><span class="sxs-lookup"><span data-stu-id="45129-172">Use an index finger to scroll a slate content up and down.</span></span>
* <span data-ttu-id="45129-173">ユーザーは、in および out 本の指の動きを相対に従ってスレートのコンテンツを拡大する 2 つの人差し指を使用します。</span><span class="sxs-lookup"><span data-stu-id="45129-173">Users use two index fingers to zoom in and out the slate content according to relative motion of fingers.</span></span>

![2D のスレート イメージ](images/2D-Slate-Interaction-720px.jpg)

<span data-ttu-id="45129-175">2D を操作するためには、それ自体をスレートします。</span><span class="sxs-lookup"><span data-stu-id="45129-175">For manipulating the 2D slate itself:</span></span>

* <span data-ttu-id="45129-176">角や辺に最も近い操作アフォーを明らかに手をアプローチします。</span><span class="sxs-lookup"><span data-stu-id="45129-176">Approach your hands toward corners and edges to reveal the closest manipulation affordances.</span></span>
* <span data-ttu-id="45129-177">操作アフォーをつかんで、上隅にあるアフォーで統一されたスケーリングを実行、およびエッジ アフォーを介してリフローします。</span><span class="sxs-lookup"><span data-stu-id="45129-177">Grab the manipulation affordances, and perform uniform scaling through the corner affordances and reflow via the edge affordances.</span></span>
* <span data-ttu-id="45129-178">2D のスレートは、全体のスレートを移動することができますの上部にある holobar を取得します。</span><span class="sxs-lookup"><span data-stu-id="45129-178">Grab the holobar at the top of the 2D slate, which lets you move the whole slate.</span></span>

![スレートの操作のイメージ](images/Manipulate-2d-slate-720px.jpg)

## <a name="3d-object-manipulation"></a><span data-ttu-id="45129-180">3D オブジェクトの操作</span><span class="sxs-lookup"><span data-stu-id="45129-180">3D object manipulation</span></span>

<span data-ttu-id="45129-181">HoloLens 2 には、境界ボックスを各 3D オブジェクトに適用することで 3D hologramphic オブジェクトを操作ユーザーが直接に手を有効にすることができます。</span><span class="sxs-lookup"><span data-stu-id="45129-181">HoloLens 2 lets lets users enable their hands to direct manipulate 3D hologramphic objects by applying a bounding box to each 3D object.</span></span> <span data-ttu-id="45129-182">境界ボックスは、その近接シェーダーを通じてより優れた深さ perception を提供します。</span><span class="sxs-lookup"><span data-stu-id="45129-182">The bounding box provides better depth perception through its proximity shader.</span></span> <span data-ttu-id="45129-183">境界ボックスを 2 つの 3D オブジェクトの操作のデザイン方法があります。</span><span class="sxs-lookup"><span data-stu-id="45129-183">With the bounding box, there are two design approaches for 3D object manipulation.</span></span>

### <a name="affordance-based-manipulation"></a><span data-ttu-id="45129-184">アフォー ダンス ベースの操作</span><span class="sxs-lookup"><span data-stu-id="45129-184">Affordance-based manipulation</span></span>

<span data-ttu-id="45129-185">これにより、境界ボックスとその周り操作アフォーを使用して 3D オブジェクトを操作できます。</span><span class="sxs-lookup"><span data-stu-id="45129-185">This lets you manipulate the 3D object through a bounding box and the manipulation affordances around it.</span></span> <span data-ttu-id="45129-186">ユーザーの手の形は、3 D オブジェクトの近くとすぐに、境界ボックスと最も近いアフォー ダンス開示されます。</span><span class="sxs-lookup"><span data-stu-id="45129-186">As soon as a user's hand is close to a 3D object, the bounding box and the nearest affordance are revealed.</span></span> <span data-ttu-id="45129-187">ユーザーは、オブジェクト全体、回転するエッジ アフォーおよびを一様にスケール上隅にあるアフォーを移動する、境界ボックスを取得できます。</span><span class="sxs-lookup"><span data-stu-id="45129-187">Users can grab the bounding box to move the whole object, the edge affordances to rotate and the corner affordances to scale uniformly.</span></span>

![3D オブジェクトの操作のイメージ](images/3D-Object-Manipulation-720px.jpg)

### <a name="non-affordance-based-manipulation"></a><span data-ttu-id="45129-189">非アフォー ダンス ベースの操作</span><span class="sxs-lookup"><span data-stu-id="45129-189">Non-affordance based manipulation</span></span>

<span data-ttu-id="45129-190">このメカニズムで境界ボックスにアフォー ダンスはアタッチされません。</span><span class="sxs-lookup"><span data-stu-id="45129-190">In this mechanism, no affordance is attached to the bounding box.</span></span> <span data-ttu-id="45129-191">ユーザーは、のみ、境界ボックスを表示し、直接やり取りすることができます。</span><span class="sxs-lookup"><span data-stu-id="45129-191">Users can only reveal the bounding box, then directly interact with it.</span></span> <span data-ttu-id="45129-192">境界ボックスは、1 つの手でグラブされる場合平行移動とオブジェクトの回転、モーション センサーとして手のアイコンの向きに関連付けられます。</span><span class="sxs-lookup"><span data-stu-id="45129-192">If the bounding box is grabbed with one hand, the translation and rotation of the object are associated to motion and orientation of the hand.</span></span> <span data-ttu-id="45129-193">オブジェクトは、2 つの手をグラブ ユーザーで変換、スケールおよび回転させる 2 つのハンドの動作を相対に従ってことができます。</span><span class="sxs-lookup"><span data-stu-id="45129-193">When the object is grabbed with two hands, users can translate, scale and rotate it according to relative motions of two hands.</span></span>

<span data-ttu-id="45129-194">特定の操作には、有効桁数が必要ですを使用することをお勧めします。**アフォー ダンスに基づく操作**粒度の高いレベルを提供するため、します。</span><span class="sxs-lookup"><span data-stu-id="45129-194">Specific manipulation requires precision, we recommend you use **affordance-based manipulation**, because it provides a high level of granularity.</span></span> <span data-ttu-id="45129-195">柔軟な操作は、ことをお勧めする使用**非アフォー ダンス操作**はインスタントとおもしろいエクスペリエンスことができます。</span><span class="sxs-lookup"><span data-stu-id="45129-195">For flexible manipulation, we recommend you uses **non-affordance manipulation** is as it allows for instant and playful experiences.</span></span>

## <a name="instinctual-gestures"></a><span data-ttu-id="45129-196">Instinctual ジェスチャ</span><span class="sxs-lookup"><span data-stu-id="45129-196">Instinctual gestures</span></span>

<span data-ttu-id="45129-197">HoloLens で (第 1 世代) 講師ユーザー ブルーム エア タップなどのいくつかの定義済みジェスチャ。</span><span class="sxs-lookup"><span data-stu-id="45129-197">With HoloLens (1st gen), we taught users a couple predefined gestures,such as Bloom and Air Tap.</span></span> <span data-ttu-id="45129-198">HoloLens の 2、シンボルのジェスチャを記憶するユーザーをお願いはありません。</span><span class="sxs-lookup"><span data-stu-id="45129-198">For HoloLens 2, we don't ask users to memorize any symbolic gestures.</span></span> <span data-ttu-id="45129-199">すべての必要なユーザー ジェスチャ、ホログラムと内容との対話をユーザーの必要性は instinctual です。</span><span class="sxs-lookup"><span data-stu-id="45129-199">All required user gestures, users need to interact with holograms and contents, are instinctual.</span></span> <span data-ttu-id="45129-200">Instinctual ジェスチャを実現する方法は、UI アフォーのデザインによってジェスチャを実行するユーザーをガイドです。</span><span class="sxs-lookup"><span data-stu-id="45129-200">The way to achieve instinctual gesture is to guide users to perform gestures through the design of UI affordances.</span></span>

<span data-ttu-id="45129-201">などの場合は、オブジェクトのグラブまたは 2 本の指のピンチでのコントロール ポイントにぜひ、オブジェクトまたはコントロール ポイントは小にします。</span><span class="sxs-lookup"><span data-stu-id="45129-201">For example, if we encourage you to grab an object or a control point with two finger pinch, the object or the control point should be small.</span></span> <span data-ttu-id="45129-202">5 本の指グラブを実行することをする場合、オブジェクトまたはコントロール ポイントが比較的大きななければなりません。</span><span class="sxs-lookup"><span data-stu-id="45129-202">If we want you to perform five finger grab, the object or the control point should be relatively big.</span></span> <span data-ttu-id="45129-203">ボタンと同様に、小さなボタンはユーザーを制限する大きなボタンをユーザーの手のひらで押すお勧めします中に 1 本の指を押します。</span><span class="sxs-lookup"><span data-stu-id="45129-203">Similar to buttons, a tiny button would limit users to press it with a single finger, while a huge button would encourage users to press it with their palms.</span></span>

![](images/Instinctual-Gestures-720px.jpg)

## <a name="symmetric-design-between-hands-and-6-dof-controllers"></a><span data-ttu-id="45129-204">ハンドと 6 の自由度のコント ローラーの間の対称のデザイン</span><span class="sxs-lookup"><span data-stu-id="45129-204">Symmetric design between hands and 6 DoF controllers</span></span>

<span data-ttu-id="45129-205">お気付き相互作用の parallels VR で AR およびモーションのコント ローラー内のハンド間に描画できることがあります。</span><span class="sxs-lookup"><span data-stu-id="45129-205">You may have noticed that there are now interaction parallels we can draw between hands in AR and motion controllers in VR.</span></span> <span data-ttu-id="45129-206">両方の入力は、それぞれの環境での直接の操作をトリガーする使用できます。</span><span class="sxs-lookup"><span data-stu-id="45129-206">Both inputs can be used to trigger direct manipulations in their respective environments.</span></span> <span data-ttu-id="45129-207">HoloLens の 2 をグラブしてドラッグ近距離 works 手をあまりグラブ ボタンと同じ方法では、WMR でモーション コント ローラーには。</span><span class="sxs-lookup"><span data-stu-id="45129-207">In HoloLens 2, grabbing and dragging with hands at a close distance works much in the same way as the grab button does on the motion controllers in WMR.</span></span> <span data-ttu-id="45129-208">これにより、2 つのプラットフォーム間の相互作用に関する知識をユーザーに提供され、便利だとこれまで決めた場合、他のいずれかからアプリを移植します。</span><span class="sxs-lookup"><span data-stu-id="45129-208">This provides users with interaction familiarity between the two platforms and may prove useful should you ever decide to port your app from one to the other.</span></span>

## <a name="optimize-with-eye-tracking"></a><span data-ttu-id="45129-209">視線を最適化します</span><span class="sxs-lookup"><span data-stu-id="45129-209">Optimize with eye tracking</span></span>

<span data-ttu-id="45129-210">ホログラムを誤ってトリガーすることがなくもう手を任意の場所を移動できない場合は、ストレスも簡単になることができますが、意図したとおりに動作する場合、直接操作を魔法のようなと思われることができます。</span><span class="sxs-lookup"><span data-stu-id="45129-210">Direct manipulation can feel magical if it works as intended, but can also quickly become frustrating if you can’t move your hand anywhere anymore without unintentionally triggering a hologram.</span></span>
<span data-ttu-id="45129-211">ユーザーの目的は、どのような改善を識別する視線が可能性のあるできます。</span><span class="sxs-lookup"><span data-stu-id="45129-211">Eye tracking can potentially help in better identifying what the user’s intent is.</span></span>

* <span data-ttu-id="45129-212">**ときに**:誤って操作応答のトリガーを減らします。</span><span class="sxs-lookup"><span data-stu-id="45129-212">**When**: Reduce falsely triggering a manipulation response.</span></span> <span data-ttu-id="45129-213">視線では、どのようなユーザーが現在進行中でより良く理解できます。</span><span class="sxs-lookup"><span data-stu-id="45129-213">Eye tracking allows for better understanding what a user is currently engaged with.</span></span>
<span data-ttu-id="45129-214">たとえば、以上に到達すると、実際の作業のツールを取得するときに、holographic (説明) のテキストを読んでいるとします。</span><span class="sxs-lookup"><span data-stu-id="45129-214">For example, imagine you are reading through a holographic (instructional) text when reaching over to grab you real-world work tool.</span></span>

<span data-ttu-id="45129-215">これにより、誤ってに (おそらくもが、ユーザーのフィールドの視野 (FOV) 外に) する前に気付きですかもいくつかの対話型 holographic ボタンに手を移動します。</span><span class="sxs-lookup"><span data-stu-id="45129-215">By doing so, you accidentally move your hand across some interactive holographic buttons that you hadn't even noticed before (maybe it even was outside of the user's Field-of-View (FOV)).</span></span>

  <span data-ttu-id="45129-216">要するに：ユーザーいないホログラムしばらくの間、見てまだ、タッチまたは把握イベントが検出されましたが、ユーザーがそのホログラムと対話する目的で実際にいる可能性があります。</span><span class="sxs-lookup"><span data-stu-id="45129-216">Long story short: If the user hasn't looked at a hologram for a while, yet a touch or grasp event has been detected for it, it is likely that the user wasn't actually intending to interact with that hologram.</span></span>

* <span data-ttu-id="45129-217">**どれを**:別の例にはとは別に正の値の false のアクティブ化をアドレス指定にはよりどのホログラムを取得したりの観点からクリアは、いくつかホログラムがそれぞれの近くに配置されている場合は特に、積集合の正確なポイントは指定できません流行っていますよねを識別するが含まれていますその他。</span><span class="sxs-lookup"><span data-stu-id="45129-217">**Which one**:  Aside from addressing false positive activations, another example includes better identifying which holograms to grab or poke as the precise intersection point may not be clear from your perspective especially if several holograms are positioned close to each other.</span></span>

  <span data-ttu-id="45129-218">あるときに目追跡 HoloLens 2 で特定の制限について正確に視線の先を目、まだが非常に役立ちますの深さのような違いのための相互作用の近く手入力で操作するときにそれを判断できます。</span><span class="sxs-lookup"><span data-stu-id="45129-218">While eye tracking on HoloLens 2 has a certain limitation on how accurately it can determine you eye gaze, this can still be very helpful for near interactions due to depth disparity when interacting with hand input.</span></span>  <span data-ttu-id="45129-219">つまりは手がの背後にある、またはホログラムたとえば操作ウィジェットを正確に取得する前にあるかどうかを確認する困難な場合があります。</span><span class="sxs-lookup"><span data-stu-id="45129-219">This means that it is sometimes difficult to determine whether your hand is behind or in front of a hologram to precisely grab a manipulation widget for example.</span></span>

* <span data-ttu-id="45129-220">**場所**:どのようなユーザーに関する情報を使用はクイック スロー ジェスチャで見る。</span><span class="sxs-lookup"><span data-stu-id="45129-220">**Where to**: Use information about what a user is looking at with quick- throwing gestures.</span></span> <span data-ttu-id="45129-221">ホログラムを取得し、約、目的の送信先に出すように頼みます。</span><span class="sxs-lookup"><span data-stu-id="45129-221">Grab a hologram and roughly toss it toward your intended destination.</span></span>  

    <span data-ttu-id="45129-222">場合もありますがこの中に非常に不正確な変換先は問題なく、すばやく手のジェスチャを実行する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="45129-222">While this may sometimes works just fine, quickly performing hand gestures may result in highly inaccurate destinations.</span></span> <span data-ttu-id="45129-223">これは、視線助け、目的の位置に戻すベクトルをスローして手のアイコンのリーン場所です。</span><span class="sxs-lookup"><span data-stu-id="45129-223">This is where eye tracking could help out to lean the hand throwing vector back to your intended position.</span></span>

## <a name="see-also"></a><span data-ttu-id="45129-224">関連項目</span><span class="sxs-lookup"><span data-stu-id="45129-224">See also</span></span>

* [<span data-ttu-id="45129-225">頭の視線入力とコミット</span><span class="sxs-lookup"><span data-stu-id="45129-225">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="45129-226">手を使ったポイントとコミット</span><span class="sxs-lookup"><span data-stu-id="45129-226">Point and commit with hands</span></span>](point-and-commit.md)
* [<span data-ttu-id="45129-227">本能的な操作</span><span class="sxs-lookup"><span data-stu-id="45129-227">Instinctual interactions</span></span>](interaction-fundamentals.md)

